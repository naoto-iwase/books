[
  {
    "objectID": "ja/olmo-3/index.html",
    "href": "ja/olmo-3/index.html",
    "title": "Olmo 3",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n論文: arXiv:2512.13961",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#目次",
    "href": "ja/olmo-3/index.html#目次",
    "title": "Olmo 3",
    "section": "目次",
    "text": "目次\n\n全体像\n\n\nBase Model Training\n\nDolma 3 データセット\nOlmoBaseEval: 評価スイート\nMidtraining: 中間訓練\nLong-context Extension: 長文脈拡張\nDeduplication: 重複排除\nolmOCR science PDFs\nData Mixing: データミキシング手法\n\n\n\nPost-training\n\nDolci: Post-training データスイート\nDelta Learning: 選好調整の新手法\nOlmoRL / GRPO: 効率的な強化学習",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#モデルバリエーション",
    "href": "ja/olmo-3/index.html#モデルバリエーション",
    "title": "Olmo 3",
    "section": "モデルバリエーション",
    "text": "モデルバリエーション\nOlmo 3 Base: 基盤モデル（7B, 32B）- 最強の完全オープン Base モデル\nOlmo 3 Think: 段階的推論を行う思考型モデル - Qwen 2.5、Gemma 2/3、DeepSeek R1 を上回る\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル - 関数呼び出しに最適化\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練 - 完全オープンな RL ベンチマーク",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#主な成果",
    "href": "ja/olmo-3/index.html#主な成果",
    "title": "Olmo 3",
    "section": "主な成果",
    "text": "主な成果\nOlmo 3.1 Think 32B の主要ベンチマーク結果:\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#訓練コスト",
    "href": "ja/olmo-3/index.html#訓練コスト",
    "title": "Olmo 3",
    "section": "訓練コスト",
    "text": "訓練コスト\n1024 台の H100 GPU を使用して約 56 日（推定コスト: $2.75M）\n\nPretraining: 約 47 日\nPost-training: 約 9 日",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#オープンアーティファクト",
    "href": "ja/olmo-3/index.html#オープンアーティファクト",
    "title": "Olmo 3",
    "section": "オープンアーティファクト",
    "text": "オープンアーティファクト\nすべての中間チェックポイント、学習データ、コード、評価ツールを公開:\n\nモデル: Base, Think, Instruct, RL-Zero のすべてのチェックポイント\nデータ: Dolma 3（事前学習）、Dolci（後訓練）\nコード: OLMo-core、Open Instruct、duplodocus、OLMES\n\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html",
    "href": "ja/olmo-3/09-delta-learning.html",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning は、Preference tuning（選好調整）における新しいアプローチです。本手法は、SFT（Supervised Fine-Tuning）モデルと Base モデルの「差分」を活用することで、高品質な contrastive data を生成し、DPO（Direct Preference Optimization）の効果を最大化します。\n\n\nDelta Learning の核心は、モデル間の能力差を明示的に捉えることにあります。\n+------------------------------------------------------------------+\n|                      Delta Learning Concept                      |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model    --&gt;  Limited reasoning capability                |\n|  SFT Model     --&gt;  Enhanced reasoning capability                |\n|  Delta         --&gt;  The \"learned\" reasoning ability              |\n|                                                                  |\n|  Goal: Amplify the delta through preference optimization        |\n+------------------------------------------------------------------+\n\n\nSFT モデルは、Base モデルに対して以下の能力を獲得しています。\n\nより構造化された推論プロセス\n段階的な問題解決アプローチ\nタスク固有の知識の適用\n\nDelta Learning は、この「獲得された能力」を優先応答（Preferred response）の生成に活用します。\n\n\n\n\nDolci Think では、Delta Learning を用いて推論能力の向上を図ります（Section 4.3）。\n\n\n+------------------------------------------------------------------+\n|                  Dolci Think Data Generation                     |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Step 1: Sample question from training set                      |\n|  Step 2: Generate response using SFT model (Preferred)          |\n|  Step 3: Generate response using Base model (Dispreferred)      |\n|  Step 4: Apply quality filtering                                |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nPreferred 応答:\n\nDolci Think SFT モデルで生成\n段階的推論プロセスを含む\n最終的な正答に到達\n\nDispreferred 応答:\n\nOLMo2 7B Base モデルで生成\n推論の深さが不足\n誤った結論または不完全な推論\n\n\n\n\n生成されたペアに対して、以下の基準でフィルタリングを実施します。\n\nPreferred 応答が正答を含む\nDispreferred 応答が誤答または不完全\n両応答間に明確な品質差が存在\n\nこの結果、約 1M の高品質な preference pair が作成されました。\n\n\n\n\nDolci Instruct では、Delta Learning を multi-turn 対話の最適化に使用します（Section 5.3）。\n\n\n+------------------------------------------------------------------+\n|                Dolci Instruct Data Generation                    |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Source: Approximately 500K multi-turn prompts                   |\n|                                                                  |\n|  Preferred:                                                      |\n|    - Generated by Dolci Instruct SFT                             |\n|    - Concise, well-structured responses                          |\n|                                                                  |\n|  Dispreferred:                                                   |\n|    - Generated by OLMo2 7B Base                                  |\n|    - Verbose or poorly structured responses                      |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning により、以下の改善が実現されます。\n\n簡潔さの維持: 不要な冗長性を排除\n情報密度の向上: 重要な情報を効率的に伝達\n構造の改善: 論理的な流れを持つ応答\n\n\n\n\n約 500K の multi-turn プロンプトから preference pair を生成し、応答品質の向上を図ります。\n\n\n\n\nDelta Learning による Preference tuning は、複数の利点をもたらします。\n\n\nDPO による追加の最適化により、SFT 単独では到達できない性能レベルを実現します。\n+------------------------------------------------------------------+\n|                    Performance Progression                       |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model  --&gt;  SFT Model  --&gt;  DPO Model (with Delta)        |\n|                                                                  |\n|  Limited     --&gt;  Enhanced   --&gt;  Optimized reasoning            |\n|  reasoning        reasoning       and preference alignment       |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning による DPO は、将来の Reinforcement Learning の基盤となります。\n\n報酬モデルとの整合性: 人間の選好との alignment を改善\n探索の効率化: より良い初期方策を提供\n安定性の向上: RL 訓練の収束を促進\n\n\n\n\nDolci Think での適用により、以下の改善が確認されました。\n\n複雑な問題に対する段階的アプローチの強化\n推論の深さと正確性の向上\n推論フロンティアの拡大\n\n\n\n\n\n\n\nNote他の Preference Tuning 手法との比較\n\n\n\n従来の DPO:\n\n人間によるラベル付けデータを使用\nデータ収集のコストが高い\nスケールに限界がある\n\nRLHF (Reinforcement Learning from Human Feedback):\n\n報酬モデルの訓練が必要\n複雑な実装と調整が必要\n計算コストが高い\n\nDelta Learning の利点:\n\nスケーラビリティ: 合成データにより大規模な訓練が可能\nコスト効率: 人間のアノテーションが不要\n品質保証: モデル間の能力差により、明確な contrastive signal を生成\n柔軟性: 異なるタスクやドメインに容易に適用可能\n\nDelta Learning は、SFT で獲得した能力を最大限に活用し、効率的かつ効果的な preference tuning を実現します。\n\n\n\n\n\n\nDelta Learning は、OLMo2 3B の preference tuning において中心的な役割を果たします。\n主要なポイント:\n\nSFT モデルと Base モデルの差分を活用\n高品質な contrastive data を自動生成\n推論能力と応答品質の両面で性能向上\nスケーラブルで cost-effective な手法\n\nこの手法により、Dolci Think と Dolci Instruct は、それぞれの領域で最先端の性能を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#基本原理",
    "href": "ja/olmo-3/09-delta-learning.html#基本原理",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning の核心は、モデル間の能力差を明示的に捉えることにあります。\n+------------------------------------------------------------------+\n|                      Delta Learning Concept                      |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model    --&gt;  Limited reasoning capability                |\n|  SFT Model     --&gt;  Enhanced reasoning capability                |\n|  Delta         --&gt;  The \"learned\" reasoning ability              |\n|                                                                  |\n|  Goal: Amplify the delta through preference optimization        |\n+------------------------------------------------------------------+\n\n\nSFT モデルは、Base モデルに対して以下の能力を獲得しています。\n\nより構造化された推論プロセス\n段階的な問題解決アプローチ\nタスク固有の知識の適用\n\nDelta Learning は、この「獲得された能力」を優先応答（Preferred response）の生成に活用します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#dolci-think-dpo-での適用",
    "href": "ja/olmo-3/09-delta-learning.html#dolci-think-dpo-での適用",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Dolci Think では、Delta Learning を用いて推論能力の向上を図ります（Section 4.3）。\n\n\n+------------------------------------------------------------------+\n|                  Dolci Think Data Generation                     |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Step 1: Sample question from training set                      |\n|  Step 2: Generate response using SFT model (Preferred)          |\n|  Step 3: Generate response using Base model (Dispreferred)      |\n|  Step 4: Apply quality filtering                                |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nPreferred 応答:\n\nDolci Think SFT モデルで生成\n段階的推論プロセスを含む\n最終的な正答に到達\n\nDispreferred 応答:\n\nOLMo2 7B Base モデルで生成\n推論の深さが不足\n誤った結論または不完全な推論\n\n\n\n\n生成されたペアに対して、以下の基準でフィルタリングを実施します。\n\nPreferred 応答が正答を含む\nDispreferred 応答が誤答または不完全\n両応答間に明確な品質差が存在\n\nこの結果、約 1M の高品質な preference pair が作成されました。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#dolci-instruct-dpo-での適用",
    "href": "ja/olmo-3/09-delta-learning.html#dolci-instruct-dpo-での適用",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Dolci Instruct では、Delta Learning を multi-turn 対話の最適化に使用します（Section 5.3）。\n\n\n+------------------------------------------------------------------+\n|                Dolci Instruct Data Generation                    |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Source: Approximately 500K multi-turn prompts                   |\n|                                                                  |\n|  Preferred:                                                      |\n|    - Generated by Dolci Instruct SFT                             |\n|    - Concise, well-structured responses                          |\n|                                                                  |\n|  Dispreferred:                                                   |\n|    - Generated by OLMo2 7B Base                                  |\n|    - Verbose or poorly structured responses                      |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning により、以下の改善が実現されます。\n\n簡潔さの維持: 不要な冗長性を排除\n情報密度の向上: 重要な情報を効率的に伝達\n構造の改善: 論理的な流れを持つ応答\n\n\n\n\n約 500K の multi-turn プロンプトから preference pair を生成し、応答品質の向上を図ります。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#効果と利点",
    "href": "ja/olmo-3/09-delta-learning.html#効果と利点",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning による Preference tuning は、複数の利点をもたらします。\n\n\nDPO による追加の最適化により、SFT 単独では到達できない性能レベルを実現します。\n+------------------------------------------------------------------+\n|                    Performance Progression                       |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model  --&gt;  SFT Model  --&gt;  DPO Model (with Delta)        |\n|                                                                  |\n|  Limited     --&gt;  Enhanced   --&gt;  Optimized reasoning            |\n|  reasoning        reasoning       and preference alignment       |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning による DPO は、将来の Reinforcement Learning の基盤となります。\n\n報酬モデルとの整合性: 人間の選好との alignment を改善\n探索の効率化: より良い初期方策を提供\n安定性の向上: RL 訓練の収束を促進\n\n\n\n\nDolci Think での適用により、以下の改善が確認されました。\n\n複雑な問題に対する段階的アプローチの強化\n推論の深さと正確性の向上\n推論フロンティアの拡大\n\n\n\n\n\n\n\nNote他の Preference Tuning 手法との比較\n\n\n\n従来の DPO:\n\n人間によるラベル付けデータを使用\nデータ収集のコストが高い\nスケールに限界がある\n\nRLHF (Reinforcement Learning from Human Feedback):\n\n報酬モデルの訓練が必要\n複雑な実装と調整が必要\n計算コストが高い\n\nDelta Learning の利点:\n\nスケーラビリティ: 合成データにより大規模な訓練が可能\nコスト効率: 人間のアノテーションが不要\n品質保証: モデル間の能力差により、明確な contrastive signal を生成\n柔軟性: 異なるタスクやドメインに容易に適用可能\n\nDelta Learning は、SFT で獲得した能力を最大限に活用し、効率的かつ効果的な preference tuning を実現します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#まとめ",
    "href": "ja/olmo-3/09-delta-learning.html#まとめ",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning は、OLMo2 3B の preference tuning において中心的な役割を果たします。\n主要なポイント:\n\nSFT モデルと Base モデルの差分を活用\n高品質な contrastive data を自動生成\n推論能力と応答品質の両面で性能向上\nスケーラブルで cost-effective な手法\n\nこの手法により、Dolci Think と Dolci Instruct は、それぞれの領域で最先端の性能を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html",
    "href": "ja/olmo-3/07-data-mixing.html",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing（データミキシング）は、複数のデータソースを最適な比率で組み合わせ、モデルの性能を最大化する手法である。Dolma 3 では、9T トークンのデータプールから 6T トークンの訓練ミックスを構成するために、Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法を導入した。これらの手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n\n\n\n\n\nモデル訓練には、トークン予算という制約が存在する:\n\n計算コスト: 訓練に使用できるトークン数は、計算リソースによって制限される\n最適配分の必要性: 限られた予算内で、どのデータソースをどの程度含めるかを決定する必要がある\n多様性と品質のバランス: データの多様性を保ちながら、高品質なデータを優先する\n\n\n\n\n9T トークンのデータプールから 6T トークンの訓練ミックスを構成する際、以下の要素を考慮する:\n\nデータソースの特性: Web テキスト、学術 PDF、コード、数学など、各ソースの特徴\nトピックのバランス: STEM、ソフトウェア開発、一般知識など、トピックの最適な配分\n品質の考慮: 高品質な文書を優先的に選択\n\n\n\n\n\nToken-constrained Mixing は、トークン予算の制約下で最適なデータミックスを決定する手法である。\n\n\n小規模プロキシモデルを多数訓練し、その結果から最適なミックスを推定する:\n手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\n利点:\n\n計算効率: 小規模モデルでの実験により、大規模モデルの訓練前に最適なミックスを推定可能\n並列化: 複数のプロキシモデルを並列に訓練できる\n反復的改善: 結果に基づいて段階的にミックスを改善可能\n\n\n\n\n\n\n\nNoteSwarm の規模\n\n\n\nDolma 3 では、1B パラメータモデルを多数訓練し、異なるミキシング比率での性能を評価した。これらのプロキシモデルは、5x Chinchilla（通常の 5 倍のトークン数）で訓練され、データミックスの効果を正確に測定している。\n\n\n\n\n\nデータソースの継続的な改善に対応するため、条件付きミキシング手順を採用する:\n特徴:\n\n柔軟性: データソースが更新されても、ミックス全体を再計算する必要がない\nモジュール性: 個別のデータソースを独立して改善可能\nスケーラビリティ: 新しいデータソースの追加が容易\n\n開発サイクルへの対応:\n\nデータソースの継続的な改善\n新しいデータソースの段階的な導入\nミックス比率の動的な調整\n\n\n\n\n\nQuality-aware Upsampling は、重複排除後のクリーンなデータセットに対して、高品質文書を選択的に再導入する手法である。\n\n\n重複排除により削除されたデータの中から、高品質な文書を選択的に復元する:\nアプローチ:\n\n重複排除の基盤: まず、すべての重複を削除したクリーンなデータセットを構築\n品質評価: 各文書の品質スコアを計算\n選択的アップサンプリング: 高品質な文書を選択的に繰り返す\n\n効果:\n\n品質の向上: 高品質データの割合を増やすことで、モデルの性能を向上\n効率的な繰り返し: 全体的な繰り返しを最小限に抑えながら、高品質データに繰り返しを集中\nトークン効率: 限られたトークン予算を高品質データに優先配分\n\n\n\n\n\n\n\nTipQuality-aware Upsampling の戦略\n\n\n\n重複排除により削除された文書の中には、高品質なものも含まれる。これらを選択的に復元することで、重複排除による品質低下を防ぎつつ、データセット全体の品質を向上させる。\n\n\n\n\n\n\nDolma 3 では、Web テキストをトピックと品質の両方の軸で分類し、きめ細かなミキシングを実現している。\n\n\nWebOrganizer は、Web テキストを 24 の主要なトピックに分類するツールである:\n主要トピック（例）:\n\nScience, Math, and Technology\nSoftware Development\nArts and Entertainment\nBusiness and Finance\nHealth and Medicine\nEducation\nNews and Current Events\nその他 17 トピック\n\n分類の利点:\n\nトピックごとの重み付け: 各トピックに最適な重みを割り当てる\nSTEM の強化: Science, Math, Technology などのトピックを優先的に配分\nバランスの取れたミックス: 特定のトピックに偏らないように調整\n\n\n\n\n各トピック内で、品質スコアによりさらに分類する:\n品質分類:\n\n20 の品質階層: 各トピックを 20 の品質ティアに分割\nfastText ベースの分類器: 高速かつ正確な品質推定\n客観的な品質指標: 一貫性のある品質評価を実現\n\n\n\n\n24 トピック × 20 品質ティア = 480 個のサブセットに分割:\nきめ細かなミキシング:\n\nサブセットごとの重み: 各サブセットに個別の重みを割り当て\n品質とトピックの両立: 高品質かつ重要なトピックを優先\n柔軟な調整: 細かい粒度でのデータ配分の最適化\n\n┌──────────────────────────────────────────────────────────────┐\n│              Topic and Quality Classification                │\n├──────────────────────────────────────────────────────────────┤\n│  WebOrganizer (24 topics)                                    │\n│    ├─&gt; Science, Math, and Technology                         │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Software Development                                  │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Arts and Entertainment                                │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    └─&gt; ... (21 more topics)                                  │\n│          └─&gt; Quality tiers (1-20)                            │\n├──────────────────────────────────────────────────────────────┤\n│  Total: 480 subsets (24 x 20)                                │\n└──────────────────────────────────────────────────────────────┘\n\n\n\n\nToken-constrained Mixing と Quality-aware Upsampling により、データソースの最適な比率が決定された。\n\n\nWeb テキストのトピック分布において、以下の傾向が観察される:\n優先されたトピック:\n\nScience, Math, and Technology: STEM ドメインを大幅にアップウェイト\nSoftware Development: プログラミングとソフトウェア開発を強化\nEducation: 教育的コンテンツの重視\n\n抑制されたトピック:\n\nエンターテインメント関連のトピック\n一般的なニュースやソーシャルメディアコンテンツ\n\n結果:\n\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\n\n\n\nDCLM（DataComp for Language Models）Baseline と比較して、以下の改善が確認された:\n改善点:\n\nSTEM タスク: 科学、数学、技術関連のタスクで大幅な性能向上\nコーディングタスク: プログラミング能力の向上\n一般知識: 幅広い知識タスクでの性能改善\n\nトレードオフ:\n\n一部のタスクでは若干の性能低下\n全体として、重要なタスクでの性能向上が優先される\n\n\n\n\n\n\n\nImportantデータミキシングの影響\n\n\n\nデータミキシングの最適化は、モデルの性能に大きな影響を与える。STEM ドメインを優先することで、科学的・技術的タスクでの性能が向上し、Olmo 3 の強みとなっている。\n\n\n\n\n\nコードデータにおいても、プログラミング言語別の最適なミックスが決定された:\n優先された言語:\n\nPython: 最も高い重み付け（機械学習、データサイエンスでの重要性）\nJavaScript/TypeScript: Web 開発の主要言語\nC++/Rust: システムプログラミング言語\n\n抑制された言語:\n\nJava: 相対的に低い重み（冗長性の高いコードが多い）\nMarkdown: ドキュメントファイルの制限\n\n結果:\n\nほぼすべてのコーディングベンチマークで改善を達成\nPython 中心のタスクで特に顕著な改善\n\n\n\n\n\nData Mixing は、Dolma 3 の品質を決定する重要なプロセスである。Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n主な特徴:\n\nToken-constrained Mixing: Swarm-based methods による最適化\nQuality-aware Upsampling: 高品質データの選択的再導入\n480 個のサブセット: トピックと品質による細かい分類\n条件付きミキシング: データソースの継続的改善に対応\n実証された改善: DCLM Baseline と比較して平均 0.056 BPB の改善\n\nこれらの手法により、Dolma 3 は Olmo 3 Base モデルの高い性能を支える基盤となっている。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#概要",
    "href": "ja/olmo-3/07-data-mixing.html#概要",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing（データミキシング）は、複数のデータソースを最適な比率で組み合わせ、モデルの性能を最大化する手法である。Dolma 3 では、9T トークンのデータプールから 6T トークンの訓練ミックスを構成するために、Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法を導入した。これらの手法により、限られたトークン予算の下で最適なデータ配分を実現している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#データミキシングの目的",
    "href": "ja/olmo-3/07-data-mixing.html#データミキシングの目的",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "モデル訓練には、トークン予算という制約が存在する:\n\n計算コスト: 訓練に使用できるトークン数は、計算リソースによって制限される\n最適配分の必要性: 限られた予算内で、どのデータソースをどの程度含めるかを決定する必要がある\n多様性と品質のバランス: データの多様性を保ちながら、高品質なデータを優先する\n\n\n\n\n9T トークンのデータプールから 6T トークンの訓練ミックスを構成する際、以下の要素を考慮する:\n\nデータソースの特性: Web テキスト、学術 PDF、コード、数学など、各ソースの特徴\nトピックのバランス: STEM、ソフトウェア開発、一般知識など、トピックの最適な配分\n品質の考慮: 高品質な文書を優先的に選択",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#token-constrained-mixing",
    "href": "ja/olmo-3/07-data-mixing.html#token-constrained-mixing",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Token-constrained Mixing は、トークン予算の制約下で最適なデータミックスを決定する手法である。\n\n\n小規模プロキシモデルを多数訓練し、その結果から最適なミックスを推定する:\n手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\n利点:\n\n計算効率: 小規模モデルでの実験により、大規模モデルの訓練前に最適なミックスを推定可能\n並列化: 複数のプロキシモデルを並列に訓練できる\n反復的改善: 結果に基づいて段階的にミックスを改善可能\n\n\n\n\n\n\n\nNoteSwarm の規模\n\n\n\nDolma 3 では、1B パラメータモデルを多数訓練し、異なるミキシング比率での性能を評価した。これらのプロキシモデルは、5x Chinchilla（通常の 5 倍のトークン数）で訓練され、データミックスの効果を正確に測定している。\n\n\n\n\n\nデータソースの継続的な改善に対応するため、条件付きミキシング手順を採用する:\n特徴:\n\n柔軟性: データソースが更新されても、ミックス全体を再計算する必要がない\nモジュール性: 個別のデータソースを独立して改善可能\nスケーラビリティ: 新しいデータソースの追加が容易\n\n開発サイクルへの対応:\n\nデータソースの継続的な改善\n新しいデータソースの段階的な導入\nミックス比率の動的な調整",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#quality-aware-upsampling",
    "href": "ja/olmo-3/07-data-mixing.html#quality-aware-upsampling",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Quality-aware Upsampling は、重複排除後のクリーンなデータセットに対して、高品質文書を選択的に再導入する手法である。\n\n\n重複排除により削除されたデータの中から、高品質な文書を選択的に復元する:\nアプローチ:\n\n重複排除の基盤: まず、すべての重複を削除したクリーンなデータセットを構築\n品質評価: 各文書の品質スコアを計算\n選択的アップサンプリング: 高品質な文書を選択的に繰り返す\n\n効果:\n\n品質の向上: 高品質データの割合を増やすことで、モデルの性能を向上\n効率的な繰り返し: 全体的な繰り返しを最小限に抑えながら、高品質データに繰り返しを集中\nトークン効率: 限られたトークン予算を高品質データに優先配分\n\n\n\n\n\n\n\nTipQuality-aware Upsampling の戦略\n\n\n\n重複排除により削除された文書の中には、高品質なものも含まれる。これらを選択的に復元することで、重複排除による品質低下を防ぎつつ、データセット全体の品質を向上させる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#topic-と-quality-による分類",
    "href": "ja/olmo-3/07-data-mixing.html#topic-と-quality-による分類",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Dolma 3 では、Web テキストをトピックと品質の両方の軸で分類し、きめ細かなミキシングを実現している。\n\n\nWebOrganizer は、Web テキストを 24 の主要なトピックに分類するツールである:\n主要トピック（例）:\n\nScience, Math, and Technology\nSoftware Development\nArts and Entertainment\nBusiness and Finance\nHealth and Medicine\nEducation\nNews and Current Events\nその他 17 トピック\n\n分類の利点:\n\nトピックごとの重み付け: 各トピックに最適な重みを割り当てる\nSTEM の強化: Science, Math, Technology などのトピックを優先的に配分\nバランスの取れたミックス: 特定のトピックに偏らないように調整\n\n\n\n\n各トピック内で、品質スコアによりさらに分類する:\n品質分類:\n\n20 の品質階層: 各トピックを 20 の品質ティアに分割\nfastText ベースの分類器: 高速かつ正確な品質推定\n客観的な品質指標: 一貫性のある品質評価を実現\n\n\n\n\n24 トピック × 20 品質ティア = 480 個のサブセットに分割:\nきめ細かなミキシング:\n\nサブセットごとの重み: 各サブセットに個別の重みを割り当て\n品質とトピックの両立: 高品質かつ重要なトピックを優先\n柔軟な調整: 細かい粒度でのデータ配分の最適化\n\n┌──────────────────────────────────────────────────────────────┐\n│              Topic and Quality Classification                │\n├──────────────────────────────────────────────────────────────┤\n│  WebOrganizer (24 topics)                                    │\n│    ├─&gt; Science, Math, and Technology                         │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Software Development                                  │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Arts and Entertainment                                │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    └─&gt; ... (21 more topics)                                  │\n│          └─&gt; Quality tiers (1-20)                            │\n├──────────────────────────────────────────────────────────────┤\n│  Total: 480 subsets (24 x 20)                                │\n└──────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#混合戦略の結果",
    "href": "ja/olmo-3/07-data-mixing.html#混合戦略の結果",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Token-constrained Mixing と Quality-aware Upsampling により、データソースの最適な比率が決定された。\n\n\nWeb テキストのトピック分布において、以下の傾向が観察される:\n優先されたトピック:\n\nScience, Math, and Technology: STEM ドメインを大幅にアップウェイト\nSoftware Development: プログラミングとソフトウェア開発を強化\nEducation: 教育的コンテンツの重視\n\n抑制されたトピック:\n\nエンターテインメント関連のトピック\n一般的なニュースやソーシャルメディアコンテンツ\n\n結果:\n\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\n\n\n\nDCLM（DataComp for Language Models）Baseline と比較して、以下の改善が確認された:\n改善点:\n\nSTEM タスク: 科学、数学、技術関連のタスクで大幅な性能向上\nコーディングタスク: プログラミング能力の向上\n一般知識: 幅広い知識タスクでの性能改善\n\nトレードオフ:\n\n一部のタスクでは若干の性能低下\n全体として、重要なタスクでの性能向上が優先される\n\n\n\n\n\n\n\nImportantデータミキシングの影響\n\n\n\nデータミキシングの最適化は、モデルの性能に大きな影響を与える。STEM ドメインを優先することで、科学的・技術的タスクでの性能が向上し、Olmo 3 の強みとなっている。\n\n\n\n\n\nコードデータにおいても、プログラミング言語別の最適なミックスが決定された:\n優先された言語:\n\nPython: 最も高い重み付け（機械学習、データサイエンスでの重要性）\nJavaScript/TypeScript: Web 開発の主要言語\nC++/Rust: システムプログラミング言語\n\n抑制された言語:\n\nJava: 相対的に低い重み（冗長性の高いコードが多い）\nMarkdown: ドキュメントファイルの制限\n\n結果:\n\nほぼすべてのコーディングベンチマークで改善を達成\nPython 中心のタスクで特に顕著な改善",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#まとめ",
    "href": "ja/olmo-3/07-data-mixing.html#まとめ",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing は、Dolma 3 の品質を決定する重要なプロセスである。Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n主な特徴:\n\nToken-constrained Mixing: Swarm-based methods による最適化\nQuality-aware Upsampling: 高品質データの選択的再導入\n480 個のサブセット: トピックと品質による細かい分類\n条件付きミキシング: データソースの継続的改善に対応\n実証された改善: DCLM Baseline と比較して平均 0.056 BPB の改善\n\nこれらの手法により、Dolma 3 は Olmo 3 Base モデルの高い性能を支える基盤となっている。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html",
    "href": "ja/olmo-3/05-deduplication.html",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Deduplication（重複排除）は、Dolma 3 データセットのキュレーションにおける中核的なプロセスである。兆トークンスケールのデータセットにおいて、重複したコンテンツを効率的に識別・削除することで、訓練の効率性と品質を向上させる。Dolma 3 では、3 段階の重複排除戦略を採用し、38.7B 文書を 9.7B 文書まで削減した（文書数で 75% 削減）。\n\n\n\n\n\n重複排除の主な目的は、トークン効率的な訓練を実現することである:\n\n計算コストの削減: 同一または類似のコンテンツを複数回訓練することは、計算リソースの無駄遣いである\nメモリ効率: 重複データを削除することで、より多様なデータをメモリに保持可能\n訓練時間の短縮: 冗長なデータを排除し、より効率的な訓練サイクルを実現\n\n\n\n\n重複の頻度は、コンテンツの品質を示す弱いシグナルとして機能する:\n\n高品質コンテンツ: 多くの場合、一度だけ出現する（オリジナルコンテンツ）\n低品質コンテンツ: スパム、ボイラープレートテキスト、テンプレート化されたコンテンツは複数のサイトで繰り返される傾向がある\nWeb スクレイピングの副産物: 同一コンテンツが複数のドメインにコピーされる現象（ミラーサイトなど）\n\n\n\n\n同一のコンテンツを複数回訓練することは、収穫逓減の法則に従う:\n\n1 回目: モデルが新しいパターンと知識を学習\n2 回目: 追加的な学習効果が減少\n3 回目以降: ほとんど追加的な利益がなく、過学習のリスクが増加\n\n\n\n\n\nDolma 3 では、異なる粒度での重複を対象とした 3 つの段階を組み合わせている。\n\n\n目的: 完全に同一の文書を識別・削除\n手法:\n\n文書全体のテキストハッシュ（SHA-256 など）を計算\nハッシュが完全に一致する文書を重複として識別\nグローバル重複排除: すべてのデータソース間で実施\n\n結果:\n\n削減率: 67% のデータを重複として識別\n文書数: 38.7B 文書から 12.8B 文書に削減\n対象: 完全コピー、ミラーサイト、クローラーの重複取得\n\n\n\n\n\n\n\nNoteExact Deduplication の効率性\n\n\n\n完全重複排除は計算コストが低く、ハッシュベースの実装により大規模データセットでも高速に動作する。この段階だけで文書数の 2/3 以上を削減できることは、Web データの重複度の高さを示している。\n\n\n\n\n\n目的: ほぼ同一の文書（ヘッダーやフッターのみが異なる文書）を識別・削除\n手法: MinHash ベースのアルゴリズム\nMinHash は、文書間の Jaccard 類似度を効率的に推定する手法である:\n\nShingling: 文書を n-gram（通常は 5-gram や 13-gram）に分割\nMinHash 署名: 各文書に対して固定長の署名を生成\nLSH (Locality-Sensitive Hashing): 類似した署名を持つ文書ペアを効率的に発見\nクラスタリング: 類似度が閾値を超える文書をクラスタ化し、各クラスタから 1 つのみを保持\n\n対象となる重複:\n\n異なるドメイン間でコピーされた文書: ニュース記事、ブログ投稿など\nテンプレートベースのコンテンツ: 同一のヘッダー/フッターを持つ文書\n軽微な編集が加えられたコンテンツ: 日付や名前のみが異なるバージョン\n\n結果:\n\n削減率: 23% のデータを重複として識別\n文書数: 12.8B 文書から 9.8B 文書に削減\n\n\n\n\n\n\n\nTipMinHash の効率性\n\n\n\nMinHash は、文書間の完全な比較（O(n^2) の計算量）を回避し、LSH により O(n) に近い計算量で類似文書を発見できる。これにより、数十億規模の文書に対しても実用的な時間で重複排除が可能になる。\n\n\n\n\n\n目的: 個別文書内の繰り返しコンテンツ（ボイラープレートテキスト、HTML アーティファクト）を削除\n手法: Fuzzy suffix-array ベースのアルゴリズム\nSuffix array は、文字列の全ての接尾辞を辞書順にソートしたデータ構造である:\n\nSuffix array 構築: 各文書に対して suffix array を構築\n繰り返し検出: 500 バイト以上の繰り返し部分文字列を識別\nマーキングと削除: 繰り返し部分をマークし、訓練データから除外\n\n対象となる重複:\n\nボイラープレートテキスト: ナビゲーションメニュー、フッター、サイドバー\nHTML アーティファクト: スクリプトタグ、スタイル定義の残骸\n繰り返しパターン: リスト項目、テーブルデータの反復\n\n結果:\n\n削減率: 14% のテキストバイトを削除\n文書数: 9.8B 文書から 9.7B 文書に削減（文書数自体はほぼ維持）\nデータサイズ: 最終的に 36.5T バイトに削減\n\n\n\n\n\n\n\nImportantStage 3 の重要性\n\n\n\nStage 3 は文書数をほとんど削減しないが、各文書の品質を大幅に向上させる。ボイラープレートテキストや HTML アーティファクトは、モデルが学習すべき有用な情報をほとんど含まないため、これらを削除することで訓練の効率性が向上する。\n\n\n\n\n\n\nDolma 3 の重複排除を実現するために、Duplodocus という新しいツールを開発した。\n\n\nNative Rust 実装:\n\n高性能: メモリ安全性を保ちつつ、C/C++ に匹敵する実行速度を実現\n並列処理: Rayon などの Rust のエコシステムを活用した効率的な並列化\nメモリ効率: 所有権システムにより、メモリリークやデータ競合を防止\n\n大規模分散実行:\n\nスケーラビリティ: 数十億規模の文書を処理可能\n分散ハッシュテーブル: 複数ノード間でハッシュテーブルを分散し、メモリ制約を緩和\nストリーミング処理: 全データをメモリに読み込むことなく、ストリーミング方式で処理\n\n統合された機能:\n\nHash-based exact deduplication: SHA-256 ハッシュによる完全重複排除\nMinHash fuzzy deduplication: Jaccard 類似度ベースの曖昧重複排除\nカスタマイズ可能: パラメータ（n-gram サイズ、類似度閾値など）を柔軟に調整可能\n\n\n\n\n┌──────────────────────────────────────────────────────────────┐\n│                   Duplodocus Workflow                        │\n├──────────────────────────────────────────────────────────────┤\n│  Input: 38.7B documents                                      │\n│    |                                                          │\n│    v                                                          │\n│  Stage 1: Hash-based exact deduplication                     │\n│    | - Compute SHA-256 hash for each document                │\n│    | - Remove duplicates                                     │\n│    | - Output: 12.8B documents (67% reduction)               │\n│    |                                                          │\n│    v                                                          │\n│  Stage 2: MinHash fuzzy deduplication                        │\n│    | - Generate MinHash signatures                           │\n│    | - LSH for candidate pairs                               │\n│    | - Cluster similar documents                             │\n│    | - Output: 9.8B documents (23% reduction)                │\n│    |                                                          │\n│    v                                                          │\n│  Stage 3: Suffix array substring deduplication               │\n│    | - Build suffix arrays                                   │\n│    | - Detect repeated substrings (&gt;=500 bytes)              │\n│    | - Mark and remove                                       │\n│    | - Output: 9.7B documents (14% byte reduction)           │\n│    |                                                          │\n│    v                                                          │\n│  Output: 9.7B documents, 36.5T bytes                         │\n└──────────────────────────────────────────────────────────────┘\n\n\n\n\n3 段階の重複排除プロセスにより、以下の結果を達成した。\n\n\n\n\n\n指標\n初期値\n最終値\n削減率\n\n\n\n\n文書数\n38.7B\n9.7B\n75%\n\n\nデータサイズ\n-\n36.5T バイト\n-\n\n\n\n段階別の削減率:\n\nStage 1 (Exact): 67% の文書を削除\nStage 2 (Fuzzy): 残りの 23% を削除\nStage 3 (Substring): 14% のバイトを削除\n\n\n\n\n重複排除されたデータは、Quality-aware Upsampling の基盤として機能する:\n\nクリーンなデータプール: 重複が排除された 9.7B 文書から高品質文書を選択\n品質スコアの信頼性向上: 重複データが品質分類に与えるノイズを削減\n効率的な繰り返し: 高品質文書のみを選択的に繰り返すことで、全体的な繰り返しを最小限に抑制\n\n重複排除により、Dolma 3 Mix は以下の最適化が可能になった:\n\nトピック分類の精度向上: 重複データのノイズを削減\n品質スコアの信頼性: より正確な品質評価が可能\nアップサンプリングの効率化: 高品質データの選択的な繰り返し\n\n詳細は ?@sec-quality-upsampling を参照。\n\n\n\n\n\n\n\n\n\n\nNote他のデータセットの重複排除手法\n\n\n\n\n\nC4 (Colossal Clean Crawled Corpus):\n\n主に exact deduplication に依存\nFuzzy deduplication は限定的\n結果: より多くの近似重複が残る可能性\n\nThe Pile:\n\nデータソースごとに異なる重複排除戦略\n一部のソースは重複排除なし\n結果: データソース間の一貫性が低い\n\nRedPajama:\n\nMinHash ベースの fuzzy deduplication を採用\nSubstring deduplication は実施せず\n結果: 文書内のボイラープレートが残る\n\nDolma 3 の優位性:\n\n3 段階の包括的アプローチ: Exact、Fuzzy、Substring の全てを実施\nグローバル重複排除: すべてのデータソース間で統一的に実施\nスケーラビリティ: Duplodocus により兆トークンスケールで高速実行\nオープン性: ツールとパイプラインを完全公開\n\n\n\n\n\n\n\nDolma 3 の重複排除は、以下の特徴を持つ:\n主な成果:\n\n大規模削減: 38.7B 文書から 9.7B 文書へ（75% 削減）\n3 段階の戦略: Exact、Fuzzy、Substring の包括的アプローチ\n高性能ツール: Duplodocus による効率的な大規模処理\n\n技術的革新:\n\nNative Rust 実装による高性能\n分散実行によるスケーラビリティ\nSuffix array ベースの substring deduplication\n\n下流への影響:\n\nQuality-aware upsampling の基盤を提供\n訓練効率の大幅な向上\nモデル品質の改善\n\n重複排除は、Dolma 3 データセットの品質と効率性を実現する上で不可欠なプロセスであり、Olmo 3 モデルの成功に大きく貢献している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#概要",
    "href": "ja/olmo-3/05-deduplication.html#概要",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Deduplication（重複排除）は、Dolma 3 データセットのキュレーションにおける中核的なプロセスである。兆トークンスケールのデータセットにおいて、重複したコンテンツを効率的に識別・削除することで、訓練の効率性と品質を向上させる。Dolma 3 では、3 段階の重複排除戦略を採用し、38.7B 文書を 9.7B 文書まで削減した（文書数で 75% 削減）。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#重複排除の目的と動機",
    "href": "ja/olmo-3/05-deduplication.html#重複排除の目的と動機",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "重複排除の主な目的は、トークン効率的な訓練を実現することである:\n\n計算コストの削減: 同一または類似のコンテンツを複数回訓練することは、計算リソースの無駄遣いである\nメモリ効率: 重複データを削除することで、より多様なデータをメモリに保持可能\n訓練時間の短縮: 冗長なデータを排除し、より効率的な訓練サイクルを実現\n\n\n\n\n重複の頻度は、コンテンツの品質を示す弱いシグナルとして機能する:\n\n高品質コンテンツ: 多くの場合、一度だけ出現する（オリジナルコンテンツ）\n低品質コンテンツ: スパム、ボイラープレートテキスト、テンプレート化されたコンテンツは複数のサイトで繰り返される傾向がある\nWeb スクレイピングの副産物: 同一コンテンツが複数のドメインにコピーされる現象（ミラーサイトなど）\n\n\n\n\n同一のコンテンツを複数回訓練することは、収穫逓減の法則に従う:\n\n1 回目: モデルが新しいパターンと知識を学習\n2 回目: 追加的な学習効果が減少\n3 回目以降: ほとんど追加的な利益がなく、過学習のリスクが増加",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#段階の重複排除戦略",
    "href": "ja/olmo-3/05-deduplication.html#段階の重複排除戦略",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 では、異なる粒度での重複を対象とした 3 つの段階を組み合わせている。\n\n\n目的: 完全に同一の文書を識別・削除\n手法:\n\n文書全体のテキストハッシュ（SHA-256 など）を計算\nハッシュが完全に一致する文書を重複として識別\nグローバル重複排除: すべてのデータソース間で実施\n\n結果:\n\n削減率: 67% のデータを重複として識別\n文書数: 38.7B 文書から 12.8B 文書に削減\n対象: 完全コピー、ミラーサイト、クローラーの重複取得\n\n\n\n\n\n\n\nNoteExact Deduplication の効率性\n\n\n\n完全重複排除は計算コストが低く、ハッシュベースの実装により大規模データセットでも高速に動作する。この段階だけで文書数の 2/3 以上を削減できることは、Web データの重複度の高さを示している。\n\n\n\n\n\n目的: ほぼ同一の文書（ヘッダーやフッターのみが異なる文書）を識別・削除\n手法: MinHash ベースのアルゴリズム\nMinHash は、文書間の Jaccard 類似度を効率的に推定する手法である:\n\nShingling: 文書を n-gram（通常は 5-gram や 13-gram）に分割\nMinHash 署名: 各文書に対して固定長の署名を生成\nLSH (Locality-Sensitive Hashing): 類似した署名を持つ文書ペアを効率的に発見\nクラスタリング: 類似度が閾値を超える文書をクラスタ化し、各クラスタから 1 つのみを保持\n\n対象となる重複:\n\n異なるドメイン間でコピーされた文書: ニュース記事、ブログ投稿など\nテンプレートベースのコンテンツ: 同一のヘッダー/フッターを持つ文書\n軽微な編集が加えられたコンテンツ: 日付や名前のみが異なるバージョン\n\n結果:\n\n削減率: 23% のデータを重複として識別\n文書数: 12.8B 文書から 9.8B 文書に削減\n\n\n\n\n\n\n\nTipMinHash の効率性\n\n\n\nMinHash は、文書間の完全な比較（O(n^2) の計算量）を回避し、LSH により O(n) に近い計算量で類似文書を発見できる。これにより、数十億規模の文書に対しても実用的な時間で重複排除が可能になる。\n\n\n\n\n\n目的: 個別文書内の繰り返しコンテンツ（ボイラープレートテキスト、HTML アーティファクト）を削除\n手法: Fuzzy suffix-array ベースのアルゴリズム\nSuffix array は、文字列の全ての接尾辞を辞書順にソートしたデータ構造である:\n\nSuffix array 構築: 各文書に対して suffix array を構築\n繰り返し検出: 500 バイト以上の繰り返し部分文字列を識別\nマーキングと削除: 繰り返し部分をマークし、訓練データから除外\n\n対象となる重複:\n\nボイラープレートテキスト: ナビゲーションメニュー、フッター、サイドバー\nHTML アーティファクト: スクリプトタグ、スタイル定義の残骸\n繰り返しパターン: リスト項目、テーブルデータの反復\n\n結果:\n\n削減率: 14% のテキストバイトを削除\n文書数: 9.8B 文書から 9.7B 文書に削減（文書数自体はほぼ維持）\nデータサイズ: 最終的に 36.5T バイトに削減\n\n\n\n\n\n\n\nImportantStage 3 の重要性\n\n\n\nStage 3 は文書数をほとんど削減しないが、各文書の品質を大幅に向上させる。ボイラープレートテキストや HTML アーティファクトは、モデルが学習すべき有用な情報をほとんど含まないため、これらを削除することで訓練の効率性が向上する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#duplodocus-ツール",
    "href": "ja/olmo-3/05-deduplication.html#duplodocus-ツール",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 の重複排除を実現するために、Duplodocus という新しいツールを開発した。\n\n\nNative Rust 実装:\n\n高性能: メモリ安全性を保ちつつ、C/C++ に匹敵する実行速度を実現\n並列処理: Rayon などの Rust のエコシステムを活用した効率的な並列化\nメモリ効率: 所有権システムにより、メモリリークやデータ競合を防止\n\n大規模分散実行:\n\nスケーラビリティ: 数十億規模の文書を処理可能\n分散ハッシュテーブル: 複数ノード間でハッシュテーブルを分散し、メモリ制約を緩和\nストリーミング処理: 全データをメモリに読み込むことなく、ストリーミング方式で処理\n\n統合された機能:\n\nHash-based exact deduplication: SHA-256 ハッシュによる完全重複排除\nMinHash fuzzy deduplication: Jaccard 類似度ベースの曖昧重複排除\nカスタマイズ可能: パラメータ（n-gram サイズ、類似度閾値など）を柔軟に調整可能\n\n\n\n\n┌──────────────────────────────────────────────────────────────┐\n│                   Duplodocus Workflow                        │\n├──────────────────────────────────────────────────────────────┤\n│  Input: 38.7B documents                                      │\n│    |                                                          │\n│    v                                                          │\n│  Stage 1: Hash-based exact deduplication                     │\n│    | - Compute SHA-256 hash for each document                │\n│    | - Remove duplicates                                     │\n│    | - Output: 12.8B documents (67% reduction)               │\n│    |                                                          │\n│    v                                                          │\n│  Stage 2: MinHash fuzzy deduplication                        │\n│    | - Generate MinHash signatures                           │\n│    | - LSH for candidate pairs                               │\n│    | - Cluster similar documents                             │\n│    | - Output: 9.8B documents (23% reduction)                │\n│    |                                                          │\n│    v                                                          │\n│  Stage 3: Suffix array substring deduplication               │\n│    | - Build suffix arrays                                   │\n│    | - Detect repeated substrings (&gt;=500 bytes)              │\n│    | - Mark and remove                                       │\n│    | - Output: 9.7B documents (14% byte reduction)           │\n│    |                                                          │\n│    v                                                          │\n│  Output: 9.7B documents, 36.5T bytes                         │\n└──────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#最終結果",
    "href": "ja/olmo-3/05-deduplication.html#最終結果",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "3 段階の重複排除プロセスにより、以下の結果を達成した。\n\n\n\n\n\n指標\n初期値\n最終値\n削減率\n\n\n\n\n文書数\n38.7B\n9.7B\n75%\n\n\nデータサイズ\n-\n36.5T バイト\n-\n\n\n\n段階別の削減率:\n\nStage 1 (Exact): 67% の文書を削除\nStage 2 (Fuzzy): 残りの 23% を削除\nStage 3 (Substring): 14% のバイトを削除\n\n\n\n\n重複排除されたデータは、Quality-aware Upsampling の基盤として機能する:\n\nクリーンなデータプール: 重複が排除された 9.7B 文書から高品質文書を選択\n品質スコアの信頼性向上: 重複データが品質分類に与えるノイズを削減\n効率的な繰り返し: 高品質文書のみを選択的に繰り返すことで、全体的な繰り返しを最小限に抑制\n\n重複排除により、Dolma 3 Mix は以下の最適化が可能になった:\n\nトピック分類の精度向上: 重複データのノイズを削減\n品質スコアの信頼性: より正確な品質評価が可能\nアップサンプリングの効率化: 高品質データの選択的な繰り返し\n\n詳細は ?@sec-quality-upsampling を参照。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#他の重複排除との比較",
    "href": "ja/olmo-3/05-deduplication.html#他の重複排除との比較",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Note他のデータセットの重複排除手法\n\n\n\n\n\nC4 (Colossal Clean Crawled Corpus):\n\n主に exact deduplication に依存\nFuzzy deduplication は限定的\n結果: より多くの近似重複が残る可能性\n\nThe Pile:\n\nデータソースごとに異なる重複排除戦略\n一部のソースは重複排除なし\n結果: データソース間の一貫性が低い\n\nRedPajama:\n\nMinHash ベースの fuzzy deduplication を採用\nSubstring deduplication は実施せず\n結果: 文書内のボイラープレートが残る\n\nDolma 3 の優位性:\n\n3 段階の包括的アプローチ: Exact、Fuzzy、Substring の全てを実施\nグローバル重複排除: すべてのデータソース間で統一的に実施\nスケーラビリティ: Duplodocus により兆トークンスケールで高速実行\nオープン性: ツールとパイプラインを完全公開",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#まとめ",
    "href": "ja/olmo-3/05-deduplication.html#まとめ",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 の重複排除は、以下の特徴を持つ:\n主な成果:\n\n大規模削減: 38.7B 文書から 9.7B 文書へ（75% 削減）\n3 段階の戦略: Exact、Fuzzy、Substring の包括的アプローチ\n高性能ツール: Duplodocus による効率的な大規模処理\n\n技術的革新:\n\nNative Rust 実装による高性能\n分散実行によるスケーラビリティ\nSuffix array ベースの substring deduplication\n\n下流への影響:\n\nQuality-aware upsampling の基盤を提供\n訓練効率の大幅な向上\nモデル品質の改善\n\n重複排除は、Dolma 3 データセットの品質と効率性を実現する上で不可欠なプロセスであり、Olmo 3 モデルの成功に大きく貢献している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html",
    "href": "ja/olmo-3/03-midtraining.html",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "OLMo 3 の訓練は、事前学習後に Midtraining（中間訓練）という追加ステージを経る。このフェーズでは 100B 個の高品質トークンを使用して、数学推論、コード生成、質問応答、指示追従、推論思考などの重要な能力を強化する。\n\n\nMidtraining は事前学習と後続の SFT（Supervised Fine-Tuning）の橋渡しを行う。データセットとして Dolma 3 Dolmino Mix を使用し、以下の特徴を持つ。\n\n100B トークンの高品質データ\nターゲット能力に特化したデータソースの選定\nDecontamination による評価データセットの汚染除去\nMicroanneal と Integration tests による効果的なデータミックス設計\n\n\n\n\nMidtraining のデータキュレーションは 2部構成のフレームワーク（Figure 11）で行われる。\n+-----------------------+     +-------------------------+\n| Distributed           |     | Centralized             |\n| Exploration           |     | Assessment              |\n+-----------------------+     +-------------------------+\n| - Individual data     | --&gt; | - Combine candidate     |\n|   source testing      |     |   datasets              |\n| - Lightweight         |     | - Full 100B integration |\n|   feedback loops      |     |   tests                 |\n| - Microanneal (10B)   |     | - Post-SFT evaluation   |\n+-----------------------+     +-------------------------+\n\n\n各データソースについて、軽量なフィードバックループで効果を評価する。\n\nMicroanneal: 5B トークンのターゲットデータ + 5B Web データ\nBaseline: Web のみの 10B トークン\n迅速な評価により、有望なデータソースを特定\n\n\n\n\n選定された候補データセットを組み合わせて統合テストを実施。\n\nIntegration tests: 100B トークンの完全な annealing run\nデータソース間の相互作用を評価\nSFT 訓練後の性能も測定\n\n\n\n\n\nTable 5 に示される Dolmino Mix は、以下のターゲット能力ごとにデータソースを構成している。\n\n\n\n\n\n\n\n\n\nCapability\nDataset\nToken Count\nDescription\n\n\n\n\nMath\nTinyMATH\n~5B\nMath problem-solution pairs\n\n\n\nCraneMath\n~3B\nMathematical reasoning\n\n\n\nMegaMatt\n~2B\nAdvanced mathematics\n\n\n\nDolmino Math\n~4B\nCurated math corpus\n\n\nCode\nStack-Edu (FIM)\n~10B\nEducational code with Fill-In-Middle\n\n\n\nCraneCode\n~5B\nHigh-quality code snippets\n\n\nQA\nReddit-to-Flashcards\n~3B\nQuestion-answer extraction\n\n\n\nWiki-to-RCQA\n~4B\nReading comprehension QA\n\n\n\nNemotron\n~2B\nSynthetic QA pairs\n\n\nInstruction\nTulu3 SFT\n~2B\nInstruction-following examples\n\n\n\nFlan\n~3B\nTask-oriented instructions\n\n\nThinking\nMeta-reasoning\n~2B\nChain-of-thought reasoning\n\n\n\nProgram-verifiable\n~1B\nVerifiable reasoning traces\n\n\n\nOMR rewrite\n~1B\nReasoning rewriting\n\n\nWeb\nDolma v1.7 Web\n~50B\nGeneral web content (baseline)\n\n\n\n\n\n\n\n\n\nNoteDolmino Mix の設計方針\n\n\n\n各能力ごとに複数のデータソースを組み合わせることで、単一データセットに依存せず、能力の汎化性能を向上させる。\n\n\n\n\n\n各ターゲット能力での改善結果（Section 3.5.2）。\n\n\n\nTinyMATH: 基本的な算術・代数問題\nCraneMath: 複雑な数式処理と証明\nMegaMatt: 大学レベルの数学問題\nDolmino Math: 上記を統合したキュレーションコーパス\n\n\n\n\n\nStack-Edu (FIM): Fill-In-Middle 形式での教育的コード\nCraneCode: 高品質なコードスニペット（複数言語）\n\n\n\n\n\n\n\nTipFill-In-Middle (FIM)\n\n\n\nコードの中間部分を予測するタスク。実際の IDE での補完シナリオに近い。\n\n\n\n\n\n\nReddit-to-Flashcards: Reddit の議論から QA ペアを抽出\nWiki-to-RCQA: Wikipedia 記事から読解問題を生成\nNemotron: 合成 QA データセット\n\n\n\n\n\nTulu3 SFT: 多様な指示追従タスク\nFlan: タスク指向の指示データ\n\n\n\n\n\nMeta-reasoning: Chain-of-Thought (CoT) スタイルの推論\nProgram-verifiable: プログラムで検証可能な推論トレース\nOMR rewrite: 推論プロセスのリライト\n\n\n\n\n\nSection 3.5.3 で詳述されている decontamination（汚染除去）プロセス。\n新しい decon パッケージ を開発し、評価データセットとの重複を除去。\n\nn-gram ベースのマッチング\n評価ベンチマークの汚染検出\n訓練データからの除外処理\n\n\n\n\n\n\n\nWarning評価データの汚染リスク\n\n\n\n高品質データセットには、評価ベンチマークと重複するサンプルが含まれる可能性がある。Decontamination により公平な評価を保証する。\n\n\n\n\n\nSection 3.5.4 の主要な発見。\n\nMicroanneal の有効性: 10B トークンの軽量テストで、100B の完全 run の結果を予測可能\nデータソースの相補性: 複数データソースの組み合わせが単一データセット以上の効果\nSFT との相性: Midtraining で強化された能力は、SFT 後もさらに向上\nDecontamination の必要性: 汚染除去により評価精度が大幅に改善\n\n\n\n\n\n03-pretraining.qmd: Stage 1 の事前学習\n04-sft.qmd: Stage 3 の SFT 訓練\n05-data-dolma3.qmd: Dolma 3 データセット全体",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#概要",
    "href": "ja/olmo-3/03-midtraining.html#概要",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Midtraining は事前学習と後続の SFT（Supervised Fine-Tuning）の橋渡しを行う。データセットとして Dolma 3 Dolmino Mix を使用し、以下の特徴を持つ。\n\n100B トークンの高品質データ\nターゲット能力に特化したデータソースの選定\nDecontamination による評価データセットの汚染除去\nMicroanneal と Integration tests による効果的なデータミックス設計",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#methodological-framework",
    "href": "ja/olmo-3/03-midtraining.html#methodological-framework",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Midtraining のデータキュレーションは 2部構成のフレームワーク（Figure 11）で行われる。\n+-----------------------+     +-------------------------+\n| Distributed           |     | Centralized             |\n| Exploration           |     | Assessment              |\n+-----------------------+     +-------------------------+\n| - Individual data     | --&gt; | - Combine candidate     |\n|   source testing      |     |   datasets              |\n| - Lightweight         |     | - Full 100B integration |\n|   feedback loops      |     |   tests                 |\n| - Microanneal (10B)   |     | - Post-SFT evaluation   |\n+-----------------------+     +-------------------------+\n\n\n各データソースについて、軽量なフィードバックループで効果を評価する。\n\nMicroanneal: 5B トークンのターゲットデータ + 5B Web データ\nBaseline: Web のみの 10B トークン\n迅速な評価により、有望なデータソースを特定\n\n\n\n\n選定された候補データセットを組み合わせて統合テストを実施。\n\nIntegration tests: 100B トークンの完全な annealing run\nデータソース間の相互作用を評価\nSFT 訓練後の性能も測定",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#midtraining-データの構成",
    "href": "ja/olmo-3/03-midtraining.html#midtraining-データの構成",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Table 5 に示される Dolmino Mix は、以下のターゲット能力ごとにデータソースを構成している。\n\n\n\n\n\n\n\n\n\nCapability\nDataset\nToken Count\nDescription\n\n\n\n\nMath\nTinyMATH\n~5B\nMath problem-solution pairs\n\n\n\nCraneMath\n~3B\nMathematical reasoning\n\n\n\nMegaMatt\n~2B\nAdvanced mathematics\n\n\n\nDolmino Math\n~4B\nCurated math corpus\n\n\nCode\nStack-Edu (FIM)\n~10B\nEducational code with Fill-In-Middle\n\n\n\nCraneCode\n~5B\nHigh-quality code snippets\n\n\nQA\nReddit-to-Flashcards\n~3B\nQuestion-answer extraction\n\n\n\nWiki-to-RCQA\n~4B\nReading comprehension QA\n\n\n\nNemotron\n~2B\nSynthetic QA pairs\n\n\nInstruction\nTulu3 SFT\n~2B\nInstruction-following examples\n\n\n\nFlan\n~3B\nTask-oriented instructions\n\n\nThinking\nMeta-reasoning\n~2B\nChain-of-thought reasoning\n\n\n\nProgram-verifiable\n~1B\nVerifiable reasoning traces\n\n\n\nOMR rewrite\n~1B\nReasoning rewriting\n\n\nWeb\nDolma v1.7 Web\n~50B\nGeneral web content (baseline)\n\n\n\n\n\n\n\n\n\nNoteDolmino Mix の設計方針\n\n\n\n各能力ごとに複数のデータソースを組み合わせることで、単一データセットに依存せず、能力の汎化性能を向上させる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#capability-improvements",
    "href": "ja/olmo-3/03-midtraining.html#capability-improvements",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "各ターゲット能力での改善結果（Section 3.5.2）。\n\n\n\nTinyMATH: 基本的な算術・代数問題\nCraneMath: 複雑な数式処理と証明\nMegaMatt: 大学レベルの数学問題\nDolmino Math: 上記を統合したキュレーションコーパス\n\n\n\n\n\nStack-Edu (FIM): Fill-In-Middle 形式での教育的コード\nCraneCode: 高品質なコードスニペット（複数言語）\n\n\n\n\n\n\n\nTipFill-In-Middle (FIM)\n\n\n\nコードの中間部分を予測するタスク。実際の IDE での補完シナリオに近い。\n\n\n\n\n\n\nReddit-to-Flashcards: Reddit の議論から QA ペアを抽出\nWiki-to-RCQA: Wikipedia 記事から読解問題を生成\nNemotron: 合成 QA データセット\n\n\n\n\n\nTulu3 SFT: 多様な指示追従タスク\nFlan: タスク指向の指示データ\n\n\n\n\n\nMeta-reasoning: Chain-of-Thought (CoT) スタイルの推論\nProgram-verifiable: プログラムで検証可能な推論トレース\nOMR rewrite: 推論プロセスのリライト",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#decontamination",
    "href": "ja/olmo-3/03-midtraining.html#decontamination",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Section 3.5.3 で詳述されている decontamination（汚染除去）プロセス。\n新しい decon パッケージ を開発し、評価データセットとの重複を除去。\n\nn-gram ベースのマッチング\n評価ベンチマークの汚染検出\n訓練データからの除外処理\n\n\n\n\n\n\n\nWarning評価データの汚染リスク\n\n\n\n高品質データセットには、評価ベンチマークと重複するサンプルが含まれる可能性がある。Decontamination により公平な評価を保証する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#key-findings",
    "href": "ja/olmo-3/03-midtraining.html#key-findings",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Section 3.5.4 の主要な発見。\n\nMicroanneal の有効性: 10B トークンの軽量テストで、100B の完全 run の結果を予測可能\nデータソースの相補性: 複数データソースの組み合わせが単一データセット以上の効果\nSFT との相性: Midtraining で強化された能力は、SFT 後もさらに向上\nDecontamination の必要性: 汚染除去により評価精度が大幅に改善",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#関連セクション",
    "href": "ja/olmo-3/03-midtraining.html#関連セクション",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "03-pretraining.qmd: Stage 1 の事前学習\n04-sft.qmd: Stage 3 の SFT 訓練\n05-data-dolma3.qmd: Dolma 3 データセット全体",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html",
    "href": "ja/olmo-3/01-dolma3-dataset.html",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模なデータセットです。Dolma 3 Mix として知られる約 6 兆トークンの多様なデータで構成されており、Web ページ、学術 PDF、コードリポジトリ、数学データなど、複数のデータソースを含んでいます。\n\n\nDolma 3 の主な目的は、Olmo 3 Base モデルに幅広い知識と能力を与えることです。データセットは、最も計算集約的な事前学習ステージ（全体の計算量の 90% 以上を消費）で使用されるため、スケーラビリティと品質が重要視されています。\nデータ戦略の基本原則:\n\n規模の重要性: 事前学習に影響を与えるには、兆トークンスケールで十分な量のデータが必要\nタスクデータの扱い: 構造化されたタスクデータ（QA ペア、チャットインスタンスなど）は、後の中間訓練や長文脈拡張のステージで使用し、事前学習では使用しない\n\n\n\n\nDolma 3 Mix は、複数のデータソースから構成されています。以下の表は、各ソースのトークン数と文書数を示しています。\n\n\n\nTable 1: Dolma 3 Mix のデータソース構成\n\n\n\n\n\n\n\n\n\n\n\n\nデータソース\nタイプ\n9T プール\n6T Mix\n6T Mix 割合\n\n\n\n\nCommon Crawl\nWeb ページ\n8.14T トークン9.67B 文書\n4.51T トークン3.15B 文書\n76.1%\n\n\nolmOCR science PDFs\n学術文書\n972B トークン101M 文書\n805B トークン83.8M 文書\n13.6%\n\n\nStack-Edu (Rebalanced)\nGitHub コード\n137B トークン167M 文書\n409B トークン526M 文書\n6.89%\n\n\narXiv\nLaTeX 論文\n21.4B トークン3.95M 文書\n50.8B トークン9.10M 文書\n0.86%\n\n\nFineMath 3+\n数学 Web ページ\n34.1B トークン21.4M 文書\n152B トークン95.5M 文書\n2.56%\n\n\nWikipedia & Wikibooks\n百科事典\n3.69B トークン6.67M 文書\n2.51B トークン4.24M 文書\n0.04%\n\n\n合計\n\n9.31T トークン9.97B 文書\n5.93T トークン3.87B 文書\n100%\n\n\n\n\n\n\n\n\nCommon Crawl (Web ページ):\n\n最も大きな割合を占めるデータソース（76.1%）\n多様な Web ページから抽出されたテキストデータ\n2024 年 12 月 31 日までのデータを含む\n\nolmOCR science PDFs (学術文書):\n\n学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n238 百万件のユニークな PDF 文書から抽出\nolmOCR ツールを使用してテキスト抽出\n\nStack-Edu (GitHub コード):\n\nThe Stack v2 データセットから厳選した教育的プログラミングコンテンツ\nプログラミング言語別に分割され、最適なミックスが適用される\n\narXiv (LaTeX 論文):\n\nProof-Pile-2 データセットから取得\n元の LaTeX 記法を保持し、数学的内容と適切なフォーマットの両方を学習可能\n\nFineMath 3+ (数学 Web ページ):\n\n数学的教育コンテンツを含む Common Crawl 文書のサブセット\n数学記法を適切に保持するように再処理\n\nWikipedia & Wikibooks (百科事典):\n\n英語版と Simple 版の Wikipedia と Wikibooks\n百科事典的知識のベースソース\n\n\n\n\n\nDolma 3 は、3 つの主要な技術革新を導入しています。\n\n\nDolma 3 では、兆トークンスケールで高速かつスケーラブルなグローバル重複排除を実現するために、Duplodocus という新しいツールを開発しました。\n重複排除は 3 つのステージで実施されます:\nStage 1: 完全重複排除 (Exact Deduplication):\n\n文書テキストハッシュに基づくグローバル重複排除\nすべての完全コピーを削除\n67% のデータを重複として識別し、38.7B から 12.8B 文書に削減\n\nStage 2: 曖昧重複排除 (Fuzzy Deduplication):\n\nMinHash ベースの重複排除で、ほぼ同一の文書を識別・削除\nヘッダーやフッターのみが異なる文書（複数ドメイン間でコピーされた文書）を削除\n23% のデータを重複として識別し、9.8B 文書に削減\n\nStage 3: 部分文字列重複排除 (Substring Deduplication):\n\n新しいファジー suffix-array ベースの重複排除手順\n個別文書内の繰り返しコンテンツ（ボイラープレートテキストや HTML アーティファクト）を削除\n500 バイト以上の繰り返し部分文字列をマーク\n14% のテキストバイトを削除し、9.7B 文書（36.5T バイト）に削減\n\nこの 3 段階の手順により、Web コーパスは 38.7B から 9.7B 文書に削減されました（文書数で 75% 削減）。\n\n\n\nDolma 3 では、データミキシングの 2 つの新しい手法を導入しています。\nToken-constrained Mixing (トークン制約付きミキシング):\n\nSwarm ベースのアプローチを使用して、多数の小型プロキシモデルを訓練・評価\nこれらの結果を使用して最適なミックスを決定\n条件付きミキシング手順により、データソースが継続的に改善・更新される開発サイクルに対応\n\nToken-constrained Mixing の手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\nQuality-aware Upsampling (品質認識アップサンプリング):\n\n各トピック内の品質バリエーションを考慮\n高品質な文書を選択的に繰り返すことで、全体的な繰り返しを最小限に抑えながら、高品質データの繰り返しを集中させる\n\n\n\n\nolmOCR science PDFs は、学術 PDF を線形化プレーンテキストに変換した新しいデータソースです。従来の peS2o データセットを置き換える形で導入されました。\n特徴:\n\nAI2Bot として識別される「礼儀正しい」クローリング\nrobots.txt を遵守し、ペイウォールを回避しない\n学術サイトと論文リポジトリに焦点を当てたクローリング\nolmOCR（バージョン 0.1.49-0.1.53）を使用してテキスト抽出\n\nデータ規模:\n\n238 百万件のユニークな PDF 文書（2024 年 12 月までのカットオフ日）\nテキスト抽出後、160 百万件の PDF 文書\n重複排除後、156 百万件の文書\n\n\n\n\n\nDolma 3 Mix のデータキュレーションは、以下のフローで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│                  Data Curation Pipeline                      │\n├──────────────────────────────────────────────────────────────┤\n│  Common Crawl (Web pages)                                    │\n│    └─&gt; HTML text extraction                                  │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  Academic PDFs                                               │\n│    └─&gt; OCR text extraction                                   │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  GitHub repos (Stack-Edu)                                    │\n│    └─&gt; Language classification                               │\n├──────────────────────────────────────────────────────────────┤\n│  FineMath, arXiv, Wiki                                       │\n│    └─&gt; (Preprocessed)                                        │\n├──────────────────────────────────────────────────────────────┤\n│  Mixing                                                      │\n│    └─&gt; Quality upsampling                                    │\n│        └─&gt; Dolma 3 Mix (6T tokens)                           │\n└──────────────────────────────────────────────────────────────┘\nパイプラインの主要ステップ:\n\nテキスト抽出: HTML または PDF からテキストを抽出\nヒューリスティックフィルタリング: 低品質文書、スパム、アダルトコンテンツを削除\n重複排除: 完全重複、曖昧重複、部分文字列重複を削除\nトピック・品質分類: WebOrganizer ツールで 24 のトピックに分類し、品質スコアを付与\nミキシング: Token-constrained mixing でデータソースの最適な比率を決定\n品質アップサンプリング: 高品質文書を選択的に繰り返す\n\n\n\n\nDolma 3 データセットは、Olmo 3 の訓練プロセスの 3 つのステージで使用されます。\n\n\n使用データ: Dolma 3 Mix（6T トークン）\n目的: 多様な知識と能力を持つ基盤モデルを構築\nデータソース:\n\nCommon Crawl: 76.1%\nolmOCR science PDFs: 13.6%\nStack-Edu: 6.89%\narXiv: 0.86%\nFineMath 3+: 2.56%\nWikipedia & Wikibooks: 0.04%\n\n\n\n\n使用データ: Dolma 3 Dolmino Mix（100B トークン）\n目的: コード、数学、一般知識 QA などの重要な能力を強化\n特徴: Post-training の下準備として、指示データと思考トレースを意図的に含める\n詳細な Midtraining の説明は別の文書で扱われています。\n\n\n\n使用データ: Dolma 3 Longmino Mix（50-100B トークン）\n目的: 最大 65K トークンのコンテキストをサポートする長文脈能力を獲得\nデータ規模:\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n詳細な Long-context Extension の説明は別の文書で扱われています。\n\n\n\n\nToken-constrained mixing により、データソースの最適な比率が決定されました。\nWeb テキストのトピック分布:\n\nSTEM ドメイン（「Science, Math, and Technology」、「Software Development」）を大幅にアップウェイト\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\nStack-Edu のプログラミング言語分布:\n\nPython を Java や Markdown よりも優先\nほぼすべてのコーディングベンチマークで改善を達成\n\n\n\n\n\n\n\nNote実験用のサンプルミックス\n\n\n\n\n\nDolma 3 では、より少ない計算リソースで実験できるように、サンプルミックスも公開しています。\nPretraining サンプルミックス:\n\n150B トークン\nDolma 3 Mix と同じデータソース構成\n\n利点:\n\n小規模な実験が可能\nデータミキシングのアブレーション研究に有用\nコンピューティングリソースが限られた研究者も利用可能\n\n\n\n\n\n\n\nDolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模で高品質なデータセットです。グローバル重複排除、Token-constrained mixing、Quality-aware upsampling などの革新的な手法により、最適なデータミキシングを実現しています。\n主な特徴:\n\n多様なデータソース: Web、学術 PDF、コード、数学、百科事典など\n大規模: 6 兆トークン（Dolma 3 Mix）\n高品質: 重複排除とヒューリスティックフィルタリングによる品質管理\n最適化されたミキシング: Swarm ベースの手法で最適な比率を決定\n3 つのステージ: Pretraining、Midtraining、Long-context Extension で使用\n\nDolma 3 は完全にオープンで、研究者が再現性の高い研究を行えるようにすべてのデータソース、処理パイプライン、ミキシング比率を公開しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#概要と目的",
    "href": "ja/olmo-3/01-dolma3-dataset.html#概要と目的",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 の主な目的は、Olmo 3 Base モデルに幅広い知識と能力を与えることです。データセットは、最も計算集約的な事前学習ステージ（全体の計算量の 90% 以上を消費）で使用されるため、スケーラビリティと品質が重要視されています。\nデータ戦略の基本原則:\n\n規模の重要性: 事前学習に影響を与えるには、兆トークンスケールで十分な量のデータが必要\nタスクデータの扱い: 構造化されたタスクデータ（QA ペア、チャットインスタンスなど）は、後の中間訓練や長文脈拡張のステージで使用し、事前学習では使用しない",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#sec-dolma3-composition",
    "href": "ja/olmo-3/01-dolma3-dataset.html#sec-dolma3-composition",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 Mix は、複数のデータソースから構成されています。以下の表は、各ソースのトークン数と文書数を示しています。\n\n\n\nTable 1: Dolma 3 Mix のデータソース構成\n\n\n\n\n\n\n\n\n\n\n\n\nデータソース\nタイプ\n9T プール\n6T Mix\n6T Mix 割合\n\n\n\n\nCommon Crawl\nWeb ページ\n8.14T トークン9.67B 文書\n4.51T トークン3.15B 文書\n76.1%\n\n\nolmOCR science PDFs\n学術文書\n972B トークン101M 文書\n805B トークン83.8M 文書\n13.6%\n\n\nStack-Edu (Rebalanced)\nGitHub コード\n137B トークン167M 文書\n409B トークン526M 文書\n6.89%\n\n\narXiv\nLaTeX 論文\n21.4B トークン3.95M 文書\n50.8B トークン9.10M 文書\n0.86%\n\n\nFineMath 3+\n数学 Web ページ\n34.1B トークン21.4M 文書\n152B トークン95.5M 文書\n2.56%\n\n\nWikipedia & Wikibooks\n百科事典\n3.69B トークン6.67M 文書\n2.51B トークン4.24M 文書\n0.04%\n\n\n合計\n\n9.31T トークン9.97B 文書\n5.93T トークン3.87B 文書\n100%\n\n\n\n\n\n\n\n\nCommon Crawl (Web ページ):\n\n最も大きな割合を占めるデータソース（76.1%）\n多様な Web ページから抽出されたテキストデータ\n2024 年 12 月 31 日までのデータを含む\n\nolmOCR science PDFs (学術文書):\n\n学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n238 百万件のユニークな PDF 文書から抽出\nolmOCR ツールを使用してテキスト抽出\n\nStack-Edu (GitHub コード):\n\nThe Stack v2 データセットから厳選した教育的プログラミングコンテンツ\nプログラミング言語別に分割され、最適なミックスが適用される\n\narXiv (LaTeX 論文):\n\nProof-Pile-2 データセットから取得\n元の LaTeX 記法を保持し、数学的内容と適切なフォーマットの両方を学習可能\n\nFineMath 3+ (数学 Web ページ):\n\n数学的教育コンテンツを含む Common Crawl 文書のサブセット\n数学記法を適切に保持するように再処理\n\nWikipedia & Wikibooks (百科事典):\n\n英語版と Simple 版の Wikipedia と Wikibooks\n百科事典的知識のベースソース",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#主要な革新点",
    "href": "ja/olmo-3/01-dolma3-dataset.html#主要な革新点",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、3 つの主要な技術革新を導入しています。\n\n\nDolma 3 では、兆トークンスケールで高速かつスケーラブルなグローバル重複排除を実現するために、Duplodocus という新しいツールを開発しました。\n重複排除は 3 つのステージで実施されます:\nStage 1: 完全重複排除 (Exact Deduplication):\n\n文書テキストハッシュに基づくグローバル重複排除\nすべての完全コピーを削除\n67% のデータを重複として識別し、38.7B から 12.8B 文書に削減\n\nStage 2: 曖昧重複排除 (Fuzzy Deduplication):\n\nMinHash ベースの重複排除で、ほぼ同一の文書を識別・削除\nヘッダーやフッターのみが異なる文書（複数ドメイン間でコピーされた文書）を削除\n23% のデータを重複として識別し、9.8B 文書に削減\n\nStage 3: 部分文字列重複排除 (Substring Deduplication):\n\n新しいファジー suffix-array ベースの重複排除手順\n個別文書内の繰り返しコンテンツ（ボイラープレートテキストや HTML アーティファクト）を削除\n500 バイト以上の繰り返し部分文字列をマーク\n14% のテキストバイトを削除し、9.7B 文書（36.5T バイト）に削減\n\nこの 3 段階の手順により、Web コーパスは 38.7B から 9.7B 文書に削減されました（文書数で 75% 削減）。\n\n\n\nDolma 3 では、データミキシングの 2 つの新しい手法を導入しています。\nToken-constrained Mixing (トークン制約付きミキシング):\n\nSwarm ベースのアプローチを使用して、多数の小型プロキシモデルを訓練・評価\nこれらの結果を使用して最適なミックスを決定\n条件付きミキシング手順により、データソースが継続的に改善・更新される開発サイクルに対応\n\nToken-constrained Mixing の手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\nQuality-aware Upsampling (品質認識アップサンプリング):\n\n各トピック内の品質バリエーションを考慮\n高品質な文書を選択的に繰り返すことで、全体的な繰り返しを最小限に抑えながら、高品質データの繰り返しを集中させる\n\n\n\n\nolmOCR science PDFs は、学術 PDF を線形化プレーンテキストに変換した新しいデータソースです。従来の peS2o データセットを置き換える形で導入されました。\n特徴:\n\nAI2Bot として識別される「礼儀正しい」クローリング\nrobots.txt を遵守し、ペイウォールを回避しない\n学術サイトと論文リポジトリに焦点を当てたクローリング\nolmOCR（バージョン 0.1.49-0.1.53）を使用してテキスト抽出\n\nデータ規模:\n\n238 百万件のユニークな PDF 文書（2024 年 12 月までのカットオフ日）\nテキスト抽出後、160 百万件の PDF 文書\n重複排除後、156 百万件の文書",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#データキュレーションのパイプライン",
    "href": "ja/olmo-3/01-dolma3-dataset.html#データキュレーションのパイプライン",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 Mix のデータキュレーションは、以下のフローで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│                  Data Curation Pipeline                      │\n├──────────────────────────────────────────────────────────────┤\n│  Common Crawl (Web pages)                                    │\n│    └─&gt; HTML text extraction                                  │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  Academic PDFs                                               │\n│    └─&gt; OCR text extraction                                   │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  GitHub repos (Stack-Edu)                                    │\n│    └─&gt; Language classification                               │\n├──────────────────────────────────────────────────────────────┤\n│  FineMath, arXiv, Wiki                                       │\n│    └─&gt; (Preprocessed)                                        │\n├──────────────────────────────────────────────────────────────┤\n│  Mixing                                                      │\n│    └─&gt; Quality upsampling                                    │\n│        └─&gt; Dolma 3 Mix (6T tokens)                           │\n└──────────────────────────────────────────────────────────────┘\nパイプラインの主要ステップ:\n\nテキスト抽出: HTML または PDF からテキストを抽出\nヒューリスティックフィルタリング: 低品質文書、スパム、アダルトコンテンツを削除\n重複排除: 完全重複、曖昧重複、部分文字列重複を削除\nトピック・品質分類: WebOrganizer ツールで 24 のトピックに分類し、品質スコアを付与\nミキシング: Token-constrained mixing でデータソースの最適な比率を決定\n品質アップサンプリング: 高品質文書を選択的に繰り返す",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#つのステージでの使用方法",
    "href": "ja/olmo-3/01-dolma3-dataset.html#つのステージでの使用方法",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 データセットは、Olmo 3 の訓練プロセスの 3 つのステージで使用されます。\n\n\n使用データ: Dolma 3 Mix（6T トークン）\n目的: 多様な知識と能力を持つ基盤モデルを構築\nデータソース:\n\nCommon Crawl: 76.1%\nolmOCR science PDFs: 13.6%\nStack-Edu: 6.89%\narXiv: 0.86%\nFineMath 3+: 2.56%\nWikipedia & Wikibooks: 0.04%\n\n\n\n\n使用データ: Dolma 3 Dolmino Mix（100B トークン）\n目的: コード、数学、一般知識 QA などの重要な能力を強化\n特徴: Post-training の下準備として、指示データと思考トレースを意図的に含める\n詳細な Midtraining の説明は別の文書で扱われています。\n\n\n\n使用データ: Dolma 3 Longmino Mix（50-100B トークン）\n目的: 最大 65K トークンのコンテキストをサポートする長文脈能力を獲得\nデータ規模:\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n詳細な Long-context Extension の説明は別の文書で扱われています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#データミキシングの結果",
    "href": "ja/olmo-3/01-dolma3-dataset.html#データミキシングの結果",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Token-constrained mixing により、データソースの最適な比率が決定されました。\nWeb テキストのトピック分布:\n\nSTEM ドメイン（「Science, Math, and Technology」、「Software Development」）を大幅にアップウェイト\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\nStack-Edu のプログラミング言語分布:\n\nPython を Java や Markdown よりも優先\nほぼすべてのコーディングベンチマークで改善を達成\n\n\n\n\n\n\n\nNote実験用のサンプルミックス\n\n\n\n\n\nDolma 3 では、より少ない計算リソースで実験できるように、サンプルミックスも公開しています。\nPretraining サンプルミックス:\n\n150B トークン\nDolma 3 Mix と同じデータソース構成\n\n利点:\n\n小規模な実験が可能\nデータミキシングのアブレーション研究に有用\nコンピューティングリソースが限られた研究者も利用可能",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#まとめ",
    "href": "ja/olmo-3/01-dolma3-dataset.html#まとめ",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模で高品質なデータセットです。グローバル重複排除、Token-constrained mixing、Quality-aware upsampling などの革新的な手法により、最適なデータミキシングを実現しています。\n主な特徴:\n\n多様なデータソース: Web、学術 PDF、コード、数学、百科事典など\n大規模: 6 兆トークン（Dolma 3 Mix）\n高品質: 重複排除とヒューリスティックフィルタリングによる品質管理\n最適化されたミキシング: Swarm ベースの手法で最適な比率を決定\n3 つのステージ: Pretraining、Midtraining、Long-context Extension で使用\n\nDolma 3 は完全にオープンで、研究者が再現性の高い研究を行えるようにすべてのデータソース、処理パイプライン、ミキシング比率を公開しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html",
    "href": "ja/molmo2/index.html",
    "title": "Molmo2",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備え、動画内の「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示すことができる点です。\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）を使用し、オープンソースモデルの中で最高水準の性能を達成しています。特に、ビデオポインティングとトラッキングでは、Gemini 3 Pro などのプロプライエタリモデルを上回る性能を示しています。\n論文: arXiv:2601.10611\nコード: github.com/allenai/molmo2\nDemo: playground.allenai.org",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#主な貢献",
    "href": "ja/molmo2/index.html#主な貢献",
    "title": "Molmo2",
    "section": "主な貢献",
    "text": "主な貢献\n\n9つの新規データセット: プロプライエタリモデルからの蒸留を一切使用せず構築\nビデオグラウンディング: 時空間的なポインティングとトラッキングを実現\n超詳細なビデオキャプション: 平均924語/動画（既存データセットの約2-12倍）\n完全オープン: モデル、データ、コードを全て公開",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#モデルサイズ",
    "href": "ja/molmo2/index.html#モデルサイズ",
    "title": "Molmo2",
    "section": "モデルサイズ",
    "text": "モデルサイズ\n\nMolmo2-4B: Qwen3 LLM ベース\nMolmo2-8B: Qwen3 LLM ベース\nMolmo2-O-7B: OLMo LLM ベース（完全オープン）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#目次",
    "href": "ja/molmo2/index.html#目次",
    "title": "Molmo2",
    "section": "目次",
    "text": "目次\n\n全体像\nDense Video Captioning\nVideo Grounding: Pointing & Tracking\nMulti-Image Understanding\nVision-Language Connector\nLong-Context Training\nToken Weighting\nPacking & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html",
    "href": "ja/molmo2/06-token-weighting.html",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting（トークン重み付け）は、訓練データに含まれる様々な長さの出力を持つタスクにおいて、損失関数の寄与度を調整する手法である。Molmo2 の訓練では、単一トークンの選択肢問題から 4,000 トークン以上の長いビデオキャプションまで、多様な出力長を持つデータが混在している。\n\n\n\n\n\n訓練データに出力長の大きな偏りがある場合、以下の問題が発生する:\n\nトークン数の偏り: 長い出力を持つタスク（例: ビデオキャプション）は、サンプリング頻度が低くても、訓練時の損失トークンの大部分を占めてしまう\n短い出力タスクの劣化: 多肢選択問題や短い回答を要求するタスクの性能が低下する\nタスクバランスの崩壊: モデルが長い出力を生成するタスクに過度に最適化され、他のタスクでの性能が犠牲になる\n\n\n\n\n\n\n\nNote例: 極端なケース\n\n\n\n1 サンプルのビデオキャプション（4,000 トークン）と 100 サンプルの多肢選択問題（各 1 トークン）を同じバッチで訓練する場合、ビデオキャプションが損失全体の約 97.5% を占めることになる（4,000 / (4,000 + 100) ≈ 0.975）。\n\n\n\n\n\n\nMolmo2 では、タスクの種類と出力長に応じて、各サンプルの損失に対する重みを調整している。\n\n\n\n\n\n\n\n\n\n\nタスク種別\n重み\n理由\n\n\n\n\nビデオキャプション\n0.1\n非常に長く密な出力（4,000+ トークン）を生成するため\n\n\nポインティング\n0.2\n座標列挙により長く密な出力を生成するため\n\n\nその他のタスク\n\\(\\frac{4}{\\sqrt{n}}\\)\n出力長 \\(n\\) に応じた適応的な重み付け\n\n\n\n\n\n\nサンプル \\(i\\) の損失重み \\(w_i\\) は以下のように定義される:\n\\[\nw_i = \\begin{cases}\n0.1 & \\text{if task is video captioning} \\\\\n0.2 & \\text{if task is pointing} \\\\\n\\frac{4}{\\sqrt{n_i}} & \\text{otherwise}\n\\end{cases}\n\\]\nここで、\\(n_i\\) はサンプル \\(i\\) の回答トークン数である。\n\n\n\nその他のタスクに対する \\(\\frac{4}{\\sqrt{n}}\\) という重み付けは、以下の特性を持つ:\n\n短い出力: \\(n = 1\\) のとき \\(w = 4.0\\)、\\(n = 16\\) のとき \\(w = 1.0\\)\n中程度の出力: \\(n = 100\\) のとき \\(w = 0.4\\)\n長い出力: \\(n = 400\\) のとき \\(w = 0.2\\)\n\nこの平方根による減衰により、長い出力を持つサンプルの影響を抑えつつ、完全に無視することを避けている。\n\n\n\n\n\n\nTip重み付けの直感\n\n\n\n平方根による重み付け \\(\\frac{4}{\\sqrt{n}}\\) は、出力長が 4 倍になると重みが半分になる特性を持つ。これにより、長い出力と短い出力の間でバランスの取れた学習が可能になる。\n例えば:\n\n1 トークン出力: 重み 4.0\n4 トークン出力: 重み 2.0\n16 トークン出力: 重み 1.0\n64 トークン出力: 重み 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant長短出力のバランス改善\n\n\n\nToken Weighting の導入により、以下の効果が得られる:\n\n多様なタスクでの性能維持: 短い回答を要求するタスク（多肢選択問題など）と長い出力を生成するタスク（ビデオキャプションなど）の両方で良好な性能を達成\n訓練の安定性向上: 損失が特定のタスクに支配されることを防ぎ、より安定した訓練が可能\n効率的なデータ活用: 出力長が異なる多様なタスクを単一のモデルで効果的に学習可能\n\n\n\n\n\n\nToken Weighting は損失計算時に適用される:\n\\[\n\\mathcal{L} = \\frac{1}{B} \\sum_{i=1}^{B} w_i \\cdot \\mathcal{L}_i\n\\]\nここで:\n\n\\(B\\) はバッチサイズ\n\\(\\mathcal{L}_i\\) はサンプル \\(i\\) の未調整損失\n\\(w_i\\) はサンプル \\(i\\) の重み\n\nこの重み付けは、各サンプルの損失を個別に計算できるため、実装が容易であり、既存の訓練パイプラインに容易に統合できる。\n\n\n\n\n\n\nサンプリング比率の調整: タスクごとのサンプリング確率を調整する方法だが、長い出力を持つタスクは少数サンプルでも損失を支配してしまうため不十分\n損失の正規化: タスクごとに損失を正規化する方法だが、同一タスク内での出力長の違いに対応できない\n固定重み: 全てのサンプルに固定の重みを使用する方法だが、出力長の多様性に対応できない\n\n\n\n\n\n適応性: 出力長に応じて自動的に重みが調整される\n柔軟性: タスクの特性（ビデオキャプション、ポインティング）に応じた固定重みと、一般的なタスク向けの適応的重みを組み合わせ\nシンプルさ: 実装が容易で、ハイパーパラメータの調整が最小限",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#概要",
    "href": "ja/molmo2/06-token-weighting.html#概要",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting（トークン重み付け）は、訓練データに含まれる様々な長さの出力を持つタスクにおいて、損失関数の寄与度を調整する手法である。Molmo2 の訓練では、単一トークンの選択肢問題から 4,000 トークン以上の長いビデオキャプションまで、多様な出力長を持つデータが混在している。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#問題設定",
    "href": "ja/molmo2/06-token-weighting.html#問題設定",
    "title": "Token Weighting",
    "section": "",
    "text": "訓練データに出力長の大きな偏りがある場合、以下の問題が発生する:\n\nトークン数の偏り: 長い出力を持つタスク（例: ビデオキャプション）は、サンプリング頻度が低くても、訓練時の損失トークンの大部分を占めてしまう\n短い出力タスクの劣化: 多肢選択問題や短い回答を要求するタスクの性能が低下する\nタスクバランスの崩壊: モデルが長い出力を生成するタスクに過度に最適化され、他のタスクでの性能が犠牲になる\n\n\n\n\n\n\n\nNote例: 極端なケース\n\n\n\n1 サンプルのビデオキャプション（4,000 トークン）と 100 サンプルの多肢選択問題（各 1 トークン）を同じバッチで訓練する場合、ビデオキャプションが損失全体の約 97.5% を占めることになる（4,000 / (4,000 + 100) ≈ 0.975）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#molmo2-の重み付け戦略",
    "href": "ja/molmo2/06-token-weighting.html#molmo2-の重み付け戦略",
    "title": "Token Weighting",
    "section": "",
    "text": "Molmo2 では、タスクの種類と出力長に応じて、各サンプルの損失に対する重みを調整している。\n\n\n\n\n\n\n\n\n\n\nタスク種別\n重み\n理由\n\n\n\n\nビデオキャプション\n0.1\n非常に長く密な出力（4,000+ トークン）を生成するため\n\n\nポインティング\n0.2\n座標列挙により長く密な出力を生成するため\n\n\nその他のタスク\n\\(\\frac{4}{\\sqrt{n}}\\)\n出力長 \\(n\\) に応じた適応的な重み付け\n\n\n\n\n\n\nサンプル \\(i\\) の損失重み \\(w_i\\) は以下のように定義される:\n\\[\nw_i = \\begin{cases}\n0.1 & \\text{if task is video captioning} \\\\\n0.2 & \\text{if task is pointing} \\\\\n\\frac{4}{\\sqrt{n_i}} & \\text{otherwise}\n\\end{cases}\n\\]\nここで、\\(n_i\\) はサンプル \\(i\\) の回答トークン数である。\n\n\n\nその他のタスクに対する \\(\\frac{4}{\\sqrt{n}}\\) という重み付けは、以下の特性を持つ:\n\n短い出力: \\(n = 1\\) のとき \\(w = 4.0\\)、\\(n = 16\\) のとき \\(w = 1.0\\)\n中程度の出力: \\(n = 100\\) のとき \\(w = 0.4\\)\n長い出力: \\(n = 400\\) のとき \\(w = 0.2\\)\n\nこの平方根による減衰により、長い出力を持つサンプルの影響を抑えつつ、完全に無視することを避けている。\n\n\n\n\n\n\nTip重み付けの直感\n\n\n\n平方根による重み付け \\(\\frac{4}{\\sqrt{n}}\\) は、出力長が 4 倍になると重みが半分になる特性を持つ。これにより、長い出力と短い出力の間でバランスの取れた学習が可能になる。\n例えば:\n\n1 トークン出力: 重み 4.0\n4 トークン出力: 重み 2.0\n16 トークン出力: 重み 1.0\n64 トークン出力: 重み 0.5",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#効果",
    "href": "ja/molmo2/06-token-weighting.html#効果",
    "title": "Token Weighting",
    "section": "",
    "text": "Important長短出力のバランス改善\n\n\n\nToken Weighting の導入により、以下の効果が得られる:\n\n多様なタスクでの性能維持: 短い回答を要求するタスク（多肢選択問題など）と長い出力を生成するタスク（ビデオキャプションなど）の両方で良好な性能を達成\n訓練の安定性向上: 損失が特定のタスクに支配されることを防ぎ、より安定した訓練が可能\n効率的なデータ活用: 出力長が異なる多様なタスクを単一のモデルで効果的に学習可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#実装上の考慮事項",
    "href": "ja/molmo2/06-token-weighting.html#実装上の考慮事項",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting は損失計算時に適用される:\n\\[\n\\mathcal{L} = \\frac{1}{B} \\sum_{i=1}^{B} w_i \\cdot \\mathcal{L}_i\n\\]\nここで:\n\n\\(B\\) はバッチサイズ\n\\(\\mathcal{L}_i\\) はサンプル \\(i\\) の未調整損失\n\\(w_i\\) はサンプル \\(i\\) の重み\n\nこの重み付けは、各サンプルの損失を個別に計算できるため、実装が容易であり、既存の訓練パイプラインに容易に統合できる。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#関連手法との比較",
    "href": "ja/molmo2/06-token-weighting.html#関連手法との比較",
    "title": "Token Weighting",
    "section": "",
    "text": "サンプリング比率の調整: タスクごとのサンプリング確率を調整する方法だが、長い出力を持つタスクは少数サンプルでも損失を支配してしまうため不十分\n損失の正規化: タスクごとに損失を正規化する方法だが、同一タスク内での出力長の違いに対応できない\n固定重み: 全てのサンプルに固定の重みを使用する方法だが、出力長の多様性に対応できない\n\n\n\n\n\n適応性: 出力長に応じて自動的に重みが調整される\n柔軟性: タスクの特性（ビデオキャプション、ポインティング）に応じた固定重みと、一般的なタスク向けの適応的重みを組み合わせ\nシンプルさ: 実装が容易で、ハイパーパラメータの調整が最小限",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html",
    "href": "ja/molmo2/04-vision-language-connector.html",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector は、Vision Transformer (ViT) が抽出した視覚特徴を、Large Language Model (LLM) が処理できる形式に変換する重要なモジュールです。Molmo2 では、標準的な VLM アーキテクチャ [@Clark2024Molmo] に従い、画像とビデオの両方を統一的に処理できる設計を採用しています。\n\n\n\n\n\n\nflowchart TD\n    A[Visual Input&lt;br/&gt;Image or Video] --&gt; B[ViT Encoding&lt;br/&gt;Patch-level Features]\n    B --&gt; C[Multi-layer Feature&lt;br/&gt;Extraction]\n    C --&gt; D{Input Type}\n    D --&gt;|Image| E[2×2 Attention&lt;br/&gt;Pooling]\n    D --&gt;|Video Frame| F[3×3 Attention&lt;br/&gt;Pooling]\n    E --&gt; G[Shared MLP&lt;br/&gt;Projection]\n    F --&gt; G\n    G --&gt; H[Visual Tokens&lt;br/&gt;for LLM]\n\n    style C fill:#e6f0ff\n    style E fill:#ffe6f0\n    style F fill:#ffe6f0\n    style G fill:#f0ffe6\n\n\n\n\nFigure 1: Vision-Language Connector のデータフロー\n\n\n\n\n\n\n\n\n\n\nMolmo2 の Vision-Language Connector は、ViT の単一層ではなく、複数の層から特徴を抽出 します。\n\nThird-to-last layer（最終層から3番目）: 高レベルのセマンティック特徴\nNinth-from-last layer（最終層から9番目）: 中レベルの特徴\n\nこの設計は、先行研究の Molmo [@Clark2024Molmo] に従っており、異なる抽象度の視覚情報を組み合わせることで、より豊かな表現を実現しています。\nViT Layer Structure:\n┌─────────────────────────────────┐\n│  Layer 0 (Input)                │\n│  Layer 1                        │\n│  ...                            │\n│  Layer N-9  ◄── 9th-from-last  │ ─┐\n│  ...                            │  │\n│  Layer N-3  ◄── 3rd-to-last    │ ─┤ Features used\n│  Layer N-2                      │  │ by Connector\n│  Layer N-1                      │  │\n│  Layer N (Output)               │ ─┘\n└─────────────────────────────────┘\n\n\n\nパッチレベルの特徴を削減するために、Attention Pooling を使用します。パッチの平均をクエリとし、各パッチ窓を単一のベクトルに集約します。\n\n\nInput Patches (4×4 example):\n┌─────┬─────┬─────┬─────┐\n│ P₁  │ P₂  │ P₃  │ P₄  │\n├─────┼─────┼─────┼─────┤\n│ P₅  │ P₆  │ P₇  │ P₈  │\n├─────┼─────┼─────┼─────┤\n│ P₉  │ P₁₀ │ P₁₁ │ P₁₂ │\n├─────┼─────┼─────┼─────┤\n│ P₁₃ │ P₁₄ │ P₁₅ │ P₁₆ │\n└─────┴─────┴─────┴─────┘\n\nAfter 2×2 Attention Pooling:\n┌───────────┬───────────┐\n│ T₁        │ T₂        │\n│ (P₁~P₆)   │ (P₃~P₈)   │\n├───────────┼───────────┤\n│ T₃        │ T₄        │\n│ (P₉~P₁₄)  │ (P₁₁~P₁₆) │\n└───────────┴───────────┘\n\nToken count: 16 → 4 (1/4 reduction)\n\n\n\nビデオではフレーム数が多いため、3×3 の窓 を使用してトークン数をさらに削減します。\nInput Patches (9×9 example):\n┌───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│P₁ │P₂ │P₃ │P₄ │P₅ │P₆ │P₇ │P₈ │P₉ │\n├───┼───┼───┼───┼───┼───┼───┼───┼───┤\n│...│...│...│...│...│...│...│...│...│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┘\n        ↓ 3×3 Attention Pooling\n┌─────────┬─────────┬─────────┐\n│   T₁    │   T₂    │   T₃    │\n│ (9 P's) │ (9 P's) │ (9 P's) │\n├─────────┼─────────┼─────────┤\n│   ...   │   ...   │   ...   │\n└─────────┴─────────┴─────────┘\n\nToken count: 81 → 9 (1/9 reduction)\n\n\n\n\n最後に、プールされた特徴は Shared MLP によって投影されます。この MLP は画像とビデオフレームで パラメータを共有 しており、統一的な視覚表現を学習します。\n\n\n\n\n\n\nflowchart LR\n    A[ViT Layer N-9] --&gt; C[Concat]\n    B[ViT Layer N-3] --&gt; C\n    C --&gt; D{Pooling Window}\n    D --&gt;|Image| E[2×2 Attention]\n    D --&gt;|Video| F[3×3 Attention]\n    E --&gt; G[Shared MLP]\n    F --&gt; G\n    G --&gt; H[Visual Tokens]\n\n    style C fill:#e6f0ff\n    style G fill:#f0ffe6\n\n\n\n\nFigure 2: Vision-Language Connector のアーキテクチャ\n\n\n\n\n\n\n\n\n\n\n\nMolmo2 は、Multi-crop 戦略を採用しています。\n\n1つのダウンスケール済み全体クロップ + 最大 K 個のオーバーラップタイルクロップ\nトレーニング時: K = 8\n推論時: K = 24（高解像度処理）\n\nK 個のクロップでタイル化できない画像は、ダウンスケールされます。\nOriginal Image:\n┌─────────────────────────────────┐\n│                                 │\n│                                 │\n│        High-res Image           │\n│                                 │\n│                                 │\n└─────────────────────────────────┘\n         ↓\nDownscaled Crop + K Overlapping Crops:\n┌───────┐  ┌─────┬─────┬─────┐\n│ Full  │  │ C₁  │ C₂  │ C₃  │\n│ (DS)  │  ├─────┼─────┼─────┤\n└───────┘  │ C₄  │ C₅  │ C₆  │\n           ├─────┼─────┼─────┤\n           │ C₇  │ C₈  │ ... │\n           └─────┴─────┴─────┘\n\n\n\n\n\n\nNoteColumn Tokens\n\n\n\nMulti-crop 画像の場合、Column tokens を LLM への入力に含めます。これにより、画像のアスペクト比情報を LLM に伝達できます。\n単一クロップ画像（常に正方形）には Column tokens を含めません。\n\n\n\n\n\nビデオの場合、計算コストを削減するために以下の戦略を取ります。\n\nサンプリングレート: S = 2 fps（2秒ごとに1フレーム）\n各フレームは単一クロップとして処理（必要に応じてダウンスケール）\n最大フレーム数: F = 128（標準トレーニング）または F = 384（長尺コンテキストトレーニング）\n\nVideo Timeline:\n0s    1s    2s    3s    4s    5s    ...\n│─────│─────│─────│─────│─────│─────│\n      ↓     ↓     ↓     ↓     ↓\n    Frame₁ Frame₂ Frame₃ ... (@ 2 fps)\n\nIf video length &gt; F/S seconds:\n→ Uniformly sample F frames\n→ Always include LAST frame\n\n\n\n\n\n\nTip最終フレームの特別な扱い\n\n\n\nビデオの 最終フレーム は常に含まれます。これは、多くのビデオプレイヤーが再生終了後に最終フレームを表示するため、ユーザーにとって特別な重要性を持つ可能性があるためです。\n\n\n\n\n\n\nMolmo2 では、LLM が視覚トークンを処理する際に、画像トークン同士が相互に attend できる ように設計されています [@Miao2024LongVU; @Wu2024DoubleLLaVA]。\n通常の LLM では、因果的マスク（causal mask）により、各トークンは自分より前のトークンにしか注意を向けられません。しかし、Molmo2 では、視覚トークンに対して bi-directional attention を許可しています。\nStandard Causal Attention:\n  T₁  T₂  T₃  T₄\nT₁ ●   ×   ×   ×\nT₂ ●   ●   ×   ×\nT₃ ●   ●   ●   ×\nT₄ ●   ●   ●   ●\n\nBi-directional Attention on Vision Tokens:\n  V₁  V₂  V₃  T₁  T₂\nV₁ ●   ●   ●   ×   ×\nV₂ ●   ●   ●   ×   ×\nV₃ ●   ●   ●   ×   ×\nT₁ ●   ●   ●   ●   ×\nT₂ ●   ●   ●   ●   ●\n\nV: Vision tokens, T: Text tokens\nこれにより、異なるフレームや異なる画像からの視覚トークンが相互に情報を交換でき、時空間的な関係を学習できます。\n\n\n\n\n\n\nImportantBi-directional Attention の効果\n\n\n\nアブレーション研究により、視覚トークンへの Bi-directional attention が 性能を向上させる ことが確認されています。\n特に、ビデオトラッキングやマルチイメージ理解など、複数のフレーム/画像間の関係を捉える必要があるタスクで有効です。\n\n\n\n\n\nVision-Language Connector によって生成された視覚トークンは、以下の形式で LLM に入力されます。\n\n\n&lt;image_start&gt; [Visual Tokens for Frame₁] &lt;timestamp&gt;0.5s&lt;/timestamp&gt;\n&lt;image_start&gt; [Visual Tokens for Frame₂] &lt;timestamp&gt;1.0s&lt;/timestamp&gt;\n...\n[Subtitle text] &lt;timestamp&gt;0.5s-2.0s&lt;/timestamp&gt;\n\n各フレームの視覚トークンに タイムスタンプ を付加\n字幕が利用可能な場合は、タイムスタンプ付きテキストとして追加\n\n\n\n\n&lt;image_start&gt; [Visual Tokens for Image₁] &lt;image&gt;1&lt;/image&gt;\n&lt;image_start&gt; [Visual Tokens for Image₂] &lt;image&gt;2&lt;/image&gt;\n...\n\n各画像に 画像インデックス を付加\n\n\n\n\n&lt;image_start&gt; [Column Tokens] [Visual Tokens for Full Crop]\n[Visual Tokens for Crop₁] [Visual Tokens for Crop₂] ...\n\nColumn tokens でアスペクト比を伝達\n全体クロップ + 部分クロップのトークンを結合\n\n\n\n\n\nMolmo2 の Vision-Language Connector は、以下の特徴を持ちます。\n\nMulti-layer features: ViT の複数層（3rd-to-last, 9th-from-last）から特徴を抽出\nAdaptive pooling: 画像には 2×2、ビデオフレームには 3×3 の Attention Pooling\nShared parameters: 画像とビデオで統一的な MLP projection\nMulti-crop strategy: 高解像度処理のため、最大24個のクロップを使用\nEfficient video processing: 2 fps サンプリング + 最終フレームの保持\nBi-directional attention: 視覚トークン間の相互作用を許可（性能向上）\nColumn tokens: Multi-crop 画像のアスペクト比情報を伝達\n\nこの設計により、Molmo2 は画像とビデオを統一的に処理しながら、計算効率と表現力のバランスを実現しています。\n\n\n\n\nClark, C., et al. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models. arXiv:2409.17146.\nMiao, X., et al. (2024). LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. arXiv:2410.17434.\nWu, H., et al. (2024). DoubleLLaVA: Efficient Long Video Understanding with Grouped Frame Tokens. arXiv:2410.00907.",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#概要",
    "href": "ja/molmo2/04-vision-language-connector.html#概要",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector は、Vision Transformer (ViT) が抽出した視覚特徴を、Large Language Model (LLM) が処理できる形式に変換する重要なモジュールです。Molmo2 では、標準的な VLM アーキテクチャ [@Clark2024Molmo] に従い、画像とビデオの両方を統一的に処理できる設計を採用しています。\n\n\n\n\n\n\nflowchart TD\n    A[Visual Input&lt;br/&gt;Image or Video] --&gt; B[ViT Encoding&lt;br/&gt;Patch-level Features]\n    B --&gt; C[Multi-layer Feature&lt;br/&gt;Extraction]\n    C --&gt; D{Input Type}\n    D --&gt;|Image| E[2×2 Attention&lt;br/&gt;Pooling]\n    D --&gt;|Video Frame| F[3×3 Attention&lt;br/&gt;Pooling]\n    E --&gt; G[Shared MLP&lt;br/&gt;Projection]\n    F --&gt; G\n    G --&gt; H[Visual Tokens&lt;br/&gt;for LLM]\n\n    style C fill:#e6f0ff\n    style E fill:#ffe6f0\n    style F fill:#ffe6f0\n    style G fill:#f0ffe6\n\n\n\n\nFigure 1: Vision-Language Connector のデータフロー",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#アーキテクチャの詳細",
    "href": "ja/molmo2/04-vision-language-connector.html#アーキテクチャの詳細",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 の Vision-Language Connector は、ViT の単一層ではなく、複数の層から特徴を抽出 します。\n\nThird-to-last layer（最終層から3番目）: 高レベルのセマンティック特徴\nNinth-from-last layer（最終層から9番目）: 中レベルの特徴\n\nこの設計は、先行研究の Molmo [@Clark2024Molmo] に従っており、異なる抽象度の視覚情報を組み合わせることで、より豊かな表現を実現しています。\nViT Layer Structure:\n┌─────────────────────────────────┐\n│  Layer 0 (Input)                │\n│  Layer 1                        │\n│  ...                            │\n│  Layer N-9  ◄── 9th-from-last  │ ─┐\n│  ...                            │  │\n│  Layer N-3  ◄── 3rd-to-last    │ ─┤ Features used\n│  Layer N-2                      │  │ by Connector\n│  Layer N-1                      │  │\n│  Layer N (Output)               │ ─┘\n└─────────────────────────────────┘\n\n\n\nパッチレベルの特徴を削減するために、Attention Pooling を使用します。パッチの平均をクエリとし、各パッチ窓を単一のベクトルに集約します。\n\n\nInput Patches (4×4 example):\n┌─────┬─────┬─────┬─────┐\n│ P₁  │ P₂  │ P₃  │ P₄  │\n├─────┼─────┼─────┼─────┤\n│ P₅  │ P₆  │ P₇  │ P₈  │\n├─────┼─────┼─────┼─────┤\n│ P₉  │ P₁₀ │ P₁₁ │ P₁₂ │\n├─────┼─────┼─────┼─────┤\n│ P₁₃ │ P₁₄ │ P₁₅ │ P₁₆ │\n└─────┴─────┴─────┴─────┘\n\nAfter 2×2 Attention Pooling:\n┌───────────┬───────────┐\n│ T₁        │ T₂        │\n│ (P₁~P₆)   │ (P₃~P₈)   │\n├───────────┼───────────┤\n│ T₃        │ T₄        │\n│ (P₉~P₁₄)  │ (P₁₁~P₁₆) │\n└───────────┴───────────┘\n\nToken count: 16 → 4 (1/4 reduction)\n\n\n\nビデオではフレーム数が多いため、3×3 の窓 を使用してトークン数をさらに削減します。\nInput Patches (9×9 example):\n┌───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│P₁ │P₂ │P₃ │P₄ │P₅ │P₆ │P₇ │P₈ │P₉ │\n├───┼───┼───┼───┼───┼───┼───┼───┼───┤\n│...│...│...│...│...│...│...│...│...│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┘\n        ↓ 3×3 Attention Pooling\n┌─────────┬─────────┬─────────┐\n│   T₁    │   T₂    │   T₃    │\n│ (9 P's) │ (9 P's) │ (9 P's) │\n├─────────┼─────────┼─────────┤\n│   ...   │   ...   │   ...   │\n└─────────┴─────────┴─────────┘\n\nToken count: 81 → 9 (1/9 reduction)\n\n\n\n\n最後に、プールされた特徴は Shared MLP によって投影されます。この MLP は画像とビデオフレームで パラメータを共有 しており、統一的な視覚表現を学習します。\n\n\n\n\n\n\nflowchart LR\n    A[ViT Layer N-9] --&gt; C[Concat]\n    B[ViT Layer N-3] --&gt; C\n    C --&gt; D{Pooling Window}\n    D --&gt;|Image| E[2×2 Attention]\n    D --&gt;|Video| F[3×3 Attention]\n    E --&gt; G[Shared MLP]\n    F --&gt; G\n    G --&gt; H[Visual Tokens]\n\n    style C fill:#e6f0ff\n    style G fill:#f0ffe6\n\n\n\n\nFigure 2: Vision-Language Connector のアーキテクチャ",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#cropping-戦略",
    "href": "ja/molmo2/04-vision-language-connector.html#cropping-戦略",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 は、Multi-crop 戦略を採用しています。\n\n1つのダウンスケール済み全体クロップ + 最大 K 個のオーバーラップタイルクロップ\nトレーニング時: K = 8\n推論時: K = 24（高解像度処理）\n\nK 個のクロップでタイル化できない画像は、ダウンスケールされます。\nOriginal Image:\n┌─────────────────────────────────┐\n│                                 │\n│                                 │\n│        High-res Image           │\n│                                 │\n│                                 │\n└─────────────────────────────────┘\n         ↓\nDownscaled Crop + K Overlapping Crops:\n┌───────┐  ┌─────┬─────┬─────┐\n│ Full  │  │ C₁  │ C₂  │ C₃  │\n│ (DS)  │  ├─────┼─────┼─────┤\n└───────┘  │ C₄  │ C₅  │ C₆  │\n           ├─────┼─────┼─────┤\n           │ C₇  │ C₈  │ ... │\n           └─────┴─────┴─────┘\n\n\n\n\n\n\nNoteColumn Tokens\n\n\n\nMulti-crop 画像の場合、Column tokens を LLM への入力に含めます。これにより、画像のアスペクト比情報を LLM に伝達できます。\n単一クロップ画像（常に正方形）には Column tokens を含めません。\n\n\n\n\n\nビデオの場合、計算コストを削減するために以下の戦略を取ります。\n\nサンプリングレート: S = 2 fps（2秒ごとに1フレーム）\n各フレームは単一クロップとして処理（必要に応じてダウンスケール）\n最大フレーム数: F = 128（標準トレーニング）または F = 384（長尺コンテキストトレーニング）\n\nVideo Timeline:\n0s    1s    2s    3s    4s    5s    ...\n│─────│─────│─────│─────│─────│─────│\n      ↓     ↓     ↓     ↓     ↓\n    Frame₁ Frame₂ Frame₃ ... (@ 2 fps)\n\nIf video length &gt; F/S seconds:\n→ Uniformly sample F frames\n→ Always include LAST frame\n\n\n\n\n\n\nTip最終フレームの特別な扱い\n\n\n\nビデオの 最終フレーム は常に含まれます。これは、多くのビデオプレイヤーが再生終了後に最終フレームを表示するため、ユーザーにとって特別な重要性を持つ可能性があるためです。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#bi-directional-attention",
    "href": "ja/molmo2/04-vision-language-connector.html#bi-directional-attention",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 では、LLM が視覚トークンを処理する際に、画像トークン同士が相互に attend できる ように設計されています [@Miao2024LongVU; @Wu2024DoubleLLaVA]。\n通常の LLM では、因果的マスク（causal mask）により、各トークンは自分より前のトークンにしか注意を向けられません。しかし、Molmo2 では、視覚トークンに対して bi-directional attention を許可しています。\nStandard Causal Attention:\n  T₁  T₂  T₃  T₄\nT₁ ●   ×   ×   ×\nT₂ ●   ●   ×   ×\nT₃ ●   ●   ●   ×\nT₄ ●   ●   ●   ●\n\nBi-directional Attention on Vision Tokens:\n  V₁  V₂  V₃  T₁  T₂\nV₁ ●   ●   ●   ×   ×\nV₂ ●   ●   ●   ×   ×\nV₃ ●   ●   ●   ×   ×\nT₁ ●   ●   ●   ●   ×\nT₂ ●   ●   ●   ●   ●\n\nV: Vision tokens, T: Text tokens\nこれにより、異なるフレームや異なる画像からの視覚トークンが相互に情報を交換でき、時空間的な関係を学習できます。\n\n\n\n\n\n\nImportantBi-directional Attention の効果\n\n\n\nアブレーション研究により、視覚トークンへの Bi-directional attention が 性能を向上させる ことが確認されています。\n特に、ビデオトラッキングやマルチイメージ理解など、複数のフレーム/画像間の関係を捉える必要があるタスクで有効です。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#llm-への入力フォーマット",
    "href": "ja/molmo2/04-vision-language-connector.html#llm-への入力フォーマット",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector によって生成された視覚トークンは、以下の形式で LLM に入力されます。\n\n\n&lt;image_start&gt; [Visual Tokens for Frame₁] &lt;timestamp&gt;0.5s&lt;/timestamp&gt;\n&lt;image_start&gt; [Visual Tokens for Frame₂] &lt;timestamp&gt;1.0s&lt;/timestamp&gt;\n...\n[Subtitle text] &lt;timestamp&gt;0.5s-2.0s&lt;/timestamp&gt;\n\n各フレームの視覚トークンに タイムスタンプ を付加\n字幕が利用可能な場合は、タイムスタンプ付きテキストとして追加\n\n\n\n\n&lt;image_start&gt; [Visual Tokens for Image₁] &lt;image&gt;1&lt;/image&gt;\n&lt;image_start&gt; [Visual Tokens for Image₂] &lt;image&gt;2&lt;/image&gt;\n...\n\n各画像に 画像インデックス を付加\n\n\n\n\n&lt;image_start&gt; [Column Tokens] [Visual Tokens for Full Crop]\n[Visual Tokens for Crop₁] [Visual Tokens for Crop₂] ...\n\nColumn tokens でアスペクト比を伝達\n全体クロップ + 部分クロップのトークンを結合",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#まとめ",
    "href": "ja/molmo2/04-vision-language-connector.html#まとめ",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 の Vision-Language Connector は、以下の特徴を持ちます。\n\nMulti-layer features: ViT の複数層（3rd-to-last, 9th-from-last）から特徴を抽出\nAdaptive pooling: 画像には 2×2、ビデオフレームには 3×3 の Attention Pooling\nShared parameters: 画像とビデオで統一的な MLP projection\nMulti-crop strategy: 高解像度処理のため、最大24個のクロップを使用\nEfficient video processing: 2 fps サンプリング + 最終フレームの保持\nBi-directional attention: 視覚トークン間の相互作用を許可（性能向上）\nColumn tokens: Multi-crop 画像のアスペクト比情報を伝達\n\nこの設計により、Molmo2 は画像とビデオを統一的に処理しながら、計算効率と表現力のバランスを実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#参考文献",
    "href": "ja/molmo2/04-vision-language-connector.html#参考文献",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Clark, C., et al. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models. arXiv:2409.17146.\nMiao, X., et al. (2024). LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. arXiv:2410.17434.\nWu, H., et al. (2024). DoubleLLaVA: Efficient Long Video Understanding with Grouped Frame Tokens. arXiv:2410.00907.",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html",
    "href": "ja/molmo2/02-video-grounding.html",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Video Grounding（ビデオグラウンディング） は、モデルがビデオ内の特定のオブジェクトやイベントを 時空間的に正確に指し示す（grounding） 能力です。従来の Vision-Language Model (VLM) は、「このビデオには何がありますか？」という質問に回答できても、「赤いブロックが何回掴まれましたか？それぞれどこですか？」という質問に対して正確な時刻と位置を返すことはできませんでした。\nMolmo2 は、この gap を埋めるために、Video Pointing と Video Tracking という2つの grounding 機能を実装しています。\n\n\n画像におけるグラウンディング（pointing）は既に標準的な機能となっており、Molmo2 の前身である Molmo1 や GPT-4V、Gemini などでサポートされています。しかし、ビデオにおけるグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nビデオグラウンディングは、以下のような実用的なユースケースで重要です。\n\nロボティクス: 「ロボットが赤いブロックを何回掴んだか？」といった質問に対して、各掴みイベントの時空間座標を返す\nビデオ検索: 「カップがいつテーブルから落ちたか？」という質問に対してカップの軌跡（track）を返す\n生成動画の品質評価: 生成されたビデオに視覚的な異常（artifacts/anomalies）がある箇所を自動検出する\n\n\n\n\n\nMolmo2 は、2種類のビデオグラウンディング機能を提供します。\n\n\nVideo Pointing は、ビデオ内の特定のフレームにおける特定のオブジェクトやイベントの 位置を点（points）で示す タスクです。複数フレームにわたる場合もありますが、各フレームは独立して扱われます。\n例:\n\n質問: 「滝をポイントして」\n回答: &lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 ...\"&gt;waterfall&lt;/points&gt;\n\n特徴:\n\nオブジェクトのカウンティング（counting）と組み合わせて使用されることが多い\n「何個ありますか？」という質問に加えて、「それぞれどこですか？」という空間的な情報を提供\nフレーム間でオブジェクトが移動しても、各フレームでの位置を個別に記録\n\n\n\n\nVideo Tracking は、ビデオ内の特定のオブジェクトを 時間を通して追跡（track） するタスクです。同一のオブジェクトが複数フレームにわたって移動する場合、そのオブジェクトの軌跡を一貫して記録します。\n例:\n\n質問: 「赤い車を追跡して」\n回答: オブジェクトごとに一意の ID を付与し、各フレームでの位置を記録\n\n特徴:\n\nオブジェクトの 一貫性 が重要（同一オブジェクトには同一 ID）\n複雑な自然言語クエリに対応（「左から2番目の選手」「緑のシャツを着た人」など）\n複数オブジェクトの同時追跡をサポート（平均2.28オブジェクト/クエリ）\n\n\n\n\n\n\n\nNotePointing vs Tracking の違い\n\n\n\n\nPointing: フレームごとに独立した位置情報（カウンティング重視）\nTracking: オブジェクトの時間的な一貫性（軌跡重視）\n\n実用上、Pointing は「いつどこにあるか」を知りたい場合に、Tracking は「どう動いたか」を知りたい場合に適しています。\n\n\n\n\n\n\nMolmo2-VideoPoint は、ビデオ内のオブジェクトやイベントをポイントするための人手アノテーションデータセットです。\n\n\n\n動画数: 280k 動画\nクエリ数: 650k 以上\n平均ポイント数: 6 ポイント/動画\nフレームレート: 2 fps でサンプリング\n\n\n\n\nMolmo2-VideoPoint は、以下の8つの多様なカテゴリをカバーしています。\n\nObjects（オブジェクト）: 一般的な物体（「車」「コップ」など）\nAnimals（動物）: 動物の検出とカウンティング\nActions/Events（行動・イベント）: 時間的なイベント（「ジャンプ」「投げる」など）\nReferring expressions（参照表現）: 複雑な記述（「左から2番目の人」など）\nIndirect references（間接参照）: 間接的な指示（「彼が持っている物」など）\nSpatial references（空間参照）: 空間的な関係（「テーブルの上にあるもの」など）\nComparative references（比較参照）: 比較的な記述（「一番大きい犬」など）\nVisual artifacts/anomalies（視覚的な異常）: 生成動画における異常検出\n\n\n\n\n\n\n\nTip生成動画の異常検出\n\n\n\nカテゴリ8の Visual artifacts/anomalies は、AI生成動画における品質評価のために設計されています。約25種類の text-to-video (T2V) モデルで生成された10k動画を使用し、消失する被写体（Vanishing Subject）、物理的な不整合（Physical Incongruity）、時間的な歪み（Temporal Dysmorphia）などの異常を検出する能力を学習します。\n\n\n\n\n\n\nクエリ生成: LLM が Molmo2-Cap で生成されたビデオキャプションからポインティングクエリを生成\nフレーム選択: アノテーターがオブジェクトが出現するフレームを特定（2 fps でサンプリング）\n位置アノテーション: アノテーターがオブジェクトの正確な位置をクリック\nフォーマット: 時刻（フレームインデックス）、カウント、正規化された (x, y) 座標を記録\n\n\n\n\n\nカウント数: 0-5個のオブジェクトが多数を占める（低カウント重視）\n\n中・高カウント例はトレーニング時にアップサンプリング\n\nフレーム数: アノテーション付きフレーム数は左側に偏った分布（多くの例は少数のフレームのみ）\nカテゴリ: Action/Event、Object、Referring expression が最も多い（これらが学習困難なため）\n\n\n\n\n\nMolmo2-VideoTrack は、複雑な自然言語クエリに対応したオブジェクトトラッキングデータセットです。\n\n\n\nビデオクリップ数: 3.6k（トレーニング用）+ 1.3k（評価用）= 合計約5k\nクエリ数: 15k の複雑な自然言語クエリ（トレーニング用）\n平均オブジェクト数: 2.28 オブジェクト/クエリ（多くは複数オブジェクトを追跡）\n平均クエリ長: 8.21 単語/クエリ\n動画長: 最長2分、多くは10-30秒\n平均アノテーション数: 6.08 オブジェクト/動画\n\n\n\n\nMolmo2-VideoTrack は、既存のセグメンテーションおよびバウンディングボックストラッキングデータセットを基に、人手で複雑なテキストクエリを追加したものです。\nセグメンテーションベース（一般的なオブジェクトトラッキング）:\n\nSAM-V, VIPSeg, MOSE, MOSEv2\n\nバウンディングボックスベース（ドメイン特化型）:\n\nスポーツ: TeamTrack, SoccerNet, SportsMOT\n自動運転: BDD100K\n動物: APTv2, AnimalTrack, BFT\nUAV（ドローン）: UAV-MOTD, SeaDrones\n人物: MOT20, PersonPath, DanceTrack\n\n\n\n\n\n\n\nImportantバウンディングボックスからセグメンテーションへの変換\n\n\n\nバウンディングボックスベースのデータセットでは、中心点がオブジェクト上にない可能性があるため、SAM 2 を使用して各バウンディングボックスをセグメンテーションマスクに変換しました。\n変換プロセス:\n\n最初のバウンディングボックスを SAM 2 にプロンプトとして入力\nセグメンテーションマスクを生成し、ビデオ全体に伝播\nIoU が 0.5 未満のトラックは除外\n生成されたマスクから中心付近の点をサンプリング\n\nこれにより、信頼性の高い点ベースのトラッキングアノテーションを得られます。\n\n\n\n\n\nMolmo2-VideoTrack の収集は、Ref-VOS（Referring Video Object Segmentation）のアプローチに従っています。\n\n既存トラックの表示: アノテーターにセグメンテーションまたはバウンディングボックスのトラックを表示\nクエリ作成: アノテーターがオブジェクトのサブセットに適用される 非自明な テキストクエリを作成\n\n例: 「緑のシャツを着た左から2番目の選手」「テーブルの上の赤いカップ」\n\n検証: 別のアノテーターが検証ラウンドでクエリの品質をチェック\n\n検証後、約70%のクエリが保持される\n\n\n\n\n\nMolmo2-VideoTrack は、多様なドメインをカバーしています。\n\n一般的なオブジェクト: 日常的な物体（セグメンテーションデータセットから）\nスポーツ: サッカー選手、チームメンバー、競技者\n交通: 車、歩行者、自転車\n動物: 野生動物、ペット\nUAV: ドローン映像における追跡\n人物: 歩行者、ダンサー\n\n複数オブジェクトの追跡が主な焦点であり、クエリの多くは複数のオブジェクトを同時に記述します（平均3.31オブジェクト/クエリ）。\n\n\n\n\nMolmo2 は、既存のオープンソースデータセットを Pointing と Tracking の形式に変換した Academic データセット も使用しています。\n\n\n既存のオブジェクトトラッキングアノテーションを 49k のポインティング・カウンティング QA に変換しました。\nソースデータセット（6つ）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-DAVIS17\n\n変換プロセス:\n\nオブジェクトが最初に出現するフレームのタイムスタンプを取得\nオブジェクトのマスク内からランダムに点をサンプリング（ガウス分布、マスク中心付近）\nポインティング QA 形式に変換\n\n\n\n\n既存のビデオオブジェクトセグメンテーション（VOS）およびトラッキングデータセットを変換しました。\nセグメンテーションベース（7つの Ref-VOS データセット）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-Youtube-VOS, Ref-DAVIS17\n\nバウンディングボックスベース（11のトラッキングデータセット）:\n\nTrackingNet, VastTrack, GOT-10k, LaSOT, TNL2K, WebUAV, WebUOT, LVOS V1/V2, UW-COT220, TNLLT, YouTube-VIS, MoCA-Video\n\nSAM 2 を使用してバウンディングボックスをセグメンテーションマスクに変換し、点ベースのトラッキングタスクを生成しました。\n\n\n\n\n\n\nNoteAcademicVideoTrack の規模\n\n\n\nAcademicVideoTrack は、トレーニングデータの大部分を占めており、130k のクエリと 800k の例（トークン数ベース）を提供しています。これに対して、Molmo2-VideoTrack は 8k のクエリですが、より複雑で多様なテキストクエリを含んでいます。\n\n\n\n\n\n\nMolmo2 は、ビデオグラウンディングにおいて プロプライエタリモデルを含めて最高水準 の性能を達成しています。\n\n\n以下の表は、BURST-VideoCount（VC）、Molmo2-VideoCount（Molmo2-VC）、Molmo2-VideoPoint（Molmo2-VP）における性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル\nBURST-VC Acc.\nBURST-VC Close Acc.\nMolmo2-VC Acc.\nMolmo2-VC Close Acc.\nMolmo2-VP F1\nMolmo2-VP Recall\nMolmo2-VP Precision\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\n\n\nGPT-5\n43.1\n73.7\n35.8\n50.3\n4.1\n4.4\n4.2\n\n\nGPT-5 mini\n46.0\n73.0\n29.8\n49.3\n2.2\n2.2\n2.2\n\n\nGemini 3 Pro\n44.0\n71.7\n37.1\n53.1\n20.0\n27.4\n19.8\n\n\nGemini 2.5 Pro\n41.6\n70.0\n35.8\n56.5\n13.0\n14.5\n13.6\n\n\nGemini 2.5 Flash\n38.7\n70.0\n31.9\n48.2\n11.1\n11.2\n12.2\n\n\nClaude Sonnet 4.5\n42.4\n72.6\n27.2\n45.1\n3.5\n3.7\n4.3\n\n\nOpen Weights Only\n\n\n\n\n\n\n\n\n\nQwen3-VL-4B\n38.9\n74.7\n25.3\n44.3\n0.0\n0.0\n0.0\n\n\nQwen3-VL-8B\n42.0\n74.4\n29.6\n47.7\n1.5\n1.5\n1.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\n\n\nMolmo2-4B\n61.5\n76.1\n34.3\n56.1\n39.9\n42.7\n39.4\n\n\nMolmo2-8B\n60.8\n75.0\n35.5\n53.3\n38.4\n39.3\n38.7\n\n\nMolmo2-O-7B\n61.6\n76.0\n33.2\n50.5\n35.8\n35.8\n37.9\n\n\n\n\n\n\n\n\n\nTip主要な結果\n\n\n\n\nBURST-VC: Molmo2 は全モデル中で最高精度（61.5% accuracy）を達成\nMolmo2-VP: Molmo2-4B は F1 Score 39.9 で、Gemini 3 Pro（20.0）の 約2倍 の性能\nQwen3-VL との比較: Qwen3-VL はビデオポインティングをほぼサポートしていない（F1 Score 0.0-1.5）\n\nMolmo2 は、オープンウェイトモデルとしてだけでなく、プロプライエタリモデルを含めても最高水準 のビデオポインティング性能を達成しています。\n\n\n評価指標の説明:\n\nAccuracy: 完全一致\nClose Accuracy: 誤差が Δ = 1 + ⌊0.05 × gt⌋ 以内であれば正解（カウント数が多いほど許容誤差が大きい）\nF1, Recall, Precision: 生成された点が ground-truth マスク内にあるかを評価\n\n\n\n\n以下の表は、主要なビデオトラッキングベンチマークにおける性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\nモデル\nMeViS valid J&F\nMeViS valid-u J&F\nRef-YT-VOS valid J&F\nRef-Davis test J&F\nReasonVOS J&F\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\nGPT-5\n23.4\n26.5\n30.9\n25.2\n24.7\n\n\nGPT-5 mini\n15.7\n15.4\n16.2\n8.4\n14.6\n\n\nGemini 3 Pro\n42.5\n51.1\n55.0\n66.6\n52.6\n\n\nGemini 2.5 Pro\n40.7\n52.8\n45.1\n45.6\n44.0\n\n\nGemini 2.5 Flash\n27.6\n31.8\n36.0\n31.6\n26.5\n\n\nOpen Weights Only\n\n\n\n\n\n\n\nQwen3-VL-4B\n29.7\n30.6\n32.1\n44.4\n26.5\n\n\nQwen3-VL-8B\n35.1\n34.4\n48.3\n41.0\n24.9\n\n\nSpecialized Open Models\n\n\n\n\n\n\n\nVideoLISA\n44.4\n53.2\n63.7\n68.8\n47.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\nMolmo2-4B\n56.2\n62.1\n67.2\n65.4\n56.5\n\n\nMolmo2-8B\n56.1\n60.4\n67.8\n64.5\n55.6\n\n\nMolmo2-O-7B\n54.5\n59.8\n64.8\n62.1\n51.9\n\n\n\n\n\n\n\n\n\nImportant特化型モデルとの比較\n\n\n\nVideoLISA は Ref-VOS に特化したモデルであり、一部のベンチマーク（MeViS valid-u, Ref-YT-VOS, Ref-Davis）で Molmo2 と同等またはそれ以上の性能を示しています。しかし、Molmo2 は 汎用的なビデオ理解モデル として、ビデオ QA、キャプション、カウンティングなど幅広いタスクをサポートしている点が異なります。\n\n\n評価指標の説明:\n\nJ&F: セグメンテーションマスクの品質を測る指標（Jaccard Index と Contour Accuracy の平均）\nF1, HOTA: オブジェクトトラッキングの精度を測る指標\n\n主要な結果:\n\nMeViS: Molmo2-4B は J&F 56.2 で、Gemini 3 Pro（42.5）を 13.7ポイント上回る\nRef-YT-VOS: Molmo2-8B は J&F 67.8 で、オープンモデル中で最高（VideoLISA 63.7 を上回る）\nQwen3-VL との比較: Molmo2 は Qwen3-VL-8B（35.1 J&F）の 約1.6倍 の性能\n\n\n\n\n\nMolmo2 は、ビデオグラウンディングの出力に プレーンテキスト座標 を使用しています。これは、特別なトークンや外部ツールを使わずに、LLM のテキスト生成能力だけでグラウンディングを実現するアプローチです。\n\n\n&lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 t^3 count_3 x_3 y_3\"&gt;\nobject_label\n&lt;/points&gt;\n要素の説明:\n\nt^i: フレームインデックス（またはタイムスタンプ）\ncount_i: そのフレームでのオブジェクトのカウント（何個目か）\nx_i, y_i: 正規化された座標（0.0-1.0）\nobject_label: オブジェクトの名前やラベル\n\n\n\n\nトラッキングでは、オブジェクトごとに一意の ID（count_i）を割り当て、複数フレームにわたって同じ ID を維持します。\n&lt;points coords=\"t^1 1 0.45 0.32 t^2 1 0.48 0.35 t^3 1 0.51 0.38\"&gt;\nred car\n&lt;/points&gt;\n&lt;points coords=\"t^1 2 0.62 0.55 t^2 2 0.65 0.57 t^3 2 0.68 0.59\"&gt;\nblue car\n&lt;/points&gt;\nこの例では、1 が赤い車、2 が青い車を示しており、各フレーム（t^1, t^2, t^3）での位置が記録されています。\n\n\n\n\n\n\nNotePlain-Text Coordinates の利点\n\n\n\n\nシンプル: 特別なトークンや外部ツールが不要\n柔軟性: LLM の生成能力をそのまま活用できる\nスケーラビリティ: 複数オブジェクト、複数フレームに自然に拡張可能\n人間可読性: デバッグや分析が容易\n\n一方で、座標の精度は LLM のテキスト生成精度に依存するため、非常に高精度な座標が必要な場合には専用のヘッドを追加するアプローチ（例: Grounding-DINO）の方が有利な場合もあります。\n\n\n\n\n\n\nMolmo2 は、Video Grounding という新しい capability を完全オープンなモデルとして実現しました。\n主要な成果:\n\n2つのグラウンディング機能:\n\nVideo Pointing: フレームごとの位置情報とカウンティング\nVideo Tracking: オブジェクトの時間的な軌跡追跡\n\n大規模な人手アノテーションデータセット:\n\nMolmo2-VideoPoint: 650k クエリ、8つの多様なカテゴリ\nMolmo2-VideoTrack: 15k クエリ、平均2.28オブジェクト/クエリ\n\nAcademic データセットの活用:\n\n既存のオープンソースデータセットを Pointing/Tracking 形式に変換\n49k の Pointing QA、130k の Tracking クエリ\n\nプロプライエタリモデルを上回る性能:\n\nVideo Pointing で F1 Score 39.9（Gemini 3 Pro の約2倍）\nVideo Tracking で J&F 56.2（Gemini 3 Pro より13.7ポイント高い）\n\nPlain-Text Coordinates フォーマット:\n\nシンプルで拡張性の高い出力形式\nLLM の生成能力を直接活用\n\n\nMolmo2 のビデオグラウンディング機能は、ロボティクス、ビデオ検索、生成動画の品質評価など、幅広い実用的なアプリケーションへの道を開きます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#video-grounding-とは",
    "href": "ja/molmo2/02-video-grounding.html#video-grounding-とは",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Video Grounding（ビデオグラウンディング） は、モデルがビデオ内の特定のオブジェクトやイベントを 時空間的に正確に指し示す（grounding） 能力です。従来の Vision-Language Model (VLM) は、「このビデオには何がありますか？」という質問に回答できても、「赤いブロックが何回掴まれましたか？それぞれどこですか？」という質問に対して正確な時刻と位置を返すことはできませんでした。\nMolmo2 は、この gap を埋めるために、Video Pointing と Video Tracking という2つの grounding 機能を実装しています。\n\n\n画像におけるグラウンディング（pointing）は既に標準的な機能となっており、Molmo2 の前身である Molmo1 や GPT-4V、Gemini などでサポートされています。しかし、ビデオにおけるグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nビデオグラウンディングは、以下のような実用的なユースケースで重要です。\n\nロボティクス: 「ロボットが赤いブロックを何回掴んだか？」といった質問に対して、各掴みイベントの時空間座標を返す\nビデオ検索: 「カップがいつテーブルから落ちたか？」という質問に対してカップの軌跡（track）を返す\n生成動画の品質評価: 生成されたビデオに視覚的な異常（artifacts/anomalies）がある箇所を自動検出する",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#video-pointing-vs-video-tracking",
    "href": "ja/molmo2/02-video-grounding.html#video-pointing-vs-video-tracking",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、2種類のビデオグラウンディング機能を提供します。\n\n\nVideo Pointing は、ビデオ内の特定のフレームにおける特定のオブジェクトやイベントの 位置を点（points）で示す タスクです。複数フレームにわたる場合もありますが、各フレームは独立して扱われます。\n例:\n\n質問: 「滝をポイントして」\n回答: &lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 ...\"&gt;waterfall&lt;/points&gt;\n\n特徴:\n\nオブジェクトのカウンティング（counting）と組み合わせて使用されることが多い\n「何個ありますか？」という質問に加えて、「それぞれどこですか？」という空間的な情報を提供\nフレーム間でオブジェクトが移動しても、各フレームでの位置を個別に記録\n\n\n\n\nVideo Tracking は、ビデオ内の特定のオブジェクトを 時間を通して追跡（track） するタスクです。同一のオブジェクトが複数フレームにわたって移動する場合、そのオブジェクトの軌跡を一貫して記録します。\n例:\n\n質問: 「赤い車を追跡して」\n回答: オブジェクトごとに一意の ID を付与し、各フレームでの位置を記録\n\n特徴:\n\nオブジェクトの 一貫性 が重要（同一オブジェクトには同一 ID）\n複雑な自然言語クエリに対応（「左から2番目の選手」「緑のシャツを着た人」など）\n複数オブジェクトの同時追跡をサポート（平均2.28オブジェクト/クエリ）\n\n\n\n\n\n\n\nNotePointing vs Tracking の違い\n\n\n\n\nPointing: フレームごとに独立した位置情報（カウンティング重視）\nTracking: オブジェクトの時間的な一貫性（軌跡重視）\n\n実用上、Pointing は「いつどこにあるか」を知りたい場合に、Tracking は「どう動いたか」を知りたい場合に適しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#molmo2-videopoint-データセット",
    "href": "ja/molmo2/02-video-grounding.html#molmo2-videopoint-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2-VideoPoint は、ビデオ内のオブジェクトやイベントをポイントするための人手アノテーションデータセットです。\n\n\n\n動画数: 280k 動画\nクエリ数: 650k 以上\n平均ポイント数: 6 ポイント/動画\nフレームレート: 2 fps でサンプリング\n\n\n\n\nMolmo2-VideoPoint は、以下の8つの多様なカテゴリをカバーしています。\n\nObjects（オブジェクト）: 一般的な物体（「車」「コップ」など）\nAnimals（動物）: 動物の検出とカウンティング\nActions/Events（行動・イベント）: 時間的なイベント（「ジャンプ」「投げる」など）\nReferring expressions（参照表現）: 複雑な記述（「左から2番目の人」など）\nIndirect references（間接参照）: 間接的な指示（「彼が持っている物」など）\nSpatial references（空間参照）: 空間的な関係（「テーブルの上にあるもの」など）\nComparative references（比較参照）: 比較的な記述（「一番大きい犬」など）\nVisual artifacts/anomalies（視覚的な異常）: 生成動画における異常検出\n\n\n\n\n\n\n\nTip生成動画の異常検出\n\n\n\nカテゴリ8の Visual artifacts/anomalies は、AI生成動画における品質評価のために設計されています。約25種類の text-to-video (T2V) モデルで生成された10k動画を使用し、消失する被写体（Vanishing Subject）、物理的な不整合（Physical Incongruity）、時間的な歪み（Temporal Dysmorphia）などの異常を検出する能力を学習します。\n\n\n\n\n\n\nクエリ生成: LLM が Molmo2-Cap で生成されたビデオキャプションからポインティングクエリを生成\nフレーム選択: アノテーターがオブジェクトが出現するフレームを特定（2 fps でサンプリング）\n位置アノテーション: アノテーターがオブジェクトの正確な位置をクリック\nフォーマット: 時刻（フレームインデックス）、カウント、正規化された (x, y) 座標を記録\n\n\n\n\n\nカウント数: 0-5個のオブジェクトが多数を占める（低カウント重視）\n\n中・高カウント例はトレーニング時にアップサンプリング\n\nフレーム数: アノテーション付きフレーム数は左側に偏った分布（多くの例は少数のフレームのみ）\nカテゴリ: Action/Event、Object、Referring expression が最も多い（これらが学習困難なため）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#molmo2-videotrack-データセット",
    "href": "ja/molmo2/02-video-grounding.html#molmo2-videotrack-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2-VideoTrack は、複雑な自然言語クエリに対応したオブジェクトトラッキングデータセットです。\n\n\n\nビデオクリップ数: 3.6k（トレーニング用）+ 1.3k（評価用）= 合計約5k\nクエリ数: 15k の複雑な自然言語クエリ（トレーニング用）\n平均オブジェクト数: 2.28 オブジェクト/クエリ（多くは複数オブジェクトを追跡）\n平均クエリ長: 8.21 単語/クエリ\n動画長: 最長2分、多くは10-30秒\n平均アノテーション数: 6.08 オブジェクト/動画\n\n\n\n\nMolmo2-VideoTrack は、既存のセグメンテーションおよびバウンディングボックストラッキングデータセットを基に、人手で複雑なテキストクエリを追加したものです。\nセグメンテーションベース（一般的なオブジェクトトラッキング）:\n\nSAM-V, VIPSeg, MOSE, MOSEv2\n\nバウンディングボックスベース（ドメイン特化型）:\n\nスポーツ: TeamTrack, SoccerNet, SportsMOT\n自動運転: BDD100K\n動物: APTv2, AnimalTrack, BFT\nUAV（ドローン）: UAV-MOTD, SeaDrones\n人物: MOT20, PersonPath, DanceTrack\n\n\n\n\n\n\n\nImportantバウンディングボックスからセグメンテーションへの変換\n\n\n\nバウンディングボックスベースのデータセットでは、中心点がオブジェクト上にない可能性があるため、SAM 2 を使用して各バウンディングボックスをセグメンテーションマスクに変換しました。\n変換プロセス:\n\n最初のバウンディングボックスを SAM 2 にプロンプトとして入力\nセグメンテーションマスクを生成し、ビデオ全体に伝播\nIoU が 0.5 未満のトラックは除外\n生成されたマスクから中心付近の点をサンプリング\n\nこれにより、信頼性の高い点ベースのトラッキングアノテーションを得られます。\n\n\n\n\n\nMolmo2-VideoTrack の収集は、Ref-VOS（Referring Video Object Segmentation）のアプローチに従っています。\n\n既存トラックの表示: アノテーターにセグメンテーションまたはバウンディングボックスのトラックを表示\nクエリ作成: アノテーターがオブジェクトのサブセットに適用される 非自明な テキストクエリを作成\n\n例: 「緑のシャツを着た左から2番目の選手」「テーブルの上の赤いカップ」\n\n検証: 別のアノテーターが検証ラウンドでクエリの品質をチェック\n\n検証後、約70%のクエリが保持される\n\n\n\n\n\nMolmo2-VideoTrack は、多様なドメインをカバーしています。\n\n一般的なオブジェクト: 日常的な物体（セグメンテーションデータセットから）\nスポーツ: サッカー選手、チームメンバー、競技者\n交通: 車、歩行者、自転車\n動物: 野生動物、ペット\nUAV: ドローン映像における追跡\n人物: 歩行者、ダンサー\n\n複数オブジェクトの追跡が主な焦点であり、クエリの多くは複数のオブジェクトを同時に記述します（平均3.31オブジェクト/クエリ）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#academic-データセット",
    "href": "ja/molmo2/02-video-grounding.html#academic-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、既存のオープンソースデータセットを Pointing と Tracking の形式に変換した Academic データセット も使用しています。\n\n\n既存のオブジェクトトラッキングアノテーションを 49k のポインティング・カウンティング QA に変換しました。\nソースデータセット（6つ）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-DAVIS17\n\n変換プロセス:\n\nオブジェクトが最初に出現するフレームのタイムスタンプを取得\nオブジェクトのマスク内からランダムに点をサンプリング（ガウス分布、マスク中心付近）\nポインティング QA 形式に変換\n\n\n\n\n既存のビデオオブジェクトセグメンテーション（VOS）およびトラッキングデータセットを変換しました。\nセグメンテーションベース（7つの Ref-VOS データセット）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-Youtube-VOS, Ref-DAVIS17\n\nバウンディングボックスベース（11のトラッキングデータセット）:\n\nTrackingNet, VastTrack, GOT-10k, LaSOT, TNL2K, WebUAV, WebUOT, LVOS V1/V2, UW-COT220, TNLLT, YouTube-VIS, MoCA-Video\n\nSAM 2 を使用してバウンディングボックスをセグメンテーションマスクに変換し、点ベースのトラッキングタスクを生成しました。\n\n\n\n\n\n\nNoteAcademicVideoTrack の規模\n\n\n\nAcademicVideoTrack は、トレーニングデータの大部分を占めており、130k のクエリと 800k の例（トークン数ベース）を提供しています。これに対して、Molmo2-VideoTrack は 8k のクエリですが、より複雑で多様なテキストクエリを含んでいます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#評価結果-プロプライエタリモデルを上回る性能",
    "href": "ja/molmo2/02-video-grounding.html#評価結果-プロプライエタリモデルを上回る性能",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、ビデオグラウンディングにおいて プロプライエタリモデルを含めて最高水準 の性能を達成しています。\n\n\n以下の表は、BURST-VideoCount（VC）、Molmo2-VideoCount（Molmo2-VC）、Molmo2-VideoPoint（Molmo2-VP）における性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル\nBURST-VC Acc.\nBURST-VC Close Acc.\nMolmo2-VC Acc.\nMolmo2-VC Close Acc.\nMolmo2-VP F1\nMolmo2-VP Recall\nMolmo2-VP Precision\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\n\n\nGPT-5\n43.1\n73.7\n35.8\n50.3\n4.1\n4.4\n4.2\n\n\nGPT-5 mini\n46.0\n73.0\n29.8\n49.3\n2.2\n2.2\n2.2\n\n\nGemini 3 Pro\n44.0\n71.7\n37.1\n53.1\n20.0\n27.4\n19.8\n\n\nGemini 2.5 Pro\n41.6\n70.0\n35.8\n56.5\n13.0\n14.5\n13.6\n\n\nGemini 2.5 Flash\n38.7\n70.0\n31.9\n48.2\n11.1\n11.2\n12.2\n\n\nClaude Sonnet 4.5\n42.4\n72.6\n27.2\n45.1\n3.5\n3.7\n4.3\n\n\nOpen Weights Only\n\n\n\n\n\n\n\n\n\nQwen3-VL-4B\n38.9\n74.7\n25.3\n44.3\n0.0\n0.0\n0.0\n\n\nQwen3-VL-8B\n42.0\n74.4\n29.6\n47.7\n1.5\n1.5\n1.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\n\n\nMolmo2-4B\n61.5\n76.1\n34.3\n56.1\n39.9\n42.7\n39.4\n\n\nMolmo2-8B\n60.8\n75.0\n35.5\n53.3\n38.4\n39.3\n38.7\n\n\nMolmo2-O-7B\n61.6\n76.0\n33.2\n50.5\n35.8\n35.8\n37.9\n\n\n\n\n\n\n\n\n\nTip主要な結果\n\n\n\n\nBURST-VC: Molmo2 は全モデル中で最高精度（61.5% accuracy）を達成\nMolmo2-VP: Molmo2-4B は F1 Score 39.9 で、Gemini 3 Pro（20.0）の 約2倍 の性能\nQwen3-VL との比較: Qwen3-VL はビデオポインティングをほぼサポートしていない（F1 Score 0.0-1.5）\n\nMolmo2 は、オープンウェイトモデルとしてだけでなく、プロプライエタリモデルを含めても最高水準 のビデオポインティング性能を達成しています。\n\n\n評価指標の説明:\n\nAccuracy: 完全一致\nClose Accuracy: 誤差が Δ = 1 + ⌊0.05 × gt⌋ 以内であれば正解（カウント数が多いほど許容誤差が大きい）\nF1, Recall, Precision: 生成された点が ground-truth マスク内にあるかを評価\n\n\n\n\n以下の表は、主要なビデオトラッキングベンチマークにおける性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\nモデル\nMeViS valid J&F\nMeViS valid-u J&F\nRef-YT-VOS valid J&F\nRef-Davis test J&F\nReasonVOS J&F\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\nGPT-5\n23.4\n26.5\n30.9\n25.2\n24.7\n\n\nGPT-5 mini\n15.7\n15.4\n16.2\n8.4\n14.6\n\n\nGemini 3 Pro\n42.5\n51.1\n55.0\n66.6\n52.6\n\n\nGemini 2.5 Pro\n40.7\n52.8\n45.1\n45.6\n44.0\n\n\nGemini 2.5 Flash\n27.6\n31.8\n36.0\n31.6\n26.5\n\n\nOpen Weights Only\n\n\n\n\n\n\n\nQwen3-VL-4B\n29.7\n30.6\n32.1\n44.4\n26.5\n\n\nQwen3-VL-8B\n35.1\n34.4\n48.3\n41.0\n24.9\n\n\nSpecialized Open Models\n\n\n\n\n\n\n\nVideoLISA\n44.4\n53.2\n63.7\n68.8\n47.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\nMolmo2-4B\n56.2\n62.1\n67.2\n65.4\n56.5\n\n\nMolmo2-8B\n56.1\n60.4\n67.8\n64.5\n55.6\n\n\nMolmo2-O-7B\n54.5\n59.8\n64.8\n62.1\n51.9\n\n\n\n\n\n\n\n\n\nImportant特化型モデルとの比較\n\n\n\nVideoLISA は Ref-VOS に特化したモデルであり、一部のベンチマーク（MeViS valid-u, Ref-YT-VOS, Ref-Davis）で Molmo2 と同等またはそれ以上の性能を示しています。しかし、Molmo2 は 汎用的なビデオ理解モデル として、ビデオ QA、キャプション、カウンティングなど幅広いタスクをサポートしている点が異なります。\n\n\n評価指標の説明:\n\nJ&F: セグメンテーションマスクの品質を測る指標（Jaccard Index と Contour Accuracy の平均）\nF1, HOTA: オブジェクトトラッキングの精度を測る指標\n\n主要な結果:\n\nMeViS: Molmo2-4B は J&F 56.2 で、Gemini 3 Pro（42.5）を 13.7ポイント上回る\nRef-YT-VOS: Molmo2-8B は J&F 67.8 で、オープンモデル中で最高（VideoLISA 63.7 を上回る）\nQwen3-VL との比較: Molmo2 は Qwen3-VL-8B（35.1 J&F）の 約1.6倍 の性能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#pointing-フォーマット-plain-text-coordinates",
    "href": "ja/molmo2/02-video-grounding.html#pointing-フォーマット-plain-text-coordinates",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、ビデオグラウンディングの出力に プレーンテキスト座標 を使用しています。これは、特別なトークンや外部ツールを使わずに、LLM のテキスト生成能力だけでグラウンディングを実現するアプローチです。\n\n\n&lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 t^3 count_3 x_3 y_3\"&gt;\nobject_label\n&lt;/points&gt;\n要素の説明:\n\nt^i: フレームインデックス（またはタイムスタンプ）\ncount_i: そのフレームでのオブジェクトのカウント（何個目か）\nx_i, y_i: 正規化された座標（0.0-1.0）\nobject_label: オブジェクトの名前やラベル\n\n\n\n\nトラッキングでは、オブジェクトごとに一意の ID（count_i）を割り当て、複数フレームにわたって同じ ID を維持します。\n&lt;points coords=\"t^1 1 0.45 0.32 t^2 1 0.48 0.35 t^3 1 0.51 0.38\"&gt;\nred car\n&lt;/points&gt;\n&lt;points coords=\"t^1 2 0.62 0.55 t^2 2 0.65 0.57 t^3 2 0.68 0.59\"&gt;\nblue car\n&lt;/points&gt;\nこの例では、1 が赤い車、2 が青い車を示しており、各フレーム（t^1, t^2, t^3）での位置が記録されています。\n\n\n\n\n\n\nNotePlain-Text Coordinates の利点\n\n\n\n\nシンプル: 特別なトークンや外部ツールが不要\n柔軟性: LLM の生成能力をそのまま活用できる\nスケーラビリティ: 複数オブジェクト、複数フレームに自然に拡張可能\n人間可読性: デバッグや分析が容易\n\n一方で、座標の精度は LLM のテキスト生成精度に依存するため、非常に高精度な座標が必要な場合には専用のヘッドを追加するアプローチ（例: Grounding-DINO）の方が有利な場合もあります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#まとめ",
    "href": "ja/molmo2/02-video-grounding.html#まとめ",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、Video Grounding という新しい capability を完全オープンなモデルとして実現しました。\n主要な成果:\n\n2つのグラウンディング機能:\n\nVideo Pointing: フレームごとの位置情報とカウンティング\nVideo Tracking: オブジェクトの時間的な軌跡追跡\n\n大規模な人手アノテーションデータセット:\n\nMolmo2-VideoPoint: 650k クエリ、8つの多様なカテゴリ\nMolmo2-VideoTrack: 15k クエリ、平均2.28オブジェクト/クエリ\n\nAcademic データセットの活用:\n\n既存のオープンソースデータセットを Pointing/Tracking 形式に変換\n49k の Pointing QA、130k の Tracking クエリ\n\nプロプライエタリモデルを上回る性能:\n\nVideo Pointing で F1 Score 39.9（Gemini 3 Pro の約2倍）\nVideo Tracking で J&F 56.2（Gemini 3 Pro より13.7ポイント高い）\n\nPlain-Text Coordinates フォーマット:\n\nシンプルで拡張性の高い出力形式\nLLM の生成能力を直接活用\n\n\nMolmo2 のビデオグラウンディング機能は、ロボティクス、ビデオ検索、生成動画の品質評価など、幅広い実用的なアプリケーションへの道を開きます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html",
    "href": "ja/molmo2/00-overview.html",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備えたことです。\n従来の VLM は画像や動画の内容を理解して説明することはできましたが、「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示す（grounding）能力が不足していました。Molmo2 は、ビデオ内の時空間的なポインティングとトラッキングを実現し、オープンソースモデルの中で最高水準の性能を達成しています。\n論文: arXiv:2601.10611\n主な貢献:\n\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）\nビデオグラウンディング（pointing & tracking）の実現\n超詳細なビデオキャプション（平均924語/動画）\n完全オープン（モデル、データ、コード）\n\nモデルサイズ:\n\nMolmo2-4B（Qwen3 LLM ベース）\nMolmo2-8B（Qwen3 LLM ベース）\nMolmo2-O-7B（OLMo LLM ベース、完全オープン）\n\n\n\n\n現在、最も強力な Video-Language Model (VLM) はプロプライエタリであり、ウェイト、データ、トレーニングレシピが公開されていません。また、オープンウェイトモデルの多くは、プロプライエタリモデルから合成データを生成する「蒸留」に依存しており、完全に独立したオープンな基盤が不足していました。\nさらに、既存の VLM には グラウンディング（grounding） 能力が欠けています。グラウンディングとは、モデルが「ロボットが赤いブロックを何回掴んだか？」という質問に対して、各掴みイベントの時空間座標を出力したり、「カップがいつテーブルから落ちたか？」に対してカップの軌跡（track）を返したりする能力です。\n画像グラウンディングは既に標準的な機能ですが、ビデオグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nMolmo2 は、この gap を埋めるために開発されました。\n\n\n\nMolmo2 の核心は、9つの新規データセット です。すべてプロプライエタリモデルからの蒸留を一切使用せず、人手アノテーションと LLM ベースの合成パイプラインで構築されています。\n\n\n\n\n\n\nImportantプロプライエタリモデル非依存の重要性\n\n\n\n多くのオープンモデル（LLaVA-Video, PLM, ShareGPT4Video など）は、GPT-4V や Gemini などのプロプライエタリモデルから合成データを生成する「蒸留」アプローチを採用しています。\nこの手法には以下の問題があります：\n\n透明性の欠如: プロプライエタリモデルの能力に依存するため、データの品質が不透明\nバイアスの継承: プロプライエタリモデルのバイアスや誤りがそのまま継承される\n改善の限界: 元モデルを超える性能を達成することが困難\n\nMolmo2 は、人手アノテーションと自前モデル（Molmo, Claude Sonnet 4.5）のみを使用することで、完全に独立したデータセット構築を実現しています。これにより、オープンソースコミュニティが SOTA を超える基盤を得られます。\n\n\n\n\n\n内容: 104k のビデオレベルキャプション + 431k のクリップレベルキャプション\n特徴: 平均 924 語/動画 という超詳細なキャプション\n\n既存データセットとの比較: Video Localized Narratives (75語), ShareGPT4-Video (280語), LLaVA-Video (547語)\n\nパイプライン:\n\nアノテーターが短いクリップを音声で説明（タイピングより詳細に記述可能）\nWhisper-1 で文字起こし\nLLM で文章を整形\nMolmo でフレームレベルのキャプションを生成し、統合\n\n\n\n詳細: Dense Video Captioning\n\n\n\n\n\n内容: 140k のビデオ QA ペア\n特徴: 人手による詳細な質問と回答\nパイプライン:\n\nビデオを31カテゴリにクラスタリングして多様性を確保\nアノテーターが詳細な質問を作成\nClaude Sonnet 4.5 が初期回答を生成\nアノテーターが反復的に回答を改善\n\n\n\n\n\n\nCapQA: 1M QA ペア（200k 動画、5 QA/動画）\n\nビデオをシーンに分割し、各シーンをキャプション化\nLLM がキャプションから QA を生成\n\nSubtitleQA: 300k QA ペア（100k 動画、3 QA/動画）\n\nWhisper-1 で字幕を抽出\n視覚情報と字幕の両方を使う推論問題を生成\n\n\n\n\n\n\n内容: 650k のビデオポインティングクエリ（280k 動画、平均6ポイント/動画）\nカテゴリ: 8種類\n\nObjects, Animals, Actions/Events\nReferring expressions, Indirect references\nSpatial references, Comparative references\nVisual artifacts/anomalies（生成動画用）\n\nパイプライン:\n\nLLM がキャプションからクエリを生成\nアノテーターがフレーム（2 fps）と正確な位置をクリック\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\n内容: 3.6k ビデオクリップ、15k の複雑な自然言語クエリ（平均2.28オブジェクト/クエリ）\n特徴: 既存のトラッキングアノテーションに対して、複雑なテキストクエリを作成\nパイプライン:\n\nセグメンテーションまたはバウンディングボックスのトラックを表示\nアノテーターがオブジェクトのサブセットに適用される非自明なクエリを作成\n別ラウンドで検証\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\nVideoPoint: 6つのデータセットから49k のポインティング・カウンティング QA に変換\nVideoTrack: 7つの Ref-VOS データセット + 11のバウンディングボックストラッキングデータセットを変換（SAM-2 でセグメンテーションマスク生成）\n\n\n\n\n\n内容: 45k 画像セット（96k ユニーク画像）、72k QA ペア\n特徴: 意味的に関連する画像セット（2-5枚、平均2.73枚）に対する QA\nパイプライン:\n\nキャプションの類似度で画像をグルーピング\nアノテーターが質問を作成\nLLM との反復ループで回答を改善\n\n\n\n詳細: Multi-Image Understanding\n\n\n\n\n\nMultiImagePoint: 470k のポインティング・カウンティング例（PixMo-Points からクラスタリングで生成）\nSynMultiImageQA: 188k の合成マルチイメージ例（CoSyn を拡張、チャート・表・文書など）\n\n\n\n\n\nMolmo2 は、標準的な VLM アーキテクチャを採用しています。\n┌─────────────────────────────────────────────────────────────┐\n│  Video Input (max 128 frames @ 2fps, or 384 for long ctx)  │\n│  or Image Input (1 crop + up to K=8 overlapping crops)     │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision Transformer (ViT)                                   │\n│  - Extracts patch-level features                            │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision-Language Connector                                  │\n│  - Uses features from 3rd-to-last & 9th-from-last ViT layer │\n│  - Attention pooling: 2x2 for images, 3x3 for video frames  │\n│  - Shared MLP projection                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  LLM (Qwen3 or OLMo)                                        │\n│  - Visual tokens + text timestamps (video) or image indices │\n│  - Bi-directional attention between vision tokens           │\n│  - Output: text + &lt;points&gt; (for grounding)                  │\n└─────────────────────────────────────────────────────────────┘\n主要な設計選択:\n\nCropping: 画像は最大24クロップ（推論時）、動画は2 fpsサンプリング\nBi-directional attention: 画像トークン同士が相互に attend 可能（性能向上）\nPointing フォーマット: 正規化された (x, y, timestamp/image_index, object_id) をプレーンテキストで出力\n\n\n詳細: Vision-Language Connector\n\n\n\n\n\n\nMolmo2 は 3段階 でトレーニングされます。\n\n\n\nデータ: PixMo-Cap（キャプション）、PixMo-Points（ポインティング）、Tulu（NLP）\nミキシング比率: 60% キャプション、30% ポインティング、10% NLP\nステップ数: 32k ステップ、バッチサイズ128（約4エポック）\n学習率: ViT、Connector、LLM で個別に設定\n\n\n\n\n\nデータ: PixMo + Molmo2 データセット + オープンソースビデオ/画像データセット\nカテゴリ別サンプリング: 手動で調整したサンプリングレート（Table 1参照）\nステップ数: 30k ステップ、バッチサイズ128、最大シーケンス長16,384\n\n\n\n\nカテゴリ\nサンプリング率\nデータセット数\n例数\n\n\n\n\nCaptions/Long QA\n13.6%\n6\n1.2M\n\n\nImage QA\n22.7%\n32\n2.4M\n\n\nVideo QA\n18.2%\n32\n2.4M\n\n\nImage Pointing\n9.1%\n4\n1.1M\n\n\nVideo Pointing\n13.6%\n7\n0.37M\n\n\nVideo Tracking\n13.6%\n22\n0.80M\n\n\nNLP\n9.1%\n1\n0.99M\n\n\n\n\n\n\n\nコンテキスト長: 36,864（Stage 2の2.25倍）\nフレーム数: F = 384（Stage 2 の3倍）\nステップ数: 2k ステップ\n並列化: Context Parallelism (CP) を使用、8 GPU で処理\n注意: オーバーヘッドが大きいため短期間のみ実施\n\n\n詳細: Long-Context Training\n\n\n\n\n\n\n\nデータには、単一トークン出力の多肢選択問題から、4000+トークンの長いビデオキャプションまで含まれます。長い出力が損失の大部分を占めてしまうと、短い回答タスクの性能が低下します。\n解決策: タスクごとに重み付けを調整\n\nビデオキャプション: 重み 0.1\nポインティング: 重み 0.2\nその他: \\(\\frac{4}{\\sqrt{n}}\\)（\\(n\\) = 回答トークン数）\n\n\n詳細: Token Weighting\n\n\n\n\n例によってトークン数が大きく異なる（数百～16k+）ため、padding を避けるために packing を使用します。Vision-language モデルでは、ViT 用のクロップと LLM 用のトークンの両方を効率的にパックする必要があります。\nMolmo2 は、オンザフライでパッキングするアルゴリズムを開発し、15倍 のトレーニング効率を達成しています。\n\n詳細: Packing & Message Trees\n\n\n\n\n1つの画像/動画に複数のアノテーションがある場合、メッセージツリー としてエンコードします。視覚入力が最初のメッセージとなり、各アノテーションが異なるブランチになります。ツリーは単一シーケンスに線形化され、カスタムアテンションマスクによってブランチ間のクロスアテンションを防ぎます。\n平均して、データ内の例には4つのアノテーションがあり、packing により16,348トークンのシーケンスに平均3.8例を詰め込むことができています。\n\n詳細: Packing & Message Trees\n\n\n\n\n\n\n\n\nMolmo2 は、標準的なビデオベンチマークと新規のキャプション・カウンティングベンチマークで評価されています。\n主要な結果:\n\n短尺ビデオ理解: オープンウェイトモデル中で SOTA\n\nNextQA: 86.2（Molmo2-8B）\nPerceptionTest: 82.1\nMVBench: 75.9\nMotionBench: 62.2\n\nキャプション: Molmo2-CapTest で F1 Score 43.2（Molmo2-8B）\n\nGPT-5 (50.1)、Gemini 2.5 Pro (42.1) に次ぐ性能\n\nカウンティング: Molmo2-VideoCount で 35.5% accuracy（Molmo2-8B）\n\nQwen3-VL-8B (29.6%) を大きく上回る\n\n長尺ビデオ: 最良のオープンウェイトモデル（Eagle2.5-8B など）には及ばない\n\n原因: オープンソースの長尺（10+分）トレーニングデータ不足\n\n\n\n\n\n\n\n\nNote長尺ビデオでの課題\n\n\n\n\n\nMolmo2 は、以下の長尺ビデオベンチマークで課題を抱えています：\n\nLongVideoBench: 67.5（Eagle2.5-8B: 66.4、PLM-8B: 56.9）\nMLVU: 60.2（Eagle2.5-8B: 60.4、PLM-8B: 52.6）\nLVBench: 52.8（Eagle2.5-8B: 50.9、PLM-8B: 44.5）\n\n原因:\n\nオープンデータ不足: 10分以上の動画に対する高品質なアノテーションが不足\n計算制約: Long-Context Training（Stage 3）は 2,000 ステップのみ実施（オーバーヘッドが大きい）\nトレードオフ: キャプション品質を優先したため、長尺ビデオタスクの性能がやや低下\n\nただし、Molmo2 の長尺ビデオ性能は依然として多くのオープンモデルを上回っており、完全オープンデータのみを使用していることを考慮すれば十分な成果です。\n\n詳細: Long-Context Training\n\n\n\n\nHuman Preference Study:\n\nElo スコア: 1057（Molmo2-8B）\nGemini 3 Pro (1082)、Gemini 2.5 Flash (1084) に次ぐ5位\n完全オープンモデルとしては最高性能\n\n\n\n\nMolmo2 の最大の強みは ビデオグラウンディング です。\nVideo Pointing:\n\nMolmo2-VP ベンチマーク（新規）で F1 Score 38.4\n\nGemini 3 Pro (20.0) を大きく上回る\nプロプライエタリモデルを含めて最高性能\n\n\nVideo Tracking:\n\nBURST（test）で accuracy 56.2\nMolmo2-VC（新規ベンチマーク）で J&F 41.1\n\nGemini 3 Pro を上回る（詳細はベンチマークによる）\n\n\n既存のオープンウェイトモデル（Qwen3-VL など）は、ビデオトラッキング機能を提供していないため、Molmo2 が新たな capability を開拓したと言えます。\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\nMolmo2 は画像タスクでも強力な性能を維持しています。\n\nMMMU: 47.9（Molmo2-8B）\nMathVista: 63.1\nChartQA: 79.5\nAI2D: 84.5\n\nビデオ能力を追加しても、画像タスクの性能を損なっていないことが確認されています。\n\n\n\n論文では、以下の要素の影響を検証しています。\n\nBi-directional attention on vision tokens: 有効（性能向上）\nToken weighting: 有効（長短出力のバランス改善）\nPacking: 15倍の効率向上\nMessage trees: 複数アノテーションの効率的な学習\n\n\n詳細: Token Weighting, Packing & Message Trees\n\n\n\n\n\nMolmo2 は、以下の研究の流れを統合しています。\n\nVision-Language Models: GPT-4V, Gemini, LLaVA, InternVL\nVideo Understanding: VideoChat, LLaVA-Video, PLM, Eagle\nGrounding: KOSMOS-2, Ferret, GPT-4V with set-of-mark\nVideo Grounding: Grounding-DINO, SAM-2, Ref-VOS\nOpen Data: PixMo, CoSyn, ShareGPT4Video\n\nMolmo2 の独自性は、完全オープン かつ ビデオグラウンディング を実現した点にあります。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\n\n\nプロプライエタリモデル:\n\nGPT-4V / GPT-5: 画像グラウンディングは可能だが、ビデオトラッキングは非サポート\nGemini 2.5 Pro / 3 Pro: 限定的なビデオグラウンディング機能を持つが、Molmo2 の pointing/tracking 性能に及ばない\nClaude Sonnet 4.5: ビデオグラウンディング機能なし\n\nオープンウェイトモデル:\n\nQwen3-VL（4B/8B）: ビデオ理解は強力だが、ビデオグラウンディング機能なし。Molmo2 が counting で上回る\nInternVL3.5（4B/8B）: 画像タスクに特化、ビデオグラウンディングなし\nLLaVA-Video-7B: プロプライエタリモデルから蒸留されたデータを使用。グラウンディング機能なし\nPLM（3B/8B）: 詳細なキャプションデータを持つが、Molmo2 より平均語数が少ない（Molmo2: 924語 vs PLM: 不明）\n\n完全オープンモデル:\n\nMolmo2-O-7B: OLMo LLM を使用した唯一の完全オープンモデル（ViT は依然としてプロプライエタリ）\n\nMolmo2 は、ビデオグラウンディング という新しい capability を開拓し、プロプライエタリモデルを含めて最高性能を達成した点で、他モデルと一線を画しています。\n\n\n\n\n\n\nMolmo2 は、完全オープンな VLM として、以下を達成しました。\n\n9つの新規データセット を構築（プロプライエタリモデルへの依存ゼロ）\nビデオグラウンディング（pointing & tracking）を実現\n短尺ビデオ理解 でオープンモデル中 SOTA\nキャプション・カウンティング でプロプライエタリモデルに迫る性能\n完全オープン（モデル、データ、コード）\n\n課題として、長尺ビデオ（10+分）でのパフォーマンスは最良のオープンウェイトモデルに及びませんが、これはオープンソースの長尺データ不足が原因です。\nMolmo2 は、オープンソースコミュニティが SOTA の VLM を構築するための強固な基盤を提供します。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#概要",
    "href": "ja/molmo2/00-overview.html#概要",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備えたことです。\n従来の VLM は画像や動画の内容を理解して説明することはできましたが、「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示す（grounding）能力が不足していました。Molmo2 は、ビデオ内の時空間的なポインティングとトラッキングを実現し、オープンソースモデルの中で最高水準の性能を達成しています。\n論文: arXiv:2601.10611\n主な貢献:\n\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）\nビデオグラウンディング（pointing & tracking）の実現\n超詳細なビデオキャプション（平均924語/動画）\n完全オープン（モデル、データ、コード）\n\nモデルサイズ:\n\nMolmo2-4B（Qwen3 LLM ベース）\nMolmo2-8B（Qwen3 LLM ベース）\nMolmo2-O-7B（OLMo LLM ベース、完全オープン）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#モチベーション-video-grounding-の重要性",
    "href": "ja/molmo2/00-overview.html#モチベーション-video-grounding-の重要性",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "現在、最も強力な Video-Language Model (VLM) はプロプライエタリであり、ウェイト、データ、トレーニングレシピが公開されていません。また、オープンウェイトモデルの多くは、プロプライエタリモデルから合成データを生成する「蒸留」に依存しており、完全に独立したオープンな基盤が不足していました。\nさらに、既存の VLM には グラウンディング（grounding） 能力が欠けています。グラウンディングとは、モデルが「ロボットが赤いブロックを何回掴んだか？」という質問に対して、各掴みイベントの時空間座標を出力したり、「カップがいつテーブルから落ちたか？」に対してカップの軌跡（track）を返したりする能力です。\n画像グラウンディングは既に標準的な機能ですが、ビデオグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nMolmo2 は、この gap を埋めるために開発されました。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#データセット-9つの新規データセット",
    "href": "ja/molmo2/00-overview.html#データセット-9つの新規データセット",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 の核心は、9つの新規データセット です。すべてプロプライエタリモデルからの蒸留を一切使用せず、人手アノテーションと LLM ベースの合成パイプラインで構築されています。\n\n\n\n\n\n\nImportantプロプライエタリモデル非依存の重要性\n\n\n\n多くのオープンモデル（LLaVA-Video, PLM, ShareGPT4Video など）は、GPT-4V や Gemini などのプロプライエタリモデルから合成データを生成する「蒸留」アプローチを採用しています。\nこの手法には以下の問題があります：\n\n透明性の欠如: プロプライエタリモデルの能力に依存するため、データの品質が不透明\nバイアスの継承: プロプライエタリモデルのバイアスや誤りがそのまま継承される\n改善の限界: 元モデルを超える性能を達成することが困難\n\nMolmo2 は、人手アノテーションと自前モデル（Molmo, Claude Sonnet 4.5）のみを使用することで、完全に独立したデータセット構築を実現しています。これにより、オープンソースコミュニティが SOTA を超える基盤を得られます。\n\n\n\n\n\n内容: 104k のビデオレベルキャプション + 431k のクリップレベルキャプション\n特徴: 平均 924 語/動画 という超詳細なキャプション\n\n既存データセットとの比較: Video Localized Narratives (75語), ShareGPT4-Video (280語), LLaVA-Video (547語)\n\nパイプライン:\n\nアノテーターが短いクリップを音声で説明（タイピングより詳細に記述可能）\nWhisper-1 で文字起こし\nLLM で文章を整形\nMolmo でフレームレベルのキャプションを生成し、統合\n\n\n\n詳細: Dense Video Captioning\n\n\n\n\n\n内容: 140k のビデオ QA ペア\n特徴: 人手による詳細な質問と回答\nパイプライン:\n\nビデオを31カテゴリにクラスタリングして多様性を確保\nアノテーターが詳細な質問を作成\nClaude Sonnet 4.5 が初期回答を生成\nアノテーターが反復的に回答を改善\n\n\n\n\n\n\nCapQA: 1M QA ペア（200k 動画、5 QA/動画）\n\nビデオをシーンに分割し、各シーンをキャプション化\nLLM がキャプションから QA を生成\n\nSubtitleQA: 300k QA ペア（100k 動画、3 QA/動画）\n\nWhisper-1 で字幕を抽出\n視覚情報と字幕の両方を使う推論問題を生成\n\n\n\n\n\n\n内容: 650k のビデオポインティングクエリ（280k 動画、平均6ポイント/動画）\nカテゴリ: 8種類\n\nObjects, Animals, Actions/Events\nReferring expressions, Indirect references\nSpatial references, Comparative references\nVisual artifacts/anomalies（生成動画用）\n\nパイプライン:\n\nLLM がキャプションからクエリを生成\nアノテーターがフレーム（2 fps）と正確な位置をクリック\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\n内容: 3.6k ビデオクリップ、15k の複雑な自然言語クエリ（平均2.28オブジェクト/クエリ）\n特徴: 既存のトラッキングアノテーションに対して、複雑なテキストクエリを作成\nパイプライン:\n\nセグメンテーションまたはバウンディングボックスのトラックを表示\nアノテーターがオブジェクトのサブセットに適用される非自明なクエリを作成\n別ラウンドで検証\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\nVideoPoint: 6つのデータセットから49k のポインティング・カウンティング QA に変換\nVideoTrack: 7つの Ref-VOS データセット + 11のバウンディングボックストラッキングデータセットを変換（SAM-2 でセグメンテーションマスク生成）\n\n\n\n\n\n内容: 45k 画像セット（96k ユニーク画像）、72k QA ペア\n特徴: 意味的に関連する画像セット（2-5枚、平均2.73枚）に対する QA\nパイプライン:\n\nキャプションの類似度で画像をグルーピング\nアノテーターが質問を作成\nLLM との反復ループで回答を改善\n\n\n\n詳細: Multi-Image Understanding\n\n\n\n\n\nMultiImagePoint: 470k のポインティング・カウンティング例（PixMo-Points からクラスタリングで生成）\nSynMultiImageQA: 188k の合成マルチイメージ例（CoSyn を拡張、チャート・表・文書など）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#アーキテクチャ",
    "href": "ja/molmo2/00-overview.html#アーキテクチャ",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、標準的な VLM アーキテクチャを採用しています。\n┌─────────────────────────────────────────────────────────────┐\n│  Video Input (max 128 frames @ 2fps, or 384 for long ctx)  │\n│  or Image Input (1 crop + up to K=8 overlapping crops)     │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision Transformer (ViT)                                   │\n│  - Extracts patch-level features                            │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision-Language Connector                                  │\n│  - Uses features from 3rd-to-last & 9th-from-last ViT layer │\n│  - Attention pooling: 2x2 for images, 3x3 for video frames  │\n│  - Shared MLP projection                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  LLM (Qwen3 or OLMo)                                        │\n│  - Visual tokens + text timestamps (video) or image indices │\n│  - Bi-directional attention between vision tokens           │\n│  - Output: text + &lt;points&gt; (for grounding)                  │\n└─────────────────────────────────────────────────────────────┘\n主要な設計選択:\n\nCropping: 画像は最大24クロップ（推論時）、動画は2 fpsサンプリング\nBi-directional attention: 画像トークン同士が相互に attend 可能（性能向上）\nPointing フォーマット: 正規化された (x, y, timestamp/image_index, object_id) をプレーンテキストで出力\n\n\n詳細: Vision-Language Connector",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#トレーニング",
    "href": "ja/molmo2/00-overview.html#トレーニング",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は 3段階 でトレーニングされます。\n\n\n\nデータ: PixMo-Cap（キャプション）、PixMo-Points（ポインティング）、Tulu（NLP）\nミキシング比率: 60% キャプション、30% ポインティング、10% NLP\nステップ数: 32k ステップ、バッチサイズ128（約4エポック）\n学習率: ViT、Connector、LLM で個別に設定\n\n\n\n\n\nデータ: PixMo + Molmo2 データセット + オープンソースビデオ/画像データセット\nカテゴリ別サンプリング: 手動で調整したサンプリングレート（Table 1参照）\nステップ数: 30k ステップ、バッチサイズ128、最大シーケンス長16,384\n\n\n\n\nカテゴリ\nサンプリング率\nデータセット数\n例数\n\n\n\n\nCaptions/Long QA\n13.6%\n6\n1.2M\n\n\nImage QA\n22.7%\n32\n2.4M\n\n\nVideo QA\n18.2%\n32\n2.4M\n\n\nImage Pointing\n9.1%\n4\n1.1M\n\n\nVideo Pointing\n13.6%\n7\n0.37M\n\n\nVideo Tracking\n13.6%\n22\n0.80M\n\n\nNLP\n9.1%\n1\n0.99M\n\n\n\n\n\n\n\nコンテキスト長: 36,864（Stage 2の2.25倍）\nフレーム数: F = 384（Stage 2 の3倍）\nステップ数: 2k ステップ\n並列化: Context Parallelism (CP) を使用、8 GPU で処理\n注意: オーバーヘッドが大きいため短期間のみ実施\n\n\n詳細: Long-Context Training\n\n\n\n\n\n\n\nデータには、単一トークン出力の多肢選択問題から、4000+トークンの長いビデオキャプションまで含まれます。長い出力が損失の大部分を占めてしまうと、短い回答タスクの性能が低下します。\n解決策: タスクごとに重み付けを調整\n\nビデオキャプション: 重み 0.1\nポインティング: 重み 0.2\nその他: \\(\\frac{4}{\\sqrt{n}}\\)（\\(n\\) = 回答トークン数）\n\n\n詳細: Token Weighting\n\n\n\n\n例によってトークン数が大きく異なる（数百～16k+）ため、padding を避けるために packing を使用します。Vision-language モデルでは、ViT 用のクロップと LLM 用のトークンの両方を効率的にパックする必要があります。\nMolmo2 は、オンザフライでパッキングするアルゴリズムを開発し、15倍 のトレーニング効率を達成しています。\n\n詳細: Packing & Message Trees\n\n\n\n\n1つの画像/動画に複数のアノテーションがある場合、メッセージツリー としてエンコードします。視覚入力が最初のメッセージとなり、各アノテーションが異なるブランチになります。ツリーは単一シーケンスに線形化され、カスタムアテンションマスクによってブランチ間のクロスアテンションを防ぎます。\n平均して、データ内の例には4つのアノテーションがあり、packing により16,348トークンのシーケンスに平均3.8例を詰め込むことができています。\n\n詳細: Packing & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#評価",
    "href": "ja/molmo2/00-overview.html#評価",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、標準的なビデオベンチマークと新規のキャプション・カウンティングベンチマークで評価されています。\n主要な結果:\n\n短尺ビデオ理解: オープンウェイトモデル中で SOTA\n\nNextQA: 86.2（Molmo2-8B）\nPerceptionTest: 82.1\nMVBench: 75.9\nMotionBench: 62.2\n\nキャプション: Molmo2-CapTest で F1 Score 43.2（Molmo2-8B）\n\nGPT-5 (50.1)、Gemini 2.5 Pro (42.1) に次ぐ性能\n\nカウンティング: Molmo2-VideoCount で 35.5% accuracy（Molmo2-8B）\n\nQwen3-VL-8B (29.6%) を大きく上回る\n\n長尺ビデオ: 最良のオープンウェイトモデル（Eagle2.5-8B など）には及ばない\n\n原因: オープンソースの長尺（10+分）トレーニングデータ不足\n\n\n\n\n\n\n\n\nNote長尺ビデオでの課題\n\n\n\n\n\nMolmo2 は、以下の長尺ビデオベンチマークで課題を抱えています：\n\nLongVideoBench: 67.5（Eagle2.5-8B: 66.4、PLM-8B: 56.9）\nMLVU: 60.2（Eagle2.5-8B: 60.4、PLM-8B: 52.6）\nLVBench: 52.8（Eagle2.5-8B: 50.9、PLM-8B: 44.5）\n\n原因:\n\nオープンデータ不足: 10分以上の動画に対する高品質なアノテーションが不足\n計算制約: Long-Context Training（Stage 3）は 2,000 ステップのみ実施（オーバーヘッドが大きい）\nトレードオフ: キャプション品質を優先したため、長尺ビデオタスクの性能がやや低下\n\nただし、Molmo2 の長尺ビデオ性能は依然として多くのオープンモデルを上回っており、完全オープンデータのみを使用していることを考慮すれば十分な成果です。\n\n詳細: Long-Context Training\n\n\n\n\nHuman Preference Study:\n\nElo スコア: 1057（Molmo2-8B）\nGemini 3 Pro (1082)、Gemini 2.5 Flash (1084) に次ぐ5位\n完全オープンモデルとしては最高性能\n\n\n\n\nMolmo2 の最大の強みは ビデオグラウンディング です。\nVideo Pointing:\n\nMolmo2-VP ベンチマーク（新規）で F1 Score 38.4\n\nGemini 3 Pro (20.0) を大きく上回る\nプロプライエタリモデルを含めて最高性能\n\n\nVideo Tracking:\n\nBURST（test）で accuracy 56.2\nMolmo2-VC（新規ベンチマーク）で J&F 41.1\n\nGemini 3 Pro を上回る（詳細はベンチマークによる）\n\n\n既存のオープンウェイトモデル（Qwen3-VL など）は、ビデオトラッキング機能を提供していないため、Molmo2 が新たな capability を開拓したと言えます。\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\nMolmo2 は画像タスクでも強力な性能を維持しています。\n\nMMMU: 47.9（Molmo2-8B）\nMathVista: 63.1\nChartQA: 79.5\nAI2D: 84.5\n\nビデオ能力を追加しても、画像タスクの性能を損なっていないことが確認されています。\n\n\n\n論文では、以下の要素の影響を検証しています。\n\nBi-directional attention on vision tokens: 有効（性能向上）\nToken weighting: 有効（長短出力のバランス改善）\nPacking: 15倍の効率向上\nMessage trees: 複数アノテーションの効率的な学習\n\n\n詳細: Token Weighting, Packing & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#関連研究",
    "href": "ja/molmo2/00-overview.html#関連研究",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、以下の研究の流れを統合しています。\n\nVision-Language Models: GPT-4V, Gemini, LLaVA, InternVL\nVideo Understanding: VideoChat, LLaVA-Video, PLM, Eagle\nGrounding: KOSMOS-2, Ferret, GPT-4V with set-of-mark\nVideo Grounding: Grounding-DINO, SAM-2, Ref-VOS\nOpen Data: PixMo, CoSyn, ShareGPT4Video\n\nMolmo2 の独自性は、完全オープン かつ ビデオグラウンディング を実現した点にあります。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\n\n\nプロプライエタリモデル:\n\nGPT-4V / GPT-5: 画像グラウンディングは可能だが、ビデオトラッキングは非サポート\nGemini 2.5 Pro / 3 Pro: 限定的なビデオグラウンディング機能を持つが、Molmo2 の pointing/tracking 性能に及ばない\nClaude Sonnet 4.5: ビデオグラウンディング機能なし\n\nオープンウェイトモデル:\n\nQwen3-VL（4B/8B）: ビデオ理解は強力だが、ビデオグラウンディング機能なし。Molmo2 が counting で上回る\nInternVL3.5（4B/8B）: 画像タスクに特化、ビデオグラウンディングなし\nLLaVA-Video-7B: プロプライエタリモデルから蒸留されたデータを使用。グラウンディング機能なし\nPLM（3B/8B）: 詳細なキャプションデータを持つが、Molmo2 より平均語数が少ない（Molmo2: 924語 vs PLM: 不明）\n\n完全オープンモデル:\n\nMolmo2-O-7B: OLMo LLM を使用した唯一の完全オープンモデル（ViT は依然としてプロプライエタリ）\n\nMolmo2 は、ビデオグラウンディング という新しい capability を開拓し、プロプライエタリモデルを含めて最高性能を達成した点で、他モデルと一線を画しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#結論",
    "href": "ja/molmo2/00-overview.html#結論",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、完全オープンな VLM として、以下を達成しました。\n\n9つの新規データセット を構築（プロプライエタリモデルへの依存ゼロ）\nビデオグラウンディング（pointing & tracking）を実現\n短尺ビデオ理解 でオープンモデル中 SOTA\nキャプション・カウンティング でプロプライエタリモデルに迫る性能\n完全オープン（モデル、データ、コード）\n\n課題として、長尺ビデオ（10+分）でのパフォーマンスは最良のオープンウェイトモデルに及びませんが、これはオープンソースの長尺データ不足が原因です。\nMolmo2 は、オープンソースコミュニティが SOTA の VLM を構築するための強固な基盤を提供します。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Books",
    "section": "",
    "text": "日本語 (Japanese)English\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMolmo2\n\n\n完全オープンな Vision-Language Model で、ビデオグラウンディング（pointing & tracking）を実現\n\n\n\n\n\nFeb 3, 2026\n\n\nNaoto Iwase\n\n\n\n\n\n\n\n\n\n\n\n\nOlmo 3\n\n\n完全オープンな言語・思考型モデル（7B/32B）\n\n\n\n\n\nFeb 2, 2025\n\n\nNaoto Iwase\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNote\n\n\n\nEnglish translations are in progress. Please check the Japanese tab for now."
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html",
    "href": "ja/molmo2/01-dense-video-captioning.html",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の事前学習に使用される超詳細（dense）なビデオキャプションデータセットです。このデータセットは、従来のビデオキャプションデータセットと比較して桁違いに詳細な記述を含み、平均 924 語/動画 という驚異的な記述量を実現しています。\n従来の VLM が生成する短く表面的なキャプションとは異なり、Molmo2-Cap は動的イベントと細かな視覚的詳細の両方を捉えることで、モデルがビデオの時空間的理解を深める基盤を提供します。\nデータセット規模:\n\n104k のビデオレベルキャプション\n431k のクリップレベルキャプション\n平均 924 語/動画 の超詳細記述\n\n\n\n\n\n\nビデオキャプションは画像キャプションよりも本質的に困難です。なぜなら、アノテーターは以下の両方を記述する必要があるからです:\n\n動的イベント: 時間とともに変化する出来事、動作、状態遷移\n細かな視覚的詳細: オブジェクトの外観、空間配置、属性の変化\n\n既存のビデオキャプションデータセットの多くは、表面的な記述に留まっており、ビデオグラウンディング（いつ・どこで何が起きたか）を学習するには不十分でした。Molmo2-Cap は、この gap を埋めるために設計されました。\n\n\n\nより詳細なキャプションは、モデルに以下の能力を与えます:\n\n時空間的な理解: 「いつ」「どこで」「何が」起きたかを正確に把握\n細粒度の視覚認識: 小さな物体、微細な動作、属性の変化を捉える\n文脈理解: イベント間の因果関係や時間的依存性を学習\n\n\n\n\n\n\n\n\n\n\n\nNote他のビデオキャプションデータセットとの比較\n\n\n\n\n\nMolmo2-Cap は既存のビデオキャプションデータセットを大きく上回る記述量を実現しています:\n\n\n\nデータセット\n平均語数/動画\n特徴\n\n\n\n\nMolmo2-Cap\n924 語\n人手による音声記述 + Molmo 統合\n\n\nLLaVA-Video-178K\n547 語\nGPT ベースの合成キャプション\n\n\nShareGPT4-Video\n280 語\nGPT ベースの合成キャプション\n\n\nRDCap\n100 語\n既存データセット\n\n\nRCap\n89 語\n既存データセット\n\n\nVideo Localized Narratives\n75 語\n人手アノテーション\n\n\n\nMolmo2-Cap は、LLaVA-Video の 1.7 倍、ShareGPT4-Video の 3.3 倍、Video Localized Narratives の 12 倍 以上の記述量を持ちます。\n重要な差異:\n\nMolmo2-Cap は プロプライエタリモデル（GPT など）に依存しない 完全にオープンなパイプラインで構築されている\n人手による音声記述 をベースにしており、合成データよりも自然で詳細\nフレームレベルのキャプション統合 により、低レベルの視覚的詳細も漏れなく記述\n\n\n\n\n\n\n\nMolmo2-Cap のデータ収集は、革新的な 2 段階パイプライン を採用しています。\n\n\n\n初期プール構築: 10M 以上のビデオクリップを複数の大規模ソース（YT-Temporal、YouTube など）から収集\n情報量フィルタリング:\n\n音声トラックを除去し、1 fps で均一サンプリング\nH.264 でエンコードし、正規化された情報量スコアを算出: bits / (duration × W × H)\n平均 - 1σ 未満のビデオを除外（視覚的・時間的多様性が低いビデオを排除）\n\n多様性ベースサンプリング:\n\nSAM 2 でフレームをセグメント化し、視覚的複雑さを推定\nMolmo でフレームをキャプション化し、MetaCLIP パイプラインでキーワード抽出\nエントロピー最大化を目指した貪欲サンプリング（キーワード分布とセグメント数分布）\n最終的に約 100k のビデオ を選定（サンプリング率 1%）\n\n\n\n\n\n\n\nビデオを可変長のクリップ（10〜30 秒）に分割します。情報密度が高いクリップほど短い期間に設定 することで、アノテーターの負担を均等化しながら詳細な記述を促します。\n\n平均 4〜5 クリップ/動画 に分割\n\n\n\n\n\n\n\n\n\n\nTipなぜ音声記述を使うのか？\n\n\n\n音声記述（Spoken Captions）は、タイピングによる記述と比較して以下の利点があります:\n\n記述速度が速い: タイピングよりも自然に詳細を語れる\n自然な言語表現: 書き言葉よりも口語の方が流暢で豊かな記述が得られる\n認知負荷の軽減: タイピングに気を取られず、ビデオに集中できる\n\nこの手法は PixMo-Cap（画像キャプションデータセット）でも採用されており、高品質なキャプション生成に有効であることが実証されています。\n\n\nアノテーションプロセス:\n\nクリップ記述:\n\nアノテーターは短いクリップごとに音声で内容を説明（音声はミュート）\n画面上で起きていることを詳細に語る\nリアルタイム文字起こし（Whisper-1）が自動実行\nアノテーターは転写結果を編集し、誤認識を修正\n\nビデオ全体の要約:\n\nすべてのクリップ記述が完了した後、ビデオ全体の包括的な説明を記述\n\n質問ベースのプロンプト:\n\nアノテーターに「動的な視覚的詳細」を記述するよう促すため、事前定義された質問セットを提示\n例: 「オブジェクトや出来事は時間とともにどう変化したか？」\n\n\n\n\n\n\nWhisper の文字起こしは不完全な文や口語表現を含むため、テキスト専用 LLM で以下を実行:\n\n文の構造を整理し、一貫性を確保\n冗長性を削除し、読みやすさを向上\n元の意味を保持しながら流暢な文章に変換\n\n\n\n\n人手記述だけでは見落としがちな 低レベルの視覚的詳細 を補完するため:\n\nMolmo でフレームレベルのキャプションを生成:\n\n個々のフレームを Molmo（早期バージョン）でキャプション化\n色、テクスチャ、細かなオブジェクト属性などを記述\n\nLLM でマージ:\n\nクリップレベルのキャプションとフレームレベルのキャプションを統合\n重複を削除し、一貫性のある長文キャプションを生成\n\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 1: Video Selection                                   │\n│  10M+ clips -&gt; Filter -&gt; Diversity Sampling -&gt; 100k         │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 2: Human Annotation                                  │\n│  Split -&gt; Voice -&gt; Whisper -&gt; Edit                          │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 3: LLM Refinement                                    │\n│  Organize -&gt; Convert to coherent text                       │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 4: Molmo Integration                                 │\n│  Molmo frame captions -&gt; LLM merge -&gt; Final caption         │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\nビデオソース:\n\nYT-Temporal\nYouTube キーワード検索\n複数の大規模ビデオデータセット\n\nライセンス: Creative Commons（一部）\nフィルタリング:\n\n視覚的・時間的多様性が低いビデオを除外\n繰り返しパターンを含む低品質キャプションをヒューリスティックルールで除去\n\n\n\n\nMolmo2-Cap は、Molmo2 の 事前学習（Pre-training） フェーズで使用されます:\n\n長さ条件付きキャプション生成: モデルは指定された長さのキャプションを生成するよう学習\n重み付けサンプリング: ビデオキャプションデータには固定重み 0.1 を設定（他のタスクとバランス）\n\n\n\n\nMolmo2-Cap は以下の点で重要な貢献をしています:\n\nオープンソースの基盤: プロプライエタリモデル（GPT など）に依存しない完全にオープンなパイプライン\nビデオグラウンディングの基礎: 超詳細な記述により、時空間的なポインティングとトラッキングを学習\nデータ品質の新基準: 平均 924 語/動画という記述量は、今後のビデオキャプションデータセットのベンチマークとなる\n再現可能な手法: 音声記述 + LLM 整形 + Molmo 統合という明確なパイプラインは、他のプロジェクトでも再利用可能\n\n\n\n\nMolmo2 のビデオキャプション能力を評価するため、Molmo2-CapTest という評価セットが構築されています:\n\n693 の Creative Commons ライセンスビデオ\nMolmo2-Cap と同様のプロトコルで収集されたが、手動選定された高品質アノテーター が担当\n各ビデオに複数のリファレンスキャプションを用意\nF1 スコア でキャプション品質を評価\n\n\n\n\nMolmo2-Cap は、以下の革新的な設計により、史上最も詳細なビデオキャプションデータセットとなっています:\n\n音声記述 + Whisper: 自然で流暢な記述を効率的に収集\nLLM 整形: 口語表現を読みやすい文章に変換\nMolmo 統合: 低レベルの視覚的詳細を補完\n多様性ベースサンプリング: 視覚的・意味的に多様なビデオセットを構築\n\nこのデータセットは、Molmo2 がビデオグラウンディング（pointing & tracking）を実現する上で不可欠な基盤となっており、完全にオープンなビデオ VLM の可能性を示しています。\n\n関連セクション:\n\nVideo Grounding: Pointing & Tracking - ビデオポインティング・トラッキングデータセット\nMulti-Image Understanding - マルチイメージ理解データセット",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#概要",
    "href": "ja/molmo2/01-dense-video-captioning.html#概要",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の事前学習に使用される超詳細（dense）なビデオキャプションデータセットです。このデータセットは、従来のビデオキャプションデータセットと比較して桁違いに詳細な記述を含み、平均 924 語/動画 という驚異的な記述量を実現しています。\n従来の VLM が生成する短く表面的なキャプションとは異なり、Molmo2-Cap は動的イベントと細かな視覚的詳細の両方を捉えることで、モデルがビデオの時空間的理解を深める基盤を提供します。\nデータセット規模:\n\n104k のビデオレベルキャプション\n431k のクリップレベルキャプション\n平均 924 語/動画 の超詳細記述",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#なぜ-dense-captioning-が重要か",
    "href": "ja/molmo2/01-dense-video-captioning.html#なぜ-dense-captioning-が重要か",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "ビデオキャプションは画像キャプションよりも本質的に困難です。なぜなら、アノテーターは以下の両方を記述する必要があるからです:\n\n動的イベント: 時間とともに変化する出来事、動作、状態遷移\n細かな視覚的詳細: オブジェクトの外観、空間配置、属性の変化\n\n既存のビデオキャプションデータセットの多くは、表面的な記述に留まっており、ビデオグラウンディング（いつ・どこで何が起きたか）を学習するには不十分でした。Molmo2-Cap は、この gap を埋めるために設計されました。\n\n\n\nより詳細なキャプションは、モデルに以下の能力を与えます:\n\n時空間的な理解: 「いつ」「どこで」「何が」起きたかを正確に把握\n細粒度の視覚認識: 小さな物体、微細な動作、属性の変化を捉える\n文脈理解: イベント間の因果関係や時間的依存性を学習",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#既存データセットとの比較",
    "href": "ja/molmo2/01-dense-video-captioning.html#既存データセットとの比較",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Note他のビデオキャプションデータセットとの比較\n\n\n\n\n\nMolmo2-Cap は既存のビデオキャプションデータセットを大きく上回る記述量を実現しています:\n\n\n\nデータセット\n平均語数/動画\n特徴\n\n\n\n\nMolmo2-Cap\n924 語\n人手による音声記述 + Molmo 統合\n\n\nLLaVA-Video-178K\n547 語\nGPT ベースの合成キャプション\n\n\nShareGPT4-Video\n280 語\nGPT ベースの合成キャプション\n\n\nRDCap\n100 語\n既存データセット\n\n\nRCap\n89 語\n既存データセット\n\n\nVideo Localized Narratives\n75 語\n人手アノテーション\n\n\n\nMolmo2-Cap は、LLaVA-Video の 1.7 倍、ShareGPT4-Video の 3.3 倍、Video Localized Narratives の 12 倍 以上の記述量を持ちます。\n重要な差異:\n\nMolmo2-Cap は プロプライエタリモデル（GPT など）に依存しない 完全にオープンなパイプラインで構築されている\n人手による音声記述 をベースにしており、合成データよりも自然で詳細\nフレームレベルのキャプション統合 により、低レベルの視覚的詳細も漏れなく記述",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#データ収集パイプライン",
    "href": "ja/molmo2/01-dense-video-captioning.html#データ収集パイプライン",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap のデータ収集は、革新的な 2 段階パイプライン を採用しています。\n\n\n\n初期プール構築: 10M 以上のビデオクリップを複数の大規模ソース（YT-Temporal、YouTube など）から収集\n情報量フィルタリング:\n\n音声トラックを除去し、1 fps で均一サンプリング\nH.264 でエンコードし、正規化された情報量スコアを算出: bits / (duration × W × H)\n平均 - 1σ 未満のビデオを除外（視覚的・時間的多様性が低いビデオを排除）\n\n多様性ベースサンプリング:\n\nSAM 2 でフレームをセグメント化し、視覚的複雑さを推定\nMolmo でフレームをキャプション化し、MetaCLIP パイプラインでキーワード抽出\nエントロピー最大化を目指した貪欲サンプリング（キーワード分布とセグメント数分布）\n最終的に約 100k のビデオ を選定（サンプリング率 1%）\n\n\n\n\n\n\n\nビデオを可変長のクリップ（10〜30 秒）に分割します。情報密度が高いクリップほど短い期間に設定 することで、アノテーターの負担を均等化しながら詳細な記述を促します。\n\n平均 4〜5 クリップ/動画 に分割\n\n\n\n\n\n\n\n\n\n\nTipなぜ音声記述を使うのか？\n\n\n\n音声記述（Spoken Captions）は、タイピングによる記述と比較して以下の利点があります:\n\n記述速度が速い: タイピングよりも自然に詳細を語れる\n自然な言語表現: 書き言葉よりも口語の方が流暢で豊かな記述が得られる\n認知負荷の軽減: タイピングに気を取られず、ビデオに集中できる\n\nこの手法は PixMo-Cap（画像キャプションデータセット）でも採用されており、高品質なキャプション生成に有効であることが実証されています。\n\n\nアノテーションプロセス:\n\nクリップ記述:\n\nアノテーターは短いクリップごとに音声で内容を説明（音声はミュート）\n画面上で起きていることを詳細に語る\nリアルタイム文字起こし（Whisper-1）が自動実行\nアノテーターは転写結果を編集し、誤認識を修正\n\nビデオ全体の要約:\n\nすべてのクリップ記述が完了した後、ビデオ全体の包括的な説明を記述\n\n質問ベースのプロンプト:\n\nアノテーターに「動的な視覚的詳細」を記述するよう促すため、事前定義された質問セットを提示\n例: 「オブジェクトや出来事は時間とともにどう変化したか？」\n\n\n\n\n\n\nWhisper の文字起こしは不完全な文や口語表現を含むため、テキスト専用 LLM で以下を実行:\n\n文の構造を整理し、一貫性を確保\n冗長性を削除し、読みやすさを向上\n元の意味を保持しながら流暢な文章に変換\n\n\n\n\n人手記述だけでは見落としがちな 低レベルの視覚的詳細 を補完するため:\n\nMolmo でフレームレベルのキャプションを生成:\n\n個々のフレームを Molmo（早期バージョン）でキャプション化\n色、テクスチャ、細かなオブジェクト属性などを記述\n\nLLM でマージ:\n\nクリップレベルのキャプションとフレームレベルのキャプションを統合\n重複を削除し、一貫性のある長文キャプションを生成\n\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 1: Video Selection                                   │\n│  10M+ clips -&gt; Filter -&gt; Diversity Sampling -&gt; 100k         │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 2: Human Annotation                                  │\n│  Split -&gt; Voice -&gt; Whisper -&gt; Edit                          │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 3: LLM Refinement                                    │\n│  Organize -&gt; Convert to coherent text                       │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 4: Molmo Integration                                 │\n│  Molmo frame captions -&gt; LLM merge -&gt; Final caption         │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#データセットの統計",
    "href": "ja/molmo2/01-dense-video-captioning.html#データセットの統計",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "ビデオソース:\n\nYT-Temporal\nYouTube キーワード検索\n複数の大規模ビデオデータセット\n\nライセンス: Creative Commons（一部）\nフィルタリング:\n\n視覚的・時間的多様性が低いビデオを除外\n繰り返しパターンを含む低品質キャプションをヒューリスティックルールで除去",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#学習での使用",
    "href": "ja/molmo2/01-dense-video-captioning.html#学習での使用",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の 事前学習（Pre-training） フェーズで使用されます:\n\n長さ条件付きキャプション生成: モデルは指定された長さのキャプションを生成するよう学習\n重み付けサンプリング: ビデオキャプションデータには固定重み 0.1 を設定（他のタスクとバランス）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#影響と貢献",
    "href": "ja/molmo2/01-dense-video-captioning.html#影響と貢献",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は以下の点で重要な貢献をしています:\n\nオープンソースの基盤: プロプライエタリモデル（GPT など）に依存しない完全にオープンなパイプライン\nビデオグラウンディングの基礎: 超詳細な記述により、時空間的なポインティングとトラッキングを学習\nデータ品質の新基準: 平均 924 語/動画という記述量は、今後のビデオキャプションデータセットのベンチマークとなる\n再現可能な手法: 音声記述 + LLM 整形 + Molmo 統合という明確なパイプラインは、他のプロジェクトでも再利用可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#評価-molmo2-captest",
    "href": "ja/molmo2/01-dense-video-captioning.html#評価-molmo2-captest",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2 のビデオキャプション能力を評価するため、Molmo2-CapTest という評価セットが構築されています:\n\n693 の Creative Commons ライセンスビデオ\nMolmo2-Cap と同様のプロトコルで収集されたが、手動選定された高品質アノテーター が担当\n各ビデオに複数のリファレンスキャプションを用意\nF1 スコア でキャプション品質を評価",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#まとめ",
    "href": "ja/molmo2/01-dense-video-captioning.html#まとめ",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、以下の革新的な設計により、史上最も詳細なビデオキャプションデータセットとなっています:\n\n音声記述 + Whisper: 自然で流暢な記述を効率的に収集\nLLM 整形: 口語表現を読みやすい文章に変換\nMolmo 統合: 低レベルの視覚的詳細を補完\n多様性ベースサンプリング: 視覚的・意味的に多様なビデオセットを構築\n\nこのデータセットは、Molmo2 がビデオグラウンディング（pointing & tracking）を実現する上で不可欠な基盤となっており、完全にオープンなビデオ VLM の可能性を示しています。\n\n関連セクション:\n\nVideo Grounding: Pointing & Tracking - ビデオポインティング・トラッキングデータセット\nMulti-Image Understanding - マルチイメージ理解データセット",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html",
    "href": "ja/molmo2/03-multi-image-understanding.html",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Multi-Image Understanding は、複数の画像を同時に処理し、それらの関連性や違いを理解する能力です。従来の単一画像処理では各画像を独立して扱うのに対し、Multi-Image Understanding では複数の画像間の関係性を捉えることができます。\n\n\n単一画像処理:\n\n1枚の画像に対して質問応答やキャプション生成を実行\n画像間の比較や関連性の理解は不可能\n文書の複数ページやビフォー・アフター比較などには対応困難\n\nMulti-Image Understanding:\n\n2〜5枚の意味的に関連する画像セットを処理\n画像間の共通点・相違点を理解\n複数画像にまたがる質問応答やグラウンディングが可能\n\n\n\n\nMolmo2-MultiImageQA は、意味的に関連する画像セットに対する質問応答データセットです。\nデータセット規模:\n\n45,000 画像セット（96,000 ユニーク画像から構成）\n72,000 QA ペア\n1セットあたり 2〜5 枚の画像（平均 2.73 枚）\n\n収集方法: 人手によるアノテーションで構築されており、以下のプロセスで作成されました。\n\nPixMoCap で訓練されたモデルで各画像のキャプションを生成\nキャプションの文レベルの類似度に基づいて画像をグルーピング\nアノテーターが各セットに対して質問を作成\nClaude Sonnet 4.5 との反復ループで回答を改善\n\nこのアプローチにより、実世界でのマルチイメージクエリをサポートする高品質なデータセットが構築されました。\n\n\n\nMolmo2-MultiImagePoint は、複数画像にわたるポインティングとカウンティングのデータセットです。\nデータセット規模:\n\n470,000 以上のポインティング・カウンティング例\n1セットあたり 2〜5 枚の画像（平均 3.24 枚）\n\n収集方法: 合成的に構築されており、以下のパイプラインで作成されました。\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Step 1: Soft Clustering of Images                          │\n│  - Use images from PixMo-Points                             │\n│  - Combine single-token & sentence-level embedding          │\n│  - Generate semantically related sets (2-5 images)          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 2: Label Normalization                                │\n│  - Lowercase, punctuation/whitespace normalization          │\n│  - Synonym consolidation                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 3: Canonical Label Generation                         │\n│  - Use LLM to merge normalized labels                       │\n│  - Create single canonical description                      │\n│  - Defines shared entity/concept across all images          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 4: Training-time Sampling                             │\n│  - Sample from original annotations (not just canonical)    │\n│  - Preserve lexical diversity & improve robustness          │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\n\n\nNoteCanonical Label の役割\n\n\n\nCanonical Label は、画像セット内の複数の人手アノテーションを統合した標準的な記述です。例えば、「waterfall」「滝」「瀑布」などの異なる表現を「waterfall」という単一の canonical label に統合します。\nただし、トレーニング時には常に canonical label を使用するのではなく、元のアノテーションからも確率的にサンプリングすることで、多様な表現に対応できるモデルを構築しています。\n\n\n\n\n\n\nMolmo2-SynMultiImageQA は、テキストリッチな画像に特化した合成マルチイメージデータセットです。\nデータセット規模:\n\n188,000 例の合成マルチイメージ QA\n\n収集方法: CoSyn [172] を拡張して構築されました。CoSyn は、チャート、表、文書などのテキストリッチな画像に対する質問応答を合成的に生成するフレームワークです。\n対象画像タイプ:\n\nチャート（charts）\n表（tables）\n文書（documents）\n\nこれらのテキストリッチな画像は、文書理解や複数文書間の比較など、実用的なタスクに直結する重要なデータです。\n\n\n\n\n\n\nTip実用例: Multi-Image Understanding の活用\n\n\n\n文書理解:\n\n契約書の複数ページにわたる条項の比較\nレポートの異なるセクション間の整合性チェック\n複数の請求書の内容比較\n\n複数画像の比較:\n\n製品の異なる角度からの写真を比較して特徴を理解\nビフォー・アフター写真の変化検出\n複数のチャートやグラフを横断した傾向分析\n\nグラウンディング:\n\n「すべての画像で滝を指し示して」のような複数画像にわたるポインティング\n「赤い車が何枚の画像に写っているか？」のようなカウンティング\nセット全体での共通オブジェクトの検出\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nデータセット\n規模\n画像セットサイズ\n収集方法\n用途\n\n\n\n\nMolmo2-MultiImageQA\n45k セット72k QA\n2-5枚(平均2.73)\n人手\n一般的な QA\n\n\nMolmo2-MultiImagePoint\n470k 例\n2-5枚(平均3.24)\n合成\nポインティング・カウンティング\n\n\nMolmo2-SynMultiImageQA\n188k 例\n-\n合成(CoSyn拡張)\nテキストリッチ画像の QA\n\n\n\n\n\n\nMulti-Image Understanding は、単一画像処理では不可能だった以下のタスクを実現します。\n情報の統合: 複数の情報源（画像）から情報を統合し、包括的な理解を提供します。\n比較・対照: 画像間の共通点や相違点を明確に識別できます。\n文書処理: 複数ページの文書や、複数の関連文書を横断した理解が可能になります。\n現実世界への適用: 実際のアプリケーションでは、複数の画像を扱うシナリオが頻繁に発生します（例: ECサイトの商品画像、医療画像の時系列比較、監視カメラの複数アングルなど）。\nMolmo2 は、これらの3つのデータセットを活用することで、オープンソースモデルの中で最高水準の Multi-Image Understanding 能力を実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#単一画像処理との違い",
    "href": "ja/molmo2/03-multi-image-understanding.html#単一画像処理との違い",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "単一画像処理:\n\n1枚の画像に対して質問応答やキャプション生成を実行\n画像間の比較や関連性の理解は不可能\n文書の複数ページやビフォー・アフター比較などには対応困難\n\nMulti-Image Understanding:\n\n2〜5枚の意味的に関連する画像セットを処理\n画像間の共通点・相違点を理解\n複数画像にまたがる質問応答やグラウンディングが可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimageqa-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimageqa-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-MultiImageQA は、意味的に関連する画像セットに対する質問応答データセットです。\nデータセット規模:\n\n45,000 画像セット（96,000 ユニーク画像から構成）\n72,000 QA ペア\n1セットあたり 2〜5 枚の画像（平均 2.73 枚）\n\n収集方法: 人手によるアノテーションで構築されており、以下のプロセスで作成されました。\n\nPixMoCap で訓練されたモデルで各画像のキャプションを生成\nキャプションの文レベルの類似度に基づいて画像をグルーピング\nアノテーターが各セットに対して質問を作成\nClaude Sonnet 4.5 との反復ループで回答を改善\n\nこのアプローチにより、実世界でのマルチイメージクエリをサポートする高品質なデータセットが構築されました。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimagepoint-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimagepoint-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-MultiImagePoint は、複数画像にわたるポインティングとカウンティングのデータセットです。\nデータセット規模:\n\n470,000 以上のポインティング・カウンティング例\n1セットあたり 2〜5 枚の画像（平均 3.24 枚）\n\n収集方法: 合成的に構築されており、以下のパイプラインで作成されました。\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Step 1: Soft Clustering of Images                          │\n│  - Use images from PixMo-Points                             │\n│  - Combine single-token & sentence-level embedding          │\n│  - Generate semantically related sets (2-5 images)          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 2: Label Normalization                                │\n│  - Lowercase, punctuation/whitespace normalization          │\n│  - Synonym consolidation                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 3: Canonical Label Generation                         │\n│  - Use LLM to merge normalized labels                       │\n│  - Create single canonical description                      │\n│  - Defines shared entity/concept across all images          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 4: Training-time Sampling                             │\n│  - Sample from original annotations (not just canonical)    │\n│  - Preserve lexical diversity & improve robustness          │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\n\n\nNoteCanonical Label の役割\n\n\n\nCanonical Label は、画像セット内の複数の人手アノテーションを統合した標準的な記述です。例えば、「waterfall」「滝」「瀑布」などの異なる表現を「waterfall」という単一の canonical label に統合します。\nただし、トレーニング時には常に canonical label を使用するのではなく、元のアノテーションからも確率的にサンプリングすることで、多様な表現に対応できるモデルを構築しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-synmultiimageqa-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-synmultiimageqa-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-SynMultiImageQA は、テキストリッチな画像に特化した合成マルチイメージデータセットです。\nデータセット規模:\n\n188,000 例の合成マルチイメージ QA\n\n収集方法: CoSyn [172] を拡張して構築されました。CoSyn は、チャート、表、文書などのテキストリッチな画像に対する質問応答を合成的に生成するフレームワークです。\n対象画像タイプ:\n\nチャート（charts）\n表（tables）\n文書（documents）\n\nこれらのテキストリッチな画像は、文書理解や複数文書間の比較など、実用的なタスクに直結する重要なデータです。\n\n\n\n\n\n\nTip実用例: Multi-Image Understanding の活用\n\n\n\n文書理解:\n\n契約書の複数ページにわたる条項の比較\nレポートの異なるセクション間の整合性チェック\n複数の請求書の内容比較\n\n複数画像の比較:\n\n製品の異なる角度からの写真を比較して特徴を理解\nビフォー・アフター写真の変化検出\n複数のチャートやグラフを横断した傾向分析\n\nグラウンディング:\n\n「すべての画像で滝を指し示して」のような複数画像にわたるポインティング\n「赤い車が何枚の画像に写っているか？」のようなカウンティング\nセット全体での共通オブジェクトの検出",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#データセット統計",
    "href": "ja/molmo2/03-multi-image-understanding.html#データセット統計",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "データセット\n規模\n画像セットサイズ\n収集方法\n用途\n\n\n\n\nMolmo2-MultiImageQA\n45k セット72k QA\n2-5枚(平均2.73)\n人手\n一般的な QA\n\n\nMolmo2-MultiImagePoint\n470k 例\n2-5枚(平均3.24)\n合成\nポインティング・カウンティング\n\n\nMolmo2-SynMultiImageQA\n188k 例\n-\n合成(CoSyn拡張)\nテキストリッチ画像の QA",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#multi-image-understanding-の重要性",
    "href": "ja/molmo2/03-multi-image-understanding.html#multi-image-understanding-の重要性",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Multi-Image Understanding は、単一画像処理では不可能だった以下のタスクを実現します。\n情報の統合: 複数の情報源（画像）から情報を統合し、包括的な理解を提供します。\n比較・対照: 画像間の共通点や相違点を明確に識別できます。\n文書処理: 複数ページの文書や、複数の関連文書を横断した理解が可能になります。\n現実世界への適用: 実際のアプリケーションでは、複数の画像を扱うシナリオが頻繁に発生します（例: ECサイトの商品画像、医療画像の時系列比較、監視カメラの複数アングルなど）。\nMolmo2 は、これらの3つのデータセットを活用することで、オープンソースモデルの中で最高水準の Multi-Image Understanding 能力を実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html",
    "href": "ja/molmo2/05-long-context-training.html",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training は、Molmo2 の Stage 3 として実施される最終的な訓練段階です。Stage 2 の Supervised Fine-Tuning (SFT) と同じデータミックスを使用しながら、コンテキスト長を大幅に拡張 することで、長尺ビデオや大量の画像を扱う能力を向上させます。\nこの段階は、オーバーヘッドが大きいため 短期間のみ実施 されますが、長尺ビデオ理解タスクでの性能向上に重要な役割を果たします。\n\n\n\nLong-Context Training の主な目的は以下の通りです。\n\n長尺ビデオの理解: 10分以上の長い動画を処理する能力の向上\n大量フレームの処理: より多くのフレームを同時に扱うことで、時間的な文脈をより正確に把握\n複雑なマルチモーダル入力: 字幕付き動画や、複数の画像セットなど、トークン数が多い入力への対応\n\n\n\n\nStage 2 (SFT) と Stage 3 (Long-Context Training) の主な違いは以下の表の通りです。\n\n\n\n\n\n\n\n\n\nパラメータ\nStage 2 (SFT)\nStage 3 (Long-Context)\n変化率\n\n\n\n\nシーケンス長\n16,384\n36,864\n+225%\n\n\n最大フレーム数 (F)\n128\n384\n+300%\n\n\n訓練ステップ数\n30,000\n2,000\n-93%\n\n\nバッチサイズ\n128\n128\n変更なし\n\n\nデータミックス\nSFT データミックス\nSFT データミックス（同一）\n変更なし\n\n\n並列化手法\n標準的な Data Parallelism\nContext Parallelism (CP)\n追加\n\n\n\n\n\n\n\n\n\nNoteなぜ短期間のみ実施するか\n\n\n\nLong-Context Training は 2,000 ステップ のみ実施されます（Stage 2 の30,000ステップと比較して6.7%）。これは、Context Parallelism による 計算オーバーヘッドが大きい ためです。\n具体的には:\n\n各例が 8 GPU のグループ で処理される\nVision encoder と attentional pooling の分散処理が必要\n通常の訓練よりも通信コストが高い\n\n短期間の訓練でも、長尺ビデオベンチマークでの性能が 有意に向上 することが確認されています（Table 11）。\n\n\n\n\n\nStage 3 では、最大シーケンス長を 16,384 → 36,864 トークンに拡張します。これにより、以下のような入力が扱えるようになります。\n例:\n\n長尺ビデオ: 384 フレーム（2 fps で約3分12秒）+ 字幕\n複雑な QA: 長いキャプション（平均924語）を含む動画に対する複数ターンの会話\nマルチ画像: 大量の高解像度画像（複数のクロップ含む）\n\nシーケンス長の拡張により、packing アルゴリズムは最大 36,864 トークン と 384 画像クロップ を単一のパックシーケンスに詰め込むことができます（Stage 2 では16,384トークン、128クロップが上限）。\n\n\n\nStage 3 では、動画の最大フレーム数を F = 128 → F = 384 に拡張します。\nサンプリング方法:\n\nサンプリングレート: 2 fps（変更なし）\n最大長: 384 / 2 = 192秒（約3分12秒）\nフレーム選択: 動画長が F/S を超える場合、F フレームを均等にサンプリング\n最終フレーム: 常に含まれる（動画プレイヤーが最終フレームを表示するため）\n\n\n\n\nStage\n最大フレーム数 (F)\n最大動画長（2 fps）\n\n\n\n\nStage 2 (SFT)\n128\n64秒（約1分）\n\n\nStage 3 (Long-Context)\n384\n192秒（約3分）\n\n\n\n\n\n\n\n\n\nTip推論時のフレーム数拡張\n\n\n\n訓練時は F = 384 ですが、推論時には テストタイムスケーリング を使用して、さらに多くのフレームを処理することが可能です。\nアブレーション（Table 13）によると、Molmo2-8B（SFT後、Long-Context訓練前）は、推論時に 224 フレーム で最良の性能を示しました。Long-Context訓練後は、さらに多くのフレームを処理できる可能性があります。\n\n\n\n\n\nLong-Context Training では、LLM に対して Context Parallelism (CP) を使用します。これは、長いシーケンスを複数の GPU に分散して処理する並列化手法です。\n\n\nMolmo2 は、CP の実装として Ulysses attention を採用しています。\n選択理由:\n\nAll-gather 操作の柔軟性: Molmo2 の packing と message tree システムで使用される カスタムアテンションマスク に対応可能\n通信効率: シーケンス次元を分割し、必要な情報のみを all-gather\n\n動作概要:\n\n各 GPU が シーケンスの一部 を処理\nAttention 計算時に all-gather で他 GPU の情報を集約\nカスタムアテンションマスク（packing や message tree 用）を適用\n\n\n\n\n\n\n\nNoteContext Parallelism の設定\n\n\n\n\nCP グループサイズ: 8 GPU\n各例の処理: 8 GPU のグループで1つの例を処理\n適用範囲: LLM のみ（Vision encoder は別途分散）\n\n参考文献: [56] Ulysses attention\n\n\n\n\n\nContext Parallelism は LLM に適用されますが、Vision encoder と attentional pooling も CP グループ全体に分散されます。\n分散方法:\n\nフレーム分割: 384 フレームを 8 GPU に分割（各 GPU が48フレームを処理）\nVision encoder: 各 GPU が担当フレームの Vision Transformer 処理を実行\nAttentional pooling: Vision token を LLM 用にプーリング（3x3 pooling for video）\n統合: プーリングされた視覚トークンを LLM の入力として結合\n\n効果:\n\nメモリフットプリント削減: Vision encoder のアクティベーションを複数 GPU に分散\n計算効率: フレーム処理を並列化\n\n\n\n\n\n\n\nImportantVision Encoder 分散の重要性\n\n\n\n論文では、Vision encoder と attentional pooling の分散処理が 非常に効果的 にモデルのメモリフットプリントを削減したと述べられています。\n384 フレームの処理は膨大なメモリを必要とするため（各フレームが複数のパッチに分割され、ViT で処理される）、この分散処理なしでは訓練が困難でした。\n\n\n\n\n\n\n\n\nLong-Context Training では、Stage 2 (SFT) と 同じハイパーパラメータ を使用します（Table 12）。\n\nOptimizer: AdamW\nLearning rate: Cosine decay（10% まで減衰）\nWarmup: ViT と LLM で長めのウォームアップ\nWeight decay: なし\n\n\n\n\n\n\n\nモデル\nGPU 数\n訓練時間\nGPU 時間\n\n\n\n\nMolmo2-4B\n128\n25.3 時間\n3,200 GPU-hr\n\n\nMolmo2-O-7B\n128\n25.7 時間\n3,300 GPU-hr\n\n\nMolmo2-8B\n128\n26.0 時間\n3,300 GPU-hr\n\n\n\n注: GPU は Nvidia H100 を使用\nStage 2 (SFT) と比較すると、訓練時間は約40%（4B: 7.5k → 3.2k GPU-hr）です。これは、ステップ数が少ない（2k vs 30k）ためです。\n\n\n\n\nLong-Context Training の効果を検証したアブレーション（Table 11）の結果は以下の通りです。\n\n\n\n設定\n短尺ビデオ QA\n長尺ビデオ QA\nMolmo2 Video Cap.\n画像 QA\n\n\n\n\nLong-Context SFT あり\n69.4\n67.4\n39.9\n80.6\n\n\nLong-Context SFT なし\n69.6\n64.4\n42.3\n80.5\n\n\n\n観察:\n\n長尺ビデオ QA: +3.0 ポイント向上（67.4 vs 64.4）\n短尺ビデオ QA: ほぼ変化なし（-0.2 ポイント）\nビデオキャプション: -2.4 ポイント低下（42.3 → 39.9）\n画像 QA: ほぼ変化なし（+0.1 ポイント）\n\n\n\n\n\n\n\nWarningトレードオフ: キャプション性能の低下\n\n\n\nLong-Context Training は長尺ビデオ理解を向上させますが、ビデオキャプション の性能がわずかに低下します。\nこれは、長いコンテキストに適応する過程で、短い出力（キャプション）の生成能力が若干犠牲になる可能性を示唆しています。実用上は、タスクに応じて Long-Context Training の有無を選択することが重要です。\n\n\n\n\n\nMolmo2 は Long-Context Training により長尺ビデオ理解が向上しましたが、最良のオープンウェイトモデル（Eagle2.5-8B など）には及びません。\n主な原因:\n\nオープンデータの不足: 10分以上の長尺ビデオのアノテーションデータが極めて少ない\n計算制限: 超長尺コンテキストの訓練は計算コストが高く、大規模実施が困難\n訓練期間の制約: わずか2,000ステップの訓練では、長尺ビデオに特化した能力の獲得が限定的\n\n\n\n\n\n\n\nNote今後の展望\n\n\n\n論文では、オープンソースの長尺ビデオデータ の不足が主要なボトルネックであると述べられています。\nコミュニティが長尺ビデオのアノテーションデータを構築し、より長期間の Long-Context Training を実施できれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。\n\n\n\n\n\nLong-Context Training（Stage 3）は、Molmo2 の訓練パイプラインの最終段階として、以下を実現します。\n\nコンテキスト長: 16,384 → 36,864（+125%）\nフレーム数: F = 128 → F = 384（+200%）\n並列化: Context Parallelism（Ulysses attention）を使用し、8 GPU で処理\nVision encoder 分散: フレーム処理を CP グループ全体に分散してメモリ削減\n短期間訓練: わずか2,000ステップ（オーバーヘッドのため）\n\nこの短期間の訓練でも、長尺ビデオ QA で +3.0 ポイント の性能向上を達成しており、Long-Context Training の有効性が示されています。ただし、ビデオキャプションの性能がわずかに低下するトレードオフも確認されています。\n今後、オープンソースの長尺ビデオデータが充実し、より長期間の Long-Context Training が可能になれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#概要",
    "href": "ja/molmo2/05-long-context-training.html#概要",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training は、Molmo2 の Stage 3 として実施される最終的な訓練段階です。Stage 2 の Supervised Fine-Tuning (SFT) と同じデータミックスを使用しながら、コンテキスト長を大幅に拡張 することで、長尺ビデオや大量の画像を扱う能力を向上させます。\nこの段階は、オーバーヘッドが大きいため 短期間のみ実施 されますが、長尺ビデオ理解タスクでの性能向上に重要な役割を果たします。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#目的",
    "href": "ja/molmo2/05-long-context-training.html#目的",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training の主な目的は以下の通りです。\n\n長尺ビデオの理解: 10分以上の長い動画を処理する能力の向上\n大量フレームの処理: より多くのフレームを同時に扱うことで、時間的な文脈をより正確に把握\n複雑なマルチモーダル入力: 字幕付き動画や、複数の画像セットなど、トークン数が多い入力への対応",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#stage-2-との比較",
    "href": "ja/molmo2/05-long-context-training.html#stage-2-との比較",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 2 (SFT) と Stage 3 (Long-Context Training) の主な違いは以下の表の通りです。\n\n\n\n\n\n\n\n\n\nパラメータ\nStage 2 (SFT)\nStage 3 (Long-Context)\n変化率\n\n\n\n\nシーケンス長\n16,384\n36,864\n+225%\n\n\n最大フレーム数 (F)\n128\n384\n+300%\n\n\n訓練ステップ数\n30,000\n2,000\n-93%\n\n\nバッチサイズ\n128\n128\n変更なし\n\n\nデータミックス\nSFT データミックス\nSFT データミックス（同一）\n変更なし\n\n\n並列化手法\n標準的な Data Parallelism\nContext Parallelism (CP)\n追加\n\n\n\n\n\n\n\n\n\nNoteなぜ短期間のみ実施するか\n\n\n\nLong-Context Training は 2,000 ステップ のみ実施されます（Stage 2 の30,000ステップと比較して6.7%）。これは、Context Parallelism による 計算オーバーヘッドが大きい ためです。\n具体的には:\n\n各例が 8 GPU のグループ で処理される\nVision encoder と attentional pooling の分散処理が必要\n通常の訓練よりも通信コストが高い\n\n短期間の訓練でも、長尺ビデオベンチマークでの性能が 有意に向上 することが確認されています（Table 11）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#コンテキスト長の拡張",
    "href": "ja/molmo2/05-long-context-training.html#コンテキスト長の拡張",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 3 では、最大シーケンス長を 16,384 → 36,864 トークンに拡張します。これにより、以下のような入力が扱えるようになります。\n例:\n\n長尺ビデオ: 384 フレーム（2 fps で約3分12秒）+ 字幕\n複雑な QA: 長いキャプション（平均924語）を含む動画に対する複数ターンの会話\nマルチ画像: 大量の高解像度画像（複数のクロップ含む）\n\nシーケンス長の拡張により、packing アルゴリズムは最大 36,864 トークン と 384 画像クロップ を単一のパックシーケンスに詰め込むことができます（Stage 2 では16,384トークン、128クロップが上限）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#フレーム数の拡張",
    "href": "ja/molmo2/05-long-context-training.html#フレーム数の拡張",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 3 では、動画の最大フレーム数を F = 128 → F = 384 に拡張します。\nサンプリング方法:\n\nサンプリングレート: 2 fps（変更なし）\n最大長: 384 / 2 = 192秒（約3分12秒）\nフレーム選択: 動画長が F/S を超える場合、F フレームを均等にサンプリング\n最終フレーム: 常に含まれる（動画プレイヤーが最終フレームを表示するため）\n\n\n\n\nStage\n最大フレーム数 (F)\n最大動画長（2 fps）\n\n\n\n\nStage 2 (SFT)\n128\n64秒（約1分）\n\n\nStage 3 (Long-Context)\n384\n192秒（約3分）\n\n\n\n\n\n\n\n\n\nTip推論時のフレーム数拡張\n\n\n\n訓練時は F = 384 ですが、推論時には テストタイムスケーリング を使用して、さらに多くのフレームを処理することが可能です。\nアブレーション（Table 13）によると、Molmo2-8B（SFT後、Long-Context訓練前）は、推論時に 224 フレーム で最良の性能を示しました。Long-Context訓練後は、さらに多くのフレームを処理できる可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#context-parallelism-cp",
    "href": "ja/molmo2/05-long-context-training.html#context-parallelism-cp",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training では、LLM に対して Context Parallelism (CP) を使用します。これは、長いシーケンスを複数の GPU に分散して処理する並列化手法です。\n\n\nMolmo2 は、CP の実装として Ulysses attention を採用しています。\n選択理由:\n\nAll-gather 操作の柔軟性: Molmo2 の packing と message tree システムで使用される カスタムアテンションマスク に対応可能\n通信効率: シーケンス次元を分割し、必要な情報のみを all-gather\n\n動作概要:\n\n各 GPU が シーケンスの一部 を処理\nAttention 計算時に all-gather で他 GPU の情報を集約\nカスタムアテンションマスク（packing や message tree 用）を適用\n\n\n\n\n\n\n\nNoteContext Parallelism の設定\n\n\n\n\nCP グループサイズ: 8 GPU\n各例の処理: 8 GPU のグループで1つの例を処理\n適用範囲: LLM のみ（Vision encoder は別途分散）\n\n参考文献: [56] Ulysses attention\n\n\n\n\n\nContext Parallelism は LLM に適用されますが、Vision encoder と attentional pooling も CP グループ全体に分散されます。\n分散方法:\n\nフレーム分割: 384 フレームを 8 GPU に分割（各 GPU が48フレームを処理）\nVision encoder: 各 GPU が担当フレームの Vision Transformer 処理を実行\nAttentional pooling: Vision token を LLM 用にプーリング（3x3 pooling for video）\n統合: プーリングされた視覚トークンを LLM の入力として結合\n\n効果:\n\nメモリフットプリント削減: Vision encoder のアクティベーションを複数 GPU に分散\n計算効率: フレーム処理を並列化\n\n\n\n\n\n\n\nImportantVision Encoder 分散の重要性\n\n\n\n論文では、Vision encoder と attentional pooling の分散処理が 非常に効果的 にモデルのメモリフットプリントを削減したと述べられています。\n384 フレームの処理は膨大なメモリを必要とするため（各フレームが複数のパッチに分割され、ViT で処理される）、この分散処理なしでは訓練が困難でした。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#訓練の詳細",
    "href": "ja/molmo2/05-long-context-training.html#訓練の詳細",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training では、Stage 2 (SFT) と 同じハイパーパラメータ を使用します（Table 12）。\n\nOptimizer: AdamW\nLearning rate: Cosine decay（10% まで減衰）\nWarmup: ViT と LLM で長めのウォームアップ\nWeight decay: なし\n\n\n\n\n\n\n\nモデル\nGPU 数\n訓練時間\nGPU 時間\n\n\n\n\nMolmo2-4B\n128\n25.3 時間\n3,200 GPU-hr\n\n\nMolmo2-O-7B\n128\n25.7 時間\n3,300 GPU-hr\n\n\nMolmo2-8B\n128\n26.0 時間\n3,300 GPU-hr\n\n\n\n注: GPU は Nvidia H100 を使用\nStage 2 (SFT) と比較すると、訓練時間は約40%（4B: 7.5k → 3.2k GPU-hr）です。これは、ステップ数が少ない（2k vs 30k）ためです。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#アブレーション結果",
    "href": "ja/molmo2/05-long-context-training.html#アブレーション結果",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training の効果を検証したアブレーション（Table 11）の結果は以下の通りです。\n\n\n\n設定\n短尺ビデオ QA\n長尺ビデオ QA\nMolmo2 Video Cap.\n画像 QA\n\n\n\n\nLong-Context SFT あり\n69.4\n67.4\n39.9\n80.6\n\n\nLong-Context SFT なし\n69.6\n64.4\n42.3\n80.5\n\n\n\n観察:\n\n長尺ビデオ QA: +3.0 ポイント向上（67.4 vs 64.4）\n短尺ビデオ QA: ほぼ変化なし（-0.2 ポイント）\nビデオキャプション: -2.4 ポイント低下（42.3 → 39.9）\n画像 QA: ほぼ変化なし（+0.1 ポイント）\n\n\n\n\n\n\n\nWarningトレードオフ: キャプション性能の低下\n\n\n\nLong-Context Training は長尺ビデオ理解を向上させますが、ビデオキャプション の性能がわずかに低下します。\nこれは、長いコンテキストに適応する過程で、短い出力（キャプション）の生成能力が若干犠牲になる可能性を示唆しています。実用上は、タスクに応じて Long-Context Training の有無を選択することが重要です。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#長尺ビデオでの課題",
    "href": "ja/molmo2/05-long-context-training.html#長尺ビデオでの課題",
    "title": "Long-Context Training",
    "section": "",
    "text": "Molmo2 は Long-Context Training により長尺ビデオ理解が向上しましたが、最良のオープンウェイトモデル（Eagle2.5-8B など）には及びません。\n主な原因:\n\nオープンデータの不足: 10分以上の長尺ビデオのアノテーションデータが極めて少ない\n計算制限: 超長尺コンテキストの訓練は計算コストが高く、大規模実施が困難\n訓練期間の制約: わずか2,000ステップの訓練では、長尺ビデオに特化した能力の獲得が限定的\n\n\n\n\n\n\n\nNote今後の展望\n\n\n\n論文では、オープンソースの長尺ビデオデータ の不足が主要なボトルネックであると述べられています。\nコミュニティが長尺ビデオのアノテーションデータを構築し、より長期間の Long-Context Training を実施できれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#まとめ",
    "href": "ja/molmo2/05-long-context-training.html#まとめ",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training（Stage 3）は、Molmo2 の訓練パイプラインの最終段階として、以下を実現します。\n\nコンテキスト長: 16,384 → 36,864（+125%）\nフレーム数: F = 128 → F = 384（+200%）\n並列化: Context Parallelism（Ulysses attention）を使用し、8 GPU で処理\nVision encoder 分散: フレーム処理を CP グループ全体に分散してメモリ削減\n短期間訓練: わずか2,000ステップ（オーバーヘッドのため）\n\nこの短期間の訓練でも、長尺ビデオ QA で +3.0 ポイント の性能向上を達成しており、Long-Context Training の有効性が示されています。ただし、ビデオキャプションの性能がわずかに低下するトレードオフも確認されています。\n今後、オープンソースの長尺ビデオデータが充実し、より長期間の Long-Context Training が可能になれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html",
    "href": "ja/molmo2/07-packing-message-trees.html",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing は、複数の短い訓練例を1つの長いシーケンスにマージして、バッチ作成時の無駄な padding を避ける技術です。訓練例のトークン数は様々で、数百トークン（純粋なテキストや小さい画像）から16,000トークン以上（字幕付き動画や長コンテキスト訓練中の長い動画）まで幅広く分布しています。\n\n\nVLM における Packing は、以下の理由により非自明な課題となります:\n\n二重のパッキング要求: ViT のクロップと LLM のトークンの両方を効率的にパックする必要がある\nモデル多様性: 画像・動画をトークンに変換する異なるアプローチを持つモデルをサポートする必要がある\n\n\n\n\nMolmo2 では、小規模なメモリ内の例のプールから最大効率のパックシーケンスを構築する オンザフライパッキングアルゴリズム を開発しました。このアルゴリズムは標準的な PyTorch データローダーに統合可能です。\n\n\n\n\n\n\nNote効率改善\n\n\n\nSFT 時には、平均 3.8 個の例 を 16,348 トークンのシーケンス に詰め込むことができ、15倍の訓練効率 を達成しています。\n\n\n\n\n\n\nMessage Trees は、複数のアノテーションを持つ動画や画像をエンコードする手法です。1つの視覚入力に対して複数の異なるアノテーション（質問応答ペア、キャプション、ポインティングなど）を効率的に処理できます。\n\n\nMessage Trees では、以下のような木構造でデータを表現します:\nVisual Input (Root)\n├─ Annotation 1 (Branch 1)\n├─ Annotation 2 (Branch 2)\n├─ Annotation 3 (Branch 3)\n└─ Annotation 4 (Branch 4)\n具体的には:\n\n視覚入力が最初のメッセージとしてエンコードされる\n各アノテーションが異なるブランチとなる\n木構造が単一のシーケンスとして線形化される\nカスタムアテンションマスクを使用してブランチ間のクロスアテンションを防止\n\n\n\n\n\n\n\nTipデータ統計\n\n\n\n訓練データ内の例は、平均して 4つのアノテーション を持っています。\n\n\n\n\n\nMessage Trees では、ブランチ間の独立性を保つために カスタムアテンションマスク を使用します。これにより、異なるアノテーション（ブランチ）が互いにアテンションすることを防ぎます。\n\n\n\n\n\n\ngraph TB\n    V[視覚入力&lt;br/&gt;Vision Input]\n\n    V --&gt; A1[アノテーション 1&lt;br/&gt;Branch 1]\n    V --&gt; A2[アノテーション 2&lt;br/&gt;Branch 2]\n    V --&gt; A3[アノテーション 3&lt;br/&gt;Branch 3]\n    V --&gt; A4[アノテーション 4&lt;br/&gt;Branch 4]\n\n    style V fill:#e1f5ff\n    style A1 fill:#fff4e1\n    style A2 fill:#fff4e1\n    style A3 fill:#fff4e1\n    style A4 fill:#fff4e1\n\n    classDef attention stroke:#2196F3,stroke-width:2px\n    class V attention\n\n\n\n\nFigure 1: Message Trees のアテンション構造\n\n\n\n\n\n各ブランチは視覚入力にはアテンションできますが、他のブランチには クロスアテンションできません。詳細なアテンションマスクのパターンは、元論文の Figure 3 を参照してください。\n\n\n\n\nPacking と Message Trees を組み合わせることで、Molmo2 は以下を実現しています:\n\n高密度の訓練データ利用: 1つの視覚入力に対する複数のアノテーションを効率的に活用\n最小限のパディング: 異なる長さの例を効率的に詰め込み、GPU メモリを有効活用\n高速な訓練: 15倍の効率化により、大規模データでの訓練を加速\n\nこの2つの技術は、Molmo2 の効率的な訓練における重要な基盤となっています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#packing-とは",
    "href": "ja/molmo2/07-packing-message-trees.html#packing-とは",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing は、複数の短い訓練例を1つの長いシーケンスにマージして、バッチ作成時の無駄な padding を避ける技術です。訓練例のトークン数は様々で、数百トークン（純粋なテキストや小さい画像）から16,000トークン以上（字幕付き動画や長コンテキスト訓練中の長い動画）まで幅広く分布しています。\n\n\nVLM における Packing は、以下の理由により非自明な課題となります:\n\n二重のパッキング要求: ViT のクロップと LLM のトークンの両方を効率的にパックする必要がある\nモデル多様性: 画像・動画をトークンに変換する異なるアプローチを持つモデルをサポートする必要がある\n\n\n\n\nMolmo2 では、小規模なメモリ内の例のプールから最大効率のパックシーケンスを構築する オンザフライパッキングアルゴリズム を開発しました。このアルゴリズムは標準的な PyTorch データローダーに統合可能です。\n\n\n\n\n\n\nNote効率改善\n\n\n\nSFT 時には、平均 3.8 個の例 を 16,348 トークンのシーケンス に詰め込むことができ、15倍の訓練効率 を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#message-trees-とは",
    "href": "ja/molmo2/07-packing-message-trees.html#message-trees-とは",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Message Trees は、複数のアノテーションを持つ動画や画像をエンコードする手法です。1つの視覚入力に対して複数の異なるアノテーション（質問応答ペア、キャプション、ポインティングなど）を効率的に処理できます。\n\n\nMessage Trees では、以下のような木構造でデータを表現します:\nVisual Input (Root)\n├─ Annotation 1 (Branch 1)\n├─ Annotation 2 (Branch 2)\n├─ Annotation 3 (Branch 3)\n└─ Annotation 4 (Branch 4)\n具体的には:\n\n視覚入力が最初のメッセージとしてエンコードされる\n各アノテーションが異なるブランチとなる\n木構造が単一のシーケンスとして線形化される\nカスタムアテンションマスクを使用してブランチ間のクロスアテンションを防止\n\n\n\n\n\n\n\nTipデータ統計\n\n\n\n訓練データ内の例は、平均して 4つのアノテーション を持っています。\n\n\n\n\n\nMessage Trees では、ブランチ間の独立性を保つために カスタムアテンションマスク を使用します。これにより、異なるアノテーション（ブランチ）が互いにアテンションすることを防ぎます。\n\n\n\n\n\n\ngraph TB\n    V[視覚入力&lt;br/&gt;Vision Input]\n\n    V --&gt; A1[アノテーション 1&lt;br/&gt;Branch 1]\n    V --&gt; A2[アノテーション 2&lt;br/&gt;Branch 2]\n    V --&gt; A3[アノテーション 3&lt;br/&gt;Branch 3]\n    V --&gt; A4[アノテーション 4&lt;br/&gt;Branch 4]\n\n    style V fill:#e1f5ff\n    style A1 fill:#fff4e1\n    style A2 fill:#fff4e1\n    style A3 fill:#fff4e1\n    style A4 fill:#fff4e1\n\n    classDef attention stroke:#2196F3,stroke-width:2px\n    class V attention\n\n\n\n\nFigure 1: Message Trees のアテンション構造\n\n\n\n\n\n各ブランチは視覚入力にはアテンションできますが、他のブランチには クロスアテンションできません。詳細なアテンションマスクのパターンは、元論文の Figure 3 を参照してください。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#packing-と-message-trees-の相乗効果",
    "href": "ja/molmo2/07-packing-message-trees.html#packing-と-message-trees-の相乗効果",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing と Message Trees を組み合わせることで、Molmo2 は以下を実現しています:\n\n高密度の訓練データ利用: 1つの視覚入力に対する複数のアノテーションを効率的に活用\n最小限のパディング: 異なる長さの例を効率的に詰め込み、GPU メモリを有効活用\n高速な訓練: 15倍の効率化により、大規模データでの訓練を加速\n\nこの2つの技術は、Molmo2 の効率的な訓練における重要な基盤となっています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html",
    "href": "ja/olmo-3/00-overview.html",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n\n完全オープン: 学習データ、コード、中間チェックポイントをすべて公開\n多様な能力: 長文脈推論、関数呼び出し、コーディング、指示追従、一般的なチャット、知識リコール\nフラッグシップモデル: Olmo 3.1 Think 32B は、これまでに公開された最強の完全オープン思考型モデル\n\nモデルバリエーション:\n\nOlmo 3 Base: 基盤モデル（7B, 32B）\nOlmo 3 Think: 段階的推論を行う思考型モデル\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練したモデル\n\n論文: arXiv:2512.13961\n\n\n\nOlmo 3 の開発は、Base Model Training（基盤モデル訓練）と Post-training（後訓練）の 2 つの主要ステージに分かれています。\n┌─────────────────────────────────────────────────────────────┐\n│                    Base Model Training                      │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 1: Pretraining (5.9T tokens)                         │\n│    → Dolma 3 Mix (Web, PDFs, Code, etc.)                    │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 2: Midtraining (100B tokens)                         │\n│    → Dolma 3 Dolmino Mix (Math, Code, QA, etc.)             │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 3: Long-context Extension (50-100B tokens)           │\n│    → Dolma 3 Longmino Mix (Long PDFs + Midtrain data)       │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n                     Olmo 3 Base\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│                      Post-training                          │\n├─────────────────────────────────────────────────────────────┤\n│  Path 1: Olmo 3 Think                                       │\n│    SFT → DPO (Delta Learning) → RLVR (OlmoRL)               │\n├─────────────────────────────────────────────────────────────┤\n│  Path 2: Olmo 3 Instruct                                    │\n│    SFT → DPO → RLVR                                         │\n├─────────────────────────────────────────────────────────────┤\n│  Path 3: Olmo 3 RL-Zero                                     │\n│    Base → RLVR (from scratch)                               │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\n\nOlmo 3 Base は、Dolma 3 Mix と呼ばれる 5.9 兆トークンの多様なデータで事前学習されます。\n\n詳細: Dolma 3 データセット\n\n主な革新点:\n\n高速でスケーラブルなグローバル重複排除: 兆トークンスケールでの新しいツール\n\n詳細: Deduplication（重複排除）\n\nolmOCR science PDFs: 学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n\n詳細: olmOCR science PDFs\n\nデータミキシングの新手法: Token-constrained mixing と Quality-aware upsampling\n\n詳細: Data Mixing 手法\n\n\nデータソース:\n\nWeb ページ\n学術 PDF（olmOCR science PDFs）\nコードリポジトリ\n数学データ\nその他の多様なソース\n\n\n\n\n100 億トークンの Dolma 3 Dolmino Mix で中間訓練を実施します。このステージの目的は、コード、数学、一般知識 QA などの重要な能力を強化することです。\n\n詳細: Midtraining（中間訓練）\n\n革新的な手法:\n\n2 部構成のフレームワーク:\n\n個別データソースへの軽量な分散フィードバックループ\n候補ミックスを評価する集中統合テスト\n\nPost-training の下準備: 指示データと思考トレースを意図的に含めることで、後訓練の基盤を構築\n\n評価スイート: OlmoBaseEval\n\n詳細: OlmoBaseEval（評価スイート）\n\n\n\n\nOlmo 3 は、最大 65K トークンのコンテキストをサポートする長文脈能力を持ちます。7B モデルは 50B トークン、32B モデルは 100B トークンの Dolma 3 Longmino Mix で拡張訓練されます。\n\n詳細: Long-context Extension（長文脈拡張）\n\n主要技術:\n\nRoPE 拡張: YaRN を使用した位置エンコーディングの拡張\nDocument packing: Best-fit packing で効率的な長文書の配置\nIntra-document masking: 同一文書内のトークンのみに attention\nModel souping: 複数チェックポイントの平均化\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n\n\n\nOlmo 3 Base は、32B パラメータスケールで 最強の完全オープンモデルです：\n\n完全オープンモデル: Stanford Marin 32B、Apertus 70B を上回る\n数学とコード: 他の完全オープン 32B モデルに対して 2 桁の改善\n長文脈性能: Qwen 2.5 32B、Mistral Small 3.1 24B、Gemma 3 27B に匹敵\n\n\n\n\n\nBase モデルから 3 つのバリエーションを開発します。\n\n\nOlmo 3 Think は、最終回答を生成する前に段階的推論を行い、中間的な思考トレースを生成するように訓練されています。\n訓練パイプライン:\n\nSFT (Supervised Finetuning): Dolci Think SFT で思考トレースを学習\nDPO (Direct Preference Optimization): Delta Learning による選好調整\n\n詳細: Delta Learning\n\nRLVR (Reinforcement Learning with Verifiable Rewards): OlmoRL による強化学習\n\n詳細: OlmoRL / GRPO\n\n\n\n詳細: Dolci データセット\n\n成果:\n\nOlmo 3.1 Think 32B: 最強の完全オープン思考型モデル\nQwen 2.5 32B、Gemma 2/3 27B、DeepSeek R1 32B を上回る\nQwen 3 32B に迫る性能（訓練トークン数は 6 分の 1）\n\n主要ベンチマーク結果 (Olmo 3.1 Think 32B):\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4\n\n\n\n\n\n\nOlmo 3 Instruct は、内部的な思考トレースを生成せずに、効率的で有用な応答を生成するように訓練されています。\n特徴:\n\n簡潔で直接的な応答\n関数呼び出し（Function calling）に最適化\n低レイテンシ（思考トレースなし）\n\n訓練パイプライン:\n\nSFT: Dolci Instruct SFT（関数呼び出しデータを含む）\nDPO: 多ターン選好データと応答長の最適化\nRLVR: 核心能力のさらなる改善\n\n成果:\n\nQwen 2.5、Gemma 3、IBM Granite 3.3、Llama 3 と同等スケールのモデルを上回る\nQwen 3 とのパフォーマンスギャップを縮小\n\n\n\n\nOlmo 3 RL-Zero は、Base モデルから直接 RL で訓練したモデルです。\n目的:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能にする\n完全にオープンな RL ベンチマークを提供\n\nドメイン:\n\nMath（数学）\nCode（コーディング）\nPrecise IF（精密な指示追従）\nGeneral Mix（一般混合）\n\n重要性:\n\n既存のオープンウェイトモデルは事前学習データを公開していないため、RL 研究が制限されていた\nOlmo 3 RL-Zero により、データリークの影響を排除した明確なベンチマークが可能に\n\n\n\n\n\nOlmo 3 Think 32B の訓練には、1024 台の H100 GPU を使用して約 56 日かかりました。\n内訳:\n\nPretraining: 約 47 日（Midtraining と Long-context Extension を含む）\nPost-training: 約 9 日（SFT、DPO、RL）\n\n推定コスト: $2/H100 時間で計算すると約 $2.75M\n\n\n\nOlmo 3 は、すべての中間チェックポイントと最終モデルを公開しています。\n公開内容:\n\nモデル:\n\nすべてのステージの中間チェックポイント\n最終モデル（Base, Think, Instruct, RL-Zero）\n\nデータ:\n\nデータミックス: 実際に訓練に使用したトークン\nソースデータプール: 各ステージの完全なソースデータ\n\nPretraining: 9T トークンのクリーンデータ\nMidtraining: 2T トークンの特化データ\nLong-context: 640B トークンの長文書データ\n\n\nサンプルミックス: より少ない計算リソースでの実験用\n\nPretraining: 150B トークン\nMidtraining: 10B トークン\n\nコード:\n\n訓練コード: OLMo-core（事前学習）、Open Instruct（後訓練）\nデータコード: datamap-rs、duplodocus（重複排除）、dolma3\n評価コード: OLMES、decon（評価データ汚染除去）\n\n\n\n\n\n\n完全オープンなモデルフロー: すべてのステージ、データ、コードを公開\n最強の完全オープンモデル: Base と Think の両方で最高性能\n新しいデータセット: Dolma 3（事前学習）と Dolci（後訓練）\n新しい手法:\n\nOlmoBaseEval（効率的な Base モデル評価）\nOlmoRL（効率的な強化学習フレームワーク）\nDelta Learning（高品質な選好データ作成）\n長文脈拡張の技術（RoPE、Document packing、Intra-document masking）\n\n研究可能性: 思考チェーンを元の訓練データまでトレース可能\n\n\n\n\nOlmo 3 は、完全オープンな AI 研究と開発を推進するための包括的なリリースです。モデルの最終的な重みだけでなく、開発プロセス全体を透明化し、研究者がモデル開発のあらゆる段階で介入・カスタマイズできるようにしています。\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。\nフラッグシップモデル: Olmo 3.1 Think 32B は、推論ベンチマークスイートで Qwen 3 32B に迫りながら、6 分の 1 の訓練トークン数で達成し、すべての訓練データと思考チェーンをトレース可能にしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#概要",
    "href": "ja/olmo-3/00-overview.html#概要",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n\n完全オープン: 学習データ、コード、中間チェックポイントをすべて公開\n多様な能力: 長文脈推論、関数呼び出し、コーディング、指示追従、一般的なチャット、知識リコール\nフラッグシップモデル: Olmo 3.1 Think 32B は、これまでに公開された最強の完全オープン思考型モデル\n\nモデルバリエーション:\n\nOlmo 3 Base: 基盤モデル（7B, 32B）\nOlmo 3 Think: 段階的推論を行う思考型モデル\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練したモデル\n\n論文: arXiv:2512.13961",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#モデルフロー-model-flow",
    "href": "ja/olmo-3/00-overview.html#モデルフロー-model-flow",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 の開発は、Base Model Training（基盤モデル訓練）と Post-training（後訓練）の 2 つの主要ステージに分かれています。\n┌─────────────────────────────────────────────────────────────┐\n│                    Base Model Training                      │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 1: Pretraining (5.9T tokens)                         │\n│    → Dolma 3 Mix (Web, PDFs, Code, etc.)                    │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 2: Midtraining (100B tokens)                         │\n│    → Dolma 3 Dolmino Mix (Math, Code, QA, etc.)             │\n├─────────────────────────────────────────────────────────────┤\n│  Stage 3: Long-context Extension (50-100B tokens)           │\n│    → Dolma 3 Longmino Mix (Long PDFs + Midtrain data)       │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n                     Olmo 3 Base\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│                      Post-training                          │\n├─────────────────────────────────────────────────────────────┤\n│  Path 1: Olmo 3 Think                                       │\n│    SFT → DPO (Delta Learning) → RLVR (OlmoRL)               │\n├─────────────────────────────────────────────────────────────┤\n│  Path 2: Olmo 3 Instruct                                    │\n│    SFT → DPO → RLVR                                         │\n├─────────────────────────────────────────────────────────────┤\n│  Path 3: Olmo 3 RL-Zero                                     │\n│    Base → RLVR (from scratch)                               │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#base-model-training",
    "href": "ja/olmo-3/00-overview.html#base-model-training",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 Base は、Dolma 3 Mix と呼ばれる 5.9 兆トークンの多様なデータで事前学習されます。\n\n詳細: Dolma 3 データセット\n\n主な革新点:\n\n高速でスケーラブルなグローバル重複排除: 兆トークンスケールでの新しいツール\n\n詳細: Deduplication（重複排除）\n\nolmOCR science PDFs: 学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n\n詳細: olmOCR science PDFs\n\nデータミキシングの新手法: Token-constrained mixing と Quality-aware upsampling\n\n詳細: Data Mixing 手法\n\n\nデータソース:\n\nWeb ページ\n学術 PDF（olmOCR science PDFs）\nコードリポジトリ\n数学データ\nその他の多様なソース\n\n\n\n\n100 億トークンの Dolma 3 Dolmino Mix で中間訓練を実施します。このステージの目的は、コード、数学、一般知識 QA などの重要な能力を強化することです。\n\n詳細: Midtraining（中間訓練）\n\n革新的な手法:\n\n2 部構成のフレームワーク:\n\n個別データソースへの軽量な分散フィードバックループ\n候補ミックスを評価する集中統合テスト\n\nPost-training の下準備: 指示データと思考トレースを意図的に含めることで、後訓練の基盤を構築\n\n評価スイート: OlmoBaseEval\n\n詳細: OlmoBaseEval（評価スイート）\n\n\n\n\nOlmo 3 は、最大 65K トークンのコンテキストをサポートする長文脈能力を持ちます。7B モデルは 50B トークン、32B モデルは 100B トークンの Dolma 3 Longmino Mix で拡張訓練されます。\n\n詳細: Long-context Extension（長文脈拡張）\n\n主要技術:\n\nRoPE 拡張: YaRN を使用した位置エンコーディングの拡張\nDocument packing: Best-fit packing で効率的な長文書の配置\nIntra-document masking: 同一文書内のトークンのみに attention\nModel souping: 複数チェックポイントの平均化\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n\n\n\nOlmo 3 Base は、32B パラメータスケールで 最強の完全オープンモデルです：\n\n完全オープンモデル: Stanford Marin 32B、Apertus 70B を上回る\n数学とコード: 他の完全オープン 32B モデルに対して 2 桁の改善\n長文脈性能: Qwen 2.5 32B、Mistral Small 3.1 24B、Gemma 3 27B に匹敵",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#post-training",
    "href": "ja/olmo-3/00-overview.html#post-training",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Base モデルから 3 つのバリエーションを開発します。\n\n\nOlmo 3 Think は、最終回答を生成する前に段階的推論を行い、中間的な思考トレースを生成するように訓練されています。\n訓練パイプライン:\n\nSFT (Supervised Finetuning): Dolci Think SFT で思考トレースを学習\nDPO (Direct Preference Optimization): Delta Learning による選好調整\n\n詳細: Delta Learning\n\nRLVR (Reinforcement Learning with Verifiable Rewards): OlmoRL による強化学習\n\n詳細: OlmoRL / GRPO\n\n\n\n詳細: Dolci データセット\n\n成果:\n\nOlmo 3.1 Think 32B: 最強の完全オープン思考型モデル\nQwen 2.5 32B、Gemma 2/3 27B、DeepSeek R1 32B を上回る\nQwen 3 32B に迫る性能（訓練トークン数は 6 分の 1）\n\n主要ベンチマーク結果 (Olmo 3.1 Think 32B):\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4\n\n\n\n\n\n\nOlmo 3 Instruct は、内部的な思考トレースを生成せずに、効率的で有用な応答を生成するように訓練されています。\n特徴:\n\n簡潔で直接的な応答\n関数呼び出し（Function calling）に最適化\n低レイテンシ（思考トレースなし）\n\n訓練パイプライン:\n\nSFT: Dolci Instruct SFT（関数呼び出しデータを含む）\nDPO: 多ターン選好データと応答長の最適化\nRLVR: 核心能力のさらなる改善\n\n成果:\n\nQwen 2.5、Gemma 3、IBM Granite 3.3、Llama 3 と同等スケールのモデルを上回る\nQwen 3 とのパフォーマンスギャップを縮小\n\n\n\n\nOlmo 3 RL-Zero は、Base モデルから直接 RL で訓練したモデルです。\n目的:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能にする\n完全にオープンな RL ベンチマークを提供\n\nドメイン:\n\nMath（数学）\nCode（コーディング）\nPrecise IF（精密な指示追従）\nGeneral Mix（一般混合）\n\n重要性:\n\n既存のオープンウェイトモデルは事前学習データを公開していないため、RL 研究が制限されていた\nOlmo 3 RL-Zero により、データリークの影響を排除した明確なベンチマークが可能に",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#訓練コストとタイムライン",
    "href": "ja/olmo-3/00-overview.html#訓練コストとタイムライン",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 Think 32B の訓練には、1024 台の H100 GPU を使用して約 56 日かかりました。\n内訳:\n\nPretraining: 約 47 日（Midtraining と Long-context Extension を含む）\nPost-training: 約 9 日（SFT、DPO、RL）\n\n推定コスト: $2/H100 時間で計算すると約 $2.75M",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#オープンアーティファクト",
    "href": "ja/olmo-3/00-overview.html#オープンアーティファクト",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、すべての中間チェックポイントと最終モデルを公開しています。\n公開内容:\n\nモデル:\n\nすべてのステージの中間チェックポイント\n最終モデル（Base, Think, Instruct, RL-Zero）\n\nデータ:\n\nデータミックス: 実際に訓練に使用したトークン\nソースデータプール: 各ステージの完全なソースデータ\n\nPretraining: 9T トークンのクリーンデータ\nMidtraining: 2T トークンの特化データ\nLong-context: 640B トークンの長文書データ\n\n\nサンプルミックス: より少ない計算リソースでの実験用\n\nPretraining: 150B トークン\nMidtraining: 10B トークン\n\nコード:\n\n訓練コード: OLMo-core（事前学習）、Open Instruct（後訓練）\nデータコード: datamap-rs、duplodocus（重複排除）、dolma3\n評価コード: OLMES、decon（評価データ汚染除去）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#主な貢献",
    "href": "ja/olmo-3/00-overview.html#主な貢献",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "完全オープンなモデルフロー: すべてのステージ、データ、コードを公開\n最強の完全オープンモデル: Base と Think の両方で最高性能\n新しいデータセット: Dolma 3（事前学習）と Dolci（後訓練）\n新しい手法:\n\nOlmoBaseEval（効率的な Base モデル評価）\nOlmoRL（効率的な強化学習フレームワーク）\nDelta Learning（高品質な選好データ作成）\n長文脈拡張の技術（RoPE、Document packing、Intra-document masking）\n\n研究可能性: 思考チェーンを元の訓練データまでトレース可能",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#まとめ",
    "href": "ja/olmo-3/00-overview.html#まとめ",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、完全オープンな AI 研究と開発を推進するための包括的なリリースです。モデルの最終的な重みだけでなく、開発プロセス全体を透明化し、研究者がモデル開発のあらゆる段階で介入・カスタマイズできるようにしています。\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。\nフラッグシップモデル: Olmo 3.1 Think 32B は、推論ベンチマークスイートで Qwen 3 32B に迫りながら、6 分の 1 の訓練トークン数で達成し、すべての訓練データと思考チェーンをトレース可能にしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html",
    "href": "ja/olmo-3/02-olmobaseeval.html",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、Base モデル（事前学習済みモデル）の性能を効率的に評価するために設計されたベンチマークスイートである。従来の評価手法が抱える課題を解決し、小規模モデルでも信頼性の高い評価を可能にする。\n\n\n言語モデルの開発プロセスでは、学習途中のモデルを頻繁に評価する必要がある。しかし、既存の評価ベンチマークの多くは完全に学習されたモデルや、指示チューニングされた大規模モデルを対象としており、小規模な Base モデルの評価には適していない。\nOlmoBaseEval は、以下の目的で開発された。\n\n小規模モデルでも意味のある評価結果を提供\n学習途中のモデルの進捗を正確に追跡\n計算コストを抑えつつ高い評価精度を実現\nモデルの基礎能力を多角的に測定\n\n\n\n\n小規模な Base モデルの評価には、主に3つの課題が存在する。\n\n\n\n\n\n\nNote課題1: ランダムチャンス性能\n\n\n\n小規模モデルは多くのタスクでランダム選択と変わらない性能しか示さない。例えば、4択問題でモデルが学習していない場合、正答率は25%前後に留まり、実質的な能力測定ができない。\n\n\n\n\n\n\n\n\nNote課題2: スコア差の小ささ\n\n\n\n学習が進んでもスコアの変化が微小であり、改善の有無を判断しにくい。統計的に有意な差を検出するには、非常に多くのサンプルが必要となる。\n\n\n\n\n\n\n\n\nNote課題3: 評価の不安定性\n\n\n\nタスクによってはノイズが大きく、同じモデルでも評価のたびに結果が変動する。これにより、真の性能向上とノイズによる変動を区別できない。\n\n\n\n\n\nOlmoBaseEval は、上記の課題に対して3つのアプローチで対処する。\n\n\n類似した能力を評価する複数のタスクを集約することで、評価の信頼性を向上させる。\n個別のタスクではノイズが大きくても、同じ能力を測定する複数のタスクの結果を統合することで、より安定した評価指標が得られる。これにより、モデルの特定の能力（例: 推論能力、言語理解能力）を正確に測定できる。\nタスククラスタリングのメリット。\n\n単一タスクのノイズを平均化\n能力の多面的な評価\n評価の再現性向上\n\n\n\n\n小規模モデルに適した評価指標を導入する。\n従来の正答率ベースの評価では、小規模モデルの能力を捉えきれない。プロキシメトリックは、正答/誤答の二値ではなく、モデルの出力分布や確信度などを考慮した連続的な指標を用いる。\n代表的なプロキシメトリック。\n\nMasked Perplexity: 特定のトークンに対するモデルの予測確率を測定\nProbability-based metrics: 正解選択肢への確率割り当てを評価\nCalibration metrics: モデルの確信度と実際の正答率の整合性を測定\n\nこれらの指標は、モデルが完全に学習していない段階でも、学習の進捗を捉えることができる。\n\n\n\n評価タスクのシグナルノイズ比を分析し、信頼性の高いタスクのみを選定する。\nすべてのタスクが等しく有用というわけではない。OlmoBaseEval では、各タスクのシグナルノイズ比を測定し、小規模モデルでも安定した結果を示すタスクを優先的に採用する。\nシグナルノイズ比の分析手法。\n\n複数のモデルサイズでの評価結果の一貫性を確認\n同一モデルの複数回評価での分散を測定\nスケーリング則との整合性を検証\n\n\n\n\n\nOlmoBaseEval は、以下の4つの新規ベンチマークを含む。\n\n\n基本的な言語理解と推論能力を測定する6つのタスク。\n\nReading Comprehension: 短文の理解と情報抽出\nFact Recall: 基本的な知識の記憶\nSimple Logic: 基礎的な論理推論\nPattern Recognition: パターンの認識と予測\nBasic Math: 算数レベルの数値計算\nCommon Sense: 常識的な推論\n\nこれらのタスクは、小規模モデルでもランダムチャンスを超える性能を示すよう設計されている。\n\n\n\n生成タスクを多肢選択形式に変換した5つのタスク。\n従来、生成タスクは Base モデルの評価に適さないとされてきた。Gen2MC は、生成の品質を多肢選択形式で評価することで、この問題を解決する。\n\nSummarization: 要約の適切性を選択肢から判定\nTranslation: 翻訳の正確性を評価\nParaphrasing: 言い換えの妥当性を測定\nQuestion Generation: 質問生成の質を評価\nTitle Generation: タイトル生成の適切性を判定\n\nこの形式により、生成タスクでも確率ベースの評価が可能になる。\n\n\n\n多言語プログラミングベンチマーク（17言語対応）。\nMBPP（Mostly Basic Programming Problems）を17の自然言語に翻訳したベンチマーク。モデルの多言語理解能力とコーディング能力を同時に評価する。\n対応言語の例。\n\nヨーロッパ言語: 英語、スペイン語、フランス語、ドイツ語\nアジア言語: 日本語、中国語、韓国語、ヒンディー語\nその他: アラビア語、ロシア語、ポルトガル語\n\n各言語で同一の問題セットを評価することで、言語間の性能差を分析できる。\n\n\n\nマスクされたトークンの予測精度を測定する評価手法。\n特定の重要なトークン（名詞、動詞など）をマスクし、モデルがそれらをどの程度正確に予測できるかを評価する。この手法は、小規模モデルでも連続的なスコアを提供し、学習の進捗を細かく追跡できる。\nMasked Perplexity の特徴。\n\n連続的なスコアリング（二値判定ではない）\n文脈理解能力の直接的な測定\n計算コストが低い\n学習初期段階から有意な差を検出\n\n\n\n\n\nOlmoBaseEval を用いた評価は、以下の流れで実施される。\n\nベースライン測定: 学習開始前のランダム初期化モデルを評価\n定期評価: 学習ステップごとに自動評価を実行\nクラスタ分析: タスククラスタごとの性能変化を追跡\nスケーリング予測: 小規模モデルの結果から大規模モデルの性能を推定\n\n\n\n\n\n\n\nTip評価の頻度\n\n\n\n小規模モデル（1B パラメータ未満）では、数百ステップごとの評価が推奨される。大規模モデルでは、計算コストを考慮して評価頻度を調整する。\n\n\n\n\n\nOlmoBaseEval は、スケーリング則の分析と組み合わせることで、さらなる洞察を提供する。\n小規模モデルでの評価結果をもとに、より大規模なモデルの性能を予測できる。これにより、大規模モデルを実際に学習する前に、学習戦略の有効性を検証できる。\n予測精度の向上には、以下の要素が重要である。\n\n複数のモデルサイズでの評価データ\nタスククラスタごとのスケーリング曲線\n学習データ量との相関分析\n\n\n\n\nOlmoBaseEval は、従来の評価スイートと比較して、大幅に計算コストを削減する。\n+----------------------------------+\n| Efficiency Comparison            |\n+----------------------------------+\n| Traditional: 100 GPU hours       |\n| OlmoBaseEval: 10 GPU hours       |\n| Reduction: 90%                   |\n+----------------------------------+\n効率化の要因。\n\nタスク数の最適化（重複の排除）\nプロキシメトリックによる評価時間短縮\nバッチ処理の最適化\n\n\n\n\nOlmoBaseEval は、Base モデルの評価における以下の革新をもたらす。\n\n小規模モデルでの信頼性: ランダムチャンス性能を超える評価を実現\n効率性: 計算コストを大幅に削減\n多角的評価: タスククラスタリングによる能力の包括的測定\n予測可能性: スケーリング分析との統合\n\nこの評価スイートにより、モデル開発の初期段階から、データ効率的かつ計算効率的な学習戦略の検証が可能になる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#背景と目的",
    "href": "ja/olmo-3/02-olmobaseeval.html#背景と目的",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "言語モデルの開発プロセスでは、学習途中のモデルを頻繁に評価する必要がある。しかし、既存の評価ベンチマークの多くは完全に学習されたモデルや、指示チューニングされた大規模モデルを対象としており、小規模な Base モデルの評価には適していない。\nOlmoBaseEval は、以下の目的で開発された。\n\n小規模モデルでも意味のある評価結果を提供\n学習途中のモデルの進捗を正確に追跡\n計算コストを抑えつつ高い評価精度を実現\nモデルの基礎能力を多角的に測定",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#従来手法の課題",
    "href": "ja/olmo-3/02-olmobaseeval.html#従来手法の課題",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "小規模な Base モデルの評価には、主に3つの課題が存在する。\n\n\n\n\n\n\nNote課題1: ランダムチャンス性能\n\n\n\n小規模モデルは多くのタスクでランダム選択と変わらない性能しか示さない。例えば、4択問題でモデルが学習していない場合、正答率は25%前後に留まり、実質的な能力測定ができない。\n\n\n\n\n\n\n\n\nNote課題2: スコア差の小ささ\n\n\n\n学習が進んでもスコアの変化が微小であり、改善の有無を判断しにくい。統計的に有意な差を検出するには、非常に多くのサンプルが必要となる。\n\n\n\n\n\n\n\n\nNote課題3: 評価の不安定性\n\n\n\nタスクによってはノイズが大きく、同じモデルでも評価のたびに結果が変動する。これにより、真の性能向上とノイズによる変動を区別できない。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#解決策の3つの柱",
    "href": "ja/olmo-3/02-olmobaseeval.html#解決策の3つの柱",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、上記の課題に対して3つのアプローチで対処する。\n\n\n類似した能力を評価する複数のタスクを集約することで、評価の信頼性を向上させる。\n個別のタスクではノイズが大きくても、同じ能力を測定する複数のタスクの結果を統合することで、より安定した評価指標が得られる。これにより、モデルの特定の能力（例: 推論能力、言語理解能力）を正確に測定できる。\nタスククラスタリングのメリット。\n\n単一タスクのノイズを平均化\n能力の多面的な評価\n評価の再現性向上\n\n\n\n\n小規模モデルに適した評価指標を導入する。\n従来の正答率ベースの評価では、小規模モデルの能力を捉えきれない。プロキシメトリックは、正答/誤答の二値ではなく、モデルの出力分布や確信度などを考慮した連続的な指標を用いる。\n代表的なプロキシメトリック。\n\nMasked Perplexity: 特定のトークンに対するモデルの予測確率を測定\nProbability-based metrics: 正解選択肢への確率割り当てを評価\nCalibration metrics: モデルの確信度と実際の正答率の整合性を測定\n\nこれらの指標は、モデルが完全に学習していない段階でも、学習の進捗を捉えることができる。\n\n\n\n評価タスクのシグナルノイズ比を分析し、信頼性の高いタスクのみを選定する。\nすべてのタスクが等しく有用というわけではない。OlmoBaseEval では、各タスクのシグナルノイズ比を測定し、小規模モデルでも安定した結果を示すタスクを優先的に採用する。\nシグナルノイズ比の分析手法。\n\n複数のモデルサイズでの評価結果の一貫性を確認\n同一モデルの複数回評価での分散を測定\nスケーリング則との整合性を検証",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#新しいベンチマーク",
    "href": "ja/olmo-3/02-olmobaseeval.html#新しいベンチマーク",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、以下の4つの新規ベンチマークを含む。\n\n\n基本的な言語理解と推論能力を測定する6つのタスク。\n\nReading Comprehension: 短文の理解と情報抽出\nFact Recall: 基本的な知識の記憶\nSimple Logic: 基礎的な論理推論\nPattern Recognition: パターンの認識と予測\nBasic Math: 算数レベルの数値計算\nCommon Sense: 常識的な推論\n\nこれらのタスクは、小規模モデルでもランダムチャンスを超える性能を示すよう設計されている。\n\n\n\n生成タスクを多肢選択形式に変換した5つのタスク。\n従来、生成タスクは Base モデルの評価に適さないとされてきた。Gen2MC は、生成の品質を多肢選択形式で評価することで、この問題を解決する。\n\nSummarization: 要約の適切性を選択肢から判定\nTranslation: 翻訳の正確性を評価\nParaphrasing: 言い換えの妥当性を測定\nQuestion Generation: 質問生成の質を評価\nTitle Generation: タイトル生成の適切性を判定\n\nこの形式により、生成タスクでも確率ベースの評価が可能になる。\n\n\n\n多言語プログラミングベンチマーク（17言語対応）。\nMBPP（Mostly Basic Programming Problems）を17の自然言語に翻訳したベンチマーク。モデルの多言語理解能力とコーディング能力を同時に評価する。\n対応言語の例。\n\nヨーロッパ言語: 英語、スペイン語、フランス語、ドイツ語\nアジア言語: 日本語、中国語、韓国語、ヒンディー語\nその他: アラビア語、ロシア語、ポルトガル語\n\n各言語で同一の問題セットを評価することで、言語間の性能差を分析できる。\n\n\n\nマスクされたトークンの予測精度を測定する評価手法。\n特定の重要なトークン（名詞、動詞など）をマスクし、モデルがそれらをどの程度正確に予測できるかを評価する。この手法は、小規模モデルでも連続的なスコアを提供し、学習の進捗を細かく追跡できる。\nMasked Perplexity の特徴。\n\n連続的なスコアリング（二値判定ではない）\n文脈理解能力の直接的な測定\n計算コストが低い\n学習初期段階から有意な差を検出",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#評価の実践",
    "href": "ja/olmo-3/02-olmobaseeval.html#評価の実践",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval を用いた評価は、以下の流れで実施される。\n\nベースライン測定: 学習開始前のランダム初期化モデルを評価\n定期評価: 学習ステップごとに自動評価を実行\nクラスタ分析: タスククラスタごとの性能変化を追跡\nスケーリング予測: 小規模モデルの結果から大規模モデルの性能を推定\n\n\n\n\n\n\n\nTip評価の頻度\n\n\n\n小規模モデル（1B パラメータ未満）では、数百ステップごとの評価が推奨される。大規模モデルでは、計算コストを考慮して評価頻度を調整する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#スケーリング分析との統合",
    "href": "ja/olmo-3/02-olmobaseeval.html#スケーリング分析との統合",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、スケーリング則の分析と組み合わせることで、さらなる洞察を提供する。\n小規模モデルでの評価結果をもとに、より大規模なモデルの性能を予測できる。これにより、大規模モデルを実際に学習する前に、学習戦略の有効性を検証できる。\n予測精度の向上には、以下の要素が重要である。\n\n複数のモデルサイズでの評価データ\nタスククラスタごとのスケーリング曲線\n学習データ量との相関分析",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#計算効率",
    "href": "ja/olmo-3/02-olmobaseeval.html#計算効率",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、従来の評価スイートと比較して、大幅に計算コストを削減する。\n+----------------------------------+\n| Efficiency Comparison            |\n+----------------------------------+\n| Traditional: 100 GPU hours       |\n| OlmoBaseEval: 10 GPU hours       |\n| Reduction: 90%                   |\n+----------------------------------+\n効率化の要因。\n\nタスク数の最適化（重複の排除）\nプロキシメトリックによる評価時間短縮\nバッチ処理の最適化",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#まとめ",
    "href": "ja/olmo-3/02-olmobaseeval.html#まとめ",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、Base モデルの評価における以下の革新をもたらす。\n\n小規模モデルでの信頼性: ランダムチャンス性能を超える評価を実現\n効率性: 計算コストを大幅に削減\n多角的評価: タスククラスタリングによる能力の包括的測定\n予測可能性: スケーリング分析との統合\n\nこの評価スイートにより、モデル開発の初期段階から、データ効率的かつ計算効率的な学習戦略の検証が可能になる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html",
    "href": "ja/olmo-3/04-long-context-extension.html",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "OLMo 3 では、ベースモデル（8K トークンのコンテキスト長）から 65K トークンへと長文脈拡張を実施しました。この拡張により、長文書の理解や複雑なタスクに対応できるモデルとなっています。\n\n\n長文脈拡張では、以下の規模でトレーニングを行いました。\n\n7B モデル: 50B トークンで学習\n32B モデル: 100B トークンで学習\nコンテキスト長: 8K トークン → 65K トークンへ拡張\n\nこの拡張は、特定のデータミックス（Dolma 3 Longmino Mix）と複数の技術的手法を組み合わせて実現されています。\n\n\n\n長文脈拡張に使用されたデータセットは、以下の3つの主要コンポーネントから構成されています（Table 11 参照）。\n\n\nPDF から抽出された長文書データで、様々な長さのバケットに分類されています。\n\n\n\nLength Bucket\nDocuments\nTokens\n\n\n\n\n8K-16K\n1,090,349\n13.1B\n\n\n16K-32K\n508,354\n11.0B\n\n\n32K-64K\n142,983\n6.1B\n\n\n64K-128K\n54,992\n4.5B\n\n\n128K-256K\n20,893\n3.2B\n\n\n256K-512K\n8,130\n2.4B\n\n\n512K-1M\n3,394\n1.7B\n\n\n1M+\n1,172\n1.8B\n\n\n\n\n\n\n長文脈能力を強化するための合成的に生成されたデータです。\n\nCWE (Common Word Extraction): 7.4B トークン\nREX (Rewriting Expressions): 1.5B トークン\n\n\n\n\nMidtraining phase で使用されたデータの 66% を含めることで、一般的な能力の維持を図っています。\n\nMidtraining data mix: 34.9B トークン（66% の割合）\n\n\n\n\n\n長文脈拡張の実現には、Figure 13 に示される 5つの技術コンポーネントが使用されています。\n\n\nRoPE (Rotary Position Embedding) を拡張するために YaRN (Yet another RoPE extensioN) を採用しました。\n\n\n\n\n\n\nNoteYaRN の適用範囲\n\n\n\nYaRN は Full attention layers のみに適用されています。Sliding window attention layers では、元の RoPE 設定を維持しています。\n\n\n\n\n\n効率的なトレーニングのために、複数の文書を1つのシーケンスにパッキングします。\n\n手法: Best-fit packing アルゴリズム\n目的: GPU メモリの効率的な利用とトレーニングスループットの向上\n\n\n\n\nパッキングされた文書間で情報が漏れないように、文書内マスキングを適用します。\n\n\n\n\n\n\nImportantマスキングの重要性\n\n\n\nDocument packing を使用する場合、異なる文書間でアテンションが計算されないようにマスキングすることが重要です。これにより、各文書が独立して処理されます。\n\n\n\n\n\n複数のチェックポイントを平均化することで、モデルの安定性と性能を向上させます。\n\n手法: 異なるトレーニングステップで保存されたチェックポイントの重みを平均化\n効果: より汎用的で安定したモデルの獲得\n\n\n\n\n長い拡張により多くのトークンを割り当てることで、より良い性能を達成します。\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\n\n\n\n\n長文脈能力を効果的に向上させるため、2つの合成データ生成手法を使用しています。\n\n\n文書内で共通して出現する単語を抽出するタスクを生成します。これにより、モデルは長文書全体を参照する能力を獲得します。\n\n\n\nREX では、12 種類の vignettes（小場面）を用いて、様々な長文脈タスクをシミュレートします。\n\n\n\n\n\n\nTipREX の vignettes\n\n\n\nREX は多様なタスク形式を含み、要約、情報抽出、質問応答など、実際のユースケースに近い合成データを生成します。これにより、モデルは様々な長文脈タスクに適応できるようになります。\n\n\n\n\n\n\n長文脈拡張モデルの性能は、RULER（開発スイート）と HELMET（Held-out 評価）で評価されています（Table 12 参照）。\n\n\nRULER は、様々なコンテキスト長での性能を測定する開発用評価スイートです。\n\n\n\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\n\n\n\n\nOLMo 3 7B\n92.7\n91.7\n88.1\n82.5\n70.3\n-\n85.1\n\n\nOLMo 3 32B\n95.8\n94.9\n92.8\n89.4\n82.1\n-\n91.0\n\n\n\n\n\n\nHELMET は、実際のユースケースに近い held-out 評価セットです。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\nOLMo 3 の長文脈モデルは、同規模の他のオープンモデルと比較して競争力のある性能を示しています。特に、32B モデルは多くのベンチマークで高いスコアを達成しています。\n\n\n\n\n\n長文脈拡張の評価から、以下の知見が得られました。\n\nToken budget の効果: より多くのトークンで学習することで、長文脈能力が大幅に向上\n合成データの重要性: CWE と REX の合成データが、実際のタスクでの性能向上に寄与\nModel souping の効果: 複数チェックポイントの平均化により、安定した性能を獲得\n\nこれらの技術を組み合わせることで、OLMo 3 は 65K トークンの長文脈を効果的に扱えるモデルとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#拡張の概要",
    "href": "ja/olmo-3/04-long-context-extension.html#拡張の概要",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張では、以下の規模でトレーニングを行いました。\n\n7B モデル: 50B トークンで学習\n32B モデル: 100B トークンで学習\nコンテキスト長: 8K トークン → 65K トークンへ拡張\n\nこの拡張は、特定のデータミックス（Dolma 3 Longmino Mix）と複数の技術的手法を組み合わせて実現されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#dolma-3-longmino-mix-の構成",
    "href": "ja/olmo-3/04-long-context-extension.html#dolma-3-longmino-mix-の構成",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張に使用されたデータセットは、以下の3つの主要コンポーネントから構成されています（Table 11 参照）。\n\n\nPDF から抽出された長文書データで、様々な長さのバケットに分類されています。\n\n\n\nLength Bucket\nDocuments\nTokens\n\n\n\n\n8K-16K\n1,090,349\n13.1B\n\n\n16K-32K\n508,354\n11.0B\n\n\n32K-64K\n142,983\n6.1B\n\n\n64K-128K\n54,992\n4.5B\n\n\n128K-256K\n20,893\n3.2B\n\n\n256K-512K\n8,130\n2.4B\n\n\n512K-1M\n3,394\n1.7B\n\n\n1M+\n1,172\n1.8B\n\n\n\n\n\n\n長文脈能力を強化するための合成的に生成されたデータです。\n\nCWE (Common Word Extraction): 7.4B トークン\nREX (Rewriting Expressions): 1.5B トークン\n\n\n\n\nMidtraining phase で使用されたデータの 66% を含めることで、一般的な能力の維持を図っています。\n\nMidtraining data mix: 34.9B トークン（66% の割合）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#主要技術コンポーネント",
    "href": "ja/olmo-3/04-long-context-extension.html#主要技術コンポーネント",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張の実現には、Figure 13 に示される 5つの技術コンポーネントが使用されています。\n\n\nRoPE (Rotary Position Embedding) を拡張するために YaRN (Yet another RoPE extensioN) を採用しました。\n\n\n\n\n\n\nNoteYaRN の適用範囲\n\n\n\nYaRN は Full attention layers のみに適用されています。Sliding window attention layers では、元の RoPE 設定を維持しています。\n\n\n\n\n\n効率的なトレーニングのために、複数の文書を1つのシーケンスにパッキングします。\n\n手法: Best-fit packing アルゴリズム\n目的: GPU メモリの効率的な利用とトレーニングスループットの向上\n\n\n\n\nパッキングされた文書間で情報が漏れないように、文書内マスキングを適用します。\n\n\n\n\n\n\nImportantマスキングの重要性\n\n\n\nDocument packing を使用する場合、異なる文書間でアテンションが計算されないようにマスキングすることが重要です。これにより、各文書が独立して処理されます。\n\n\n\n\n\n複数のチェックポイントを平均化することで、モデルの安定性と性能を向上させます。\n\n手法: 異なるトレーニングステップで保存されたチェックポイントの重みを平均化\n効果: より汎用的で安定したモデルの獲得\n\n\n\n\n長い拡張により多くのトークンを割り当てることで、より良い性能を達成します。\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#合成データ生成パイプライン",
    "href": "ja/olmo-3/04-long-context-extension.html#合成データ生成パイプライン",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈能力を効果的に向上させるため、2つの合成データ生成手法を使用しています。\n\n\n文書内で共通して出現する単語を抽出するタスクを生成します。これにより、モデルは長文書全体を参照する能力を獲得します。\n\n\n\nREX では、12 種類の vignettes（小場面）を用いて、様々な長文脈タスクをシミュレートします。\n\n\n\n\n\n\nTipREX の vignettes\n\n\n\nREX は多様なタスク形式を含み、要約、情報抽出、質問応答など、実際のユースケースに近い合成データを生成します。これにより、モデルは様々な長文脈タスクに適応できるようになります。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#評価結果",
    "href": "ja/olmo-3/04-long-context-extension.html#評価結果",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張モデルの性能は、RULER（開発スイート）と HELMET（Held-out 評価）で評価されています（Table 12 参照）。\n\n\nRULER は、様々なコンテキスト長での性能を測定する開発用評価スイートです。\n\n\n\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\n\n\n\n\nOLMo 3 7B\n92.7\n91.7\n88.1\n82.5\n70.3\n-\n85.1\n\n\nOLMo 3 32B\n95.8\n94.9\n92.8\n89.4\n82.1\n-\n91.0\n\n\n\n\n\n\nHELMET は、実際のユースケースに近い held-out 評価セットです。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\nOLMo 3 の長文脈モデルは、同規模の他のオープンモデルと比較して競争力のある性能を示しています。特に、32B モデルは多くのベンチマークで高いスコアを達成しています。\n\n\n\n\n\n長文脈拡張の評価から、以下の知見が得られました。\n\nToken budget の効果: より多くのトークンで学習することで、長文脈能力が大幅に向上\n合成データの重要性: CWE と REX の合成データが、実際のタスクでの性能向上に寄与\nModel souping の効果: 複数チェックポイントの平均化により、安定した性能を獲得\n\nこれらの技術を組み合わせることで、OLMo 3 は 65K トークンの長文脈を効果的に扱えるモデルとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "+------------------------------------------------------------------+\n|                      olmOCR science PDFs                         |\n+------------------------------------------------------------------+\n\n\nolmOCR science PDFs は、学術 PDF 文書から構築された新しいデータソースです。\n\npeS2o の代替: Semantic Scholar の既存データセット peS2o を置き換えるために開発\nAI2Bot クローラー: robots.txt に準拠した独自のウェブクローラーを使用\n規模: 初期収集で 238M（2億3800万）の PDF 文書を取得\n\n\n\n\nPDF からプレーンテキストへの変換には、以下の 2 段階のアプローチを採用しています。\n\n第 1 段階: olmOCR（AI2 の OCR モデル）による PDF → プレーンテキスト変換\n第 2 段階: Poppler の pdftotext をフォールバックとして使用\n言語検出: Lingua による言語識別（英語文書のみを抽出）\n\n\n\n\nolmOCR science PDFs は、複数段階のフィルタリングプロセスを経て構築されています。\nInitial Collection:     238M PDF documents\n         |\n         v\nLanguage Detection &    160M documents\nSpam Filtering\n         |\n         v\nFuzzy Deduplication:    156M documents (2.3% reduction)\n         |\n         v\nPII Filtering:          148M documents (4.9% reduction)\n         |\n         v\nHeuristic Filtering:    108M documents (final)\n各段階での削減率は以下の通りです。\n\n言語検出・スパムフィルタ: 238M → 160M（約 33% 削減）\nFuzzy deduplication: 160M → 156M（2.3% 削減）\nPII フィルタリング: 156M → 148M（4.9% 削減）\nヒューリスティックフィルタ: 148M → 108M（約 27% 削減）\n\n\n\n\n個人情報（PII: Personally Identifiable Information）を含む文書を除外するため、文書タイプごとの判定を実施しました。\n\n使用モデル: Gemma 3 12B および Gemma 3 4B\n判定基準: 公開意図のない文書かどうかを分類\n対象例: 個人の医療記録、学生の成績表、履歴書など\n削減効果: 4.9% の文書を除外\n\nこのフィルタリングにより、プライバシーを尊重したデータセットを構築しています。\n\n\n\nolmOCR science PDFs は、長文脈研究のための最大のオープンコレクションです。\n文書長別の統計は以下の通りです。\n\n8K+ トークン: 22.3M 文書（640B トークン）\n32K+ トークン: 4.5M 文書（380B トークン）\n\nこれらの長文文書は、長文脈モデリングの研究に特に有用です。\n\n\n\n\n\n\nNote長文脈データの重要性\n\n\n\n8K トークン以上の文書が 22.3M、32K トークン以上の文書が 4.5M も含まれており、長文脈を扱う言語モデルのトレーニングに最適なデータセットとなっています。\n\n\n\n\n\n最終的な文書コレクションは、WebOrganizer を用いて 24 の学術トピック に分類されています。\n\n分類手法: WebOrganizer（AI2 のドメイン分類器）\nトピック数: 24 カテゴリ\n用途: データ分析、トピック別のサンプリング、ドメイン適応研究\n\n\n\n\n\n\n\nTippeS2o との違い\n\n\n\nolmOCR science PDFs は、peS2o と比較して以下の利点があります。\n\nより新しいクローリングデータ\nより厳格な PII フィルタリング\n長文文書が豊富\nrobots.txt に準拠した倫理的なクローリング",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#概要",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#概要",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、学術 PDF 文書から構築された新しいデータソースです。\n\npeS2o の代替: Semantic Scholar の既存データセット peS2o を置き換えるために開発\nAI2Bot クローラー: robots.txt に準拠した独自のウェブクローラーを使用\n規模: 初期収集で 238M（2億3800万）の PDF 文書を取得",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#olmocr-テキスト抽出プロセス",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#olmocr-テキスト抽出プロセス",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "PDF からプレーンテキストへの変換には、以下の 2 段階のアプローチを採用しています。\n\n第 1 段階: olmOCR（AI2 の OCR モデル）による PDF → プレーンテキスト変換\n第 2 段階: Poppler の pdftotext をフォールバックとして使用\n言語検出: Lingua による言語識別（英語文書のみを抽出）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#データ処理パイプライン",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#データ処理パイプライン",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、複数段階のフィルタリングプロセスを経て構築されています。\nInitial Collection:     238M PDF documents\n         |\n         v\nLanguage Detection &    160M documents\nSpam Filtering\n         |\n         v\nFuzzy Deduplication:    156M documents (2.3% reduction)\n         |\n         v\nPII Filtering:          148M documents (4.9% reduction)\n         |\n         v\nHeuristic Filtering:    108M documents (final)\n各段階での削減率は以下の通りです。\n\n言語検出・スパムフィルタ: 238M → 160M（約 33% 削減）\nFuzzy deduplication: 160M → 156M（2.3% 削減）\nPII フィルタリング: 156M → 148M（4.9% 削減）\nヒューリスティックフィルタ: 148M → 108M（約 27% 削減）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#pii-フィルタリング",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#pii-フィルタリング",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "個人情報（PII: Personally Identifiable Information）を含む文書を除外するため、文書タイプごとの判定を実施しました。\n\n使用モデル: Gemma 3 12B および Gemma 3 4B\n判定基準: 公開意図のない文書かどうかを分類\n対象例: 個人の医療記録、学生の成績表、履歴書など\n削減効果: 4.9% の文書を除外\n\nこのフィルタリングにより、プライバシーを尊重したデータセットを構築しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#データ規模と特徴",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#データ規模と特徴",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、長文脈研究のための最大のオープンコレクションです。\n文書長別の統計は以下の通りです。\n\n8K+ トークン: 22.3M 文書（640B トークン）\n32K+ トークン: 4.5M 文書（380B トークン）\n\nこれらの長文文書は、長文脈モデリングの研究に特に有用です。\n\n\n\n\n\n\nNote長文脈データの重要性\n\n\n\n8K トークン以上の文書が 22.3M、32K トークン以上の文書が 4.5M も含まれており、長文脈を扱う言語モデルのトレーニングに最適なデータセットとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#weborganizer-による分類",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#weborganizer-による分類",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "最終的な文書コレクションは、WebOrganizer を用いて 24 の学術トピック に分類されています。\n\n分類手法: WebOrganizer（AI2 のドメイン分類器）\nトピック数: 24 カテゴリ\n用途: データ分析、トピック別のサンプリング、ドメイン適応研究\n\n\n\n\n\n\n\nTippeS2o との違い\n\n\n\nolmOCR science PDFs は、peS2o と比較して以下の利点があります。\n\nより新しいクローリングデータ\nより厳格な PII フィルタリング\n長文文書が豊富\nrobots.txt に準拠した倫理的なクローリング",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html",
    "href": "ja/olmo-3/08-dolci-dataset.html",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci (Dolma Instruct) は、Olmo 3 の Post-training（後訓練）に使用される包括的なデータスイートです。Think（思考型）、Instruct（指示追従型）、RL-Zero（Base から直接 RL）という 3 つの異なるモデルバリエーションをサポートするため、複数のサブセットから構成されています。\n\n\nDolci は、Olmo 3 Base モデルを特定のタスクに特化させるための高品質なデータセットです。事前学習で獲得した幅広い知識を、実用的な能力（数学的推論、コーディング、指示追従、チャット）に変換することを目的としています。\nDolci の主な特徴:\n\n完全オープン: すべてのデータソースとキュレーションパイプラインを公開\n高品質: モデル生成データの厳格なフィルタリングと検証\n多様なドメイン: Math, Code, Chat, Instruction Following, Safety をカバー\n段階的訓練: SFT、DPO、RL の各ステージに最適化されたデータ\n\n\n\n\nDolci は、3 つの主要なサブセットから構成されています。\n┌──────────────────────────────────────────────────────────────┐\n│                     Dolci Data Suite                         │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Think                                                 │\n│    └─&gt; Think SFT (1.94M samples)                             │\n│        └─&gt; Think DPO (187K pairs)                            │\n│            └─&gt; Think RL (40K prompts)                        │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Instruct                                              │\n│    └─&gt; Instruct SFT (544K samples)                           │\n│        └─&gt; Instruct DPO (105K pairs)                         │\n│            └─&gt; Instruct RL (128K prompts)                    │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci RL-Zero                                               │\n│    └─&gt; Math (30K prompts)                                    │\n│    └─&gt; Code (30K prompts)                                    │\n│    └─&gt; IF (30K prompts)                                      │\n│    └─&gt; General Mix (30K prompts)                             │\n└──────────────────────────────────────────────────────────────┘\n各サブセットは、異なるモデルバリエーションの訓練パイプラインに対応しています。\nDolci Think: 段階的推論を行う思考型モデル用\nDolci Instruct: 簡潔で直接的な応答を生成するモデル用\nDolci RL-Zero: Base モデルから直接 RL で訓練するモデル用\n\n\n\nDolci Think は、最終回答を生成する前に段階的推論を行う思考型モデル（Olmo 3 Think）を訓練するためのデータセットです。\n\n\n規模: 約 194 万サンプル\n目的: モデルに思考トレースを生成する能力を教える\nデータソースの構成:\n\n\n\nTable 1: Dolci Think SFT のデータソース構成\n\n\n\n\n\nカテゴリ\n主なソース\nサンプル数\n\n\n\n\nMath\nOpenThoughts3+, SYNTHETIC-2-Verified\n約 85 万\n\n\nCode\nOpenThoughts3+, Dolci Think Python Algorithms\n約 55 万\n\n\nChat & IF\nWildChat, Persona IF, OpenAssistant\n約 45 万\n\n\nSafety\nCoCoNot, WildGuardMix, WildJailbreak\n約 9 万\n\n\nその他\nAya, TableGPT\n約 10 万\n\n\n\n\n\n\nデータ生成手法:\nDolci Think SFT のデータは、既存のプロンプトに対して強力なモデルで思考トレースを生成することで作成されています。\n使用モデル:\n\nMath / Code: QwQ-32B（思考型モデル）\nChat / Safety: DeepSeek R1（推論特化モデル）\n\nフィルタリング基準:\n\n不完全な思考トレース（途中で打ち切られたもの）を削除\nドメイン固有エラー（数式の誤り、コード構文エラー）を削除\n過度の繰り返しや冗長性を削除\nOpenAI taxonomy を使用したトピックフィルタリング\n\n\n\n\n規模: 約 18.7 万ペア\n目的: 思考トレースの品質を向上させる\nDelta Learning の原理:\nDPO（Direct Preference Optimization）で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。Dolci Think DPO では、強いモデルと弱いモデルのペアを使用して、明確な品質差を持つ選好データを作成しています。\nデータ生成設定:\n\n\n\nTable 2: Delta Learning のモデルペア\n\n\n\n\n\n役割\nモデル\n\n\n\n\nChosen (選択)\nQwen 3 32B (thinking mode)\n\n\nRejected (棄却)\nQwen 3 0.6B (thinking mode)\n\n\n\n\n\n\n主要な知見:\nSFT では改善しないデータでも、DPO では大幅に改善可能です。\n\nQwen3-32B の出力で SFT すると性能低下\n同じデータを弱いモデルとペアにして DPO すると大幅改善\n\nこれは、「絶対的な品質」よりも「相対的な品質差」が学習に重要であることを示しています。\n\n\n\n規模: 約 4 万プロンプト\n目的: RLVR（Reinforcement Learning with Verifiable Rewards）による性能向上\n特徴:\nDolci Think RL は、思考型モデルが苦手とする挑戦的なプロンプトを集めたデータセットです。\nドメイン:\n\nMath: AIME（アメリカ数学招待試験）レベルの高難度問題\nCode: LiveCodeBench などの実践的プログラミング課題\nReasoning: ZebraLogic などの論理パズル\n\n\n\n\n\nDolci Instruct は、思考トレースを生成せずに、簡潔で直接的な応答を生成するモデル（Olmo 3 Instruct）を訓練するためのデータセットです。\n\n\n規模: 約 54.4 万サンプル\n目的: 効率的で有用な応答を生成する能力を教える\n主要なデータソース:\n\nTulu 3 SFT: 多様な指示追従タスク\nFunction-calling データ: ツール使用と API 呼び出しのサンプル\nFlan: タスクフォーマット学習データ\n\nDolci Think SFT との違い:\n\n\n\nTable 3: Think と Instruct の SFT データの比較\n\n\n\n\n\n項目\nDolci Think SFT\nDolci Instruct SFT\n\n\n\n\n思考トレース\nあり\nなし\n\n\n応答スタイル\n段階的推論\n簡潔・直接的\n\n\nFunction-calling\nなし\nあり\n\n\nサンプル数\n194 万\n54.4 万\n\n\n\n\n\n\n\n\n\n規模: 約 10.5 万ペア\n目的: 簡潔性とユーザビリティの向上\n主要な改善点:\nMulti-turn preferences（複数ターン選好データ）:\n合成会話を生成し、複数ターンにわたる一貫した応答を学習します。\nLength control（応答長制御）:\nChosen と Rejected の長さ差を 100 トークン以下に制限することで、冗長性を抑えています。\n\nモデルが単に「長い応答」を学習するのを防ぐ\nユーザビリティを重視した簡潔な応答を促進\n\n\n\n\n規模: 約 12.8 万プロンプト\n目的: RLVR による核心能力のさらなる向上\nドメイン分布:\n\nInstruction Following: 複雑な指示の正確な追従\nChat: 多様な会話シナリオへの対応\nFunction-calling: ツール使用の精度向上\nKnowledge Recall: 知識の正確な想起\n\n\n\n\n\nDolci RL-Zero は、Base モデルから SFT/DPO を経由せずに、直接 RL で訓練するための特別なデータセットです。\n\n\n研究的価値:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能\n完全にオープンな RL ベンチマークを提供\n\n既存の課題:\n従来のオープンウェイトモデル（Llama 3、Qwen 2.5 など）は事前学習データを公開していないため、RL 研究が制限されていました。\nDolci RL-Zero により、データリークの影響を排除した明確なベンチマークが可能になります。\n\n\n\nDolci RL-Zero は、4 つの異なるドメインで構成されています。\nMath（数学）:\n\n規模: 3 万プロンプト\nタスク: GSM8K、MATH などの数学的推論問題\n検証方法: SymPy による数式比較\n\nCode（コーディング）:\n\n規模: 3 万プロンプト\nタスク: HumanEvalPlus、LiveCodeBench などのプログラミング課題\n検証方法: テストケースの実行と検証\n\nIF（Instruction Following）:\n\n規模: 3 万プロンプト\nタスク: IFEval、IFBench などの精密な指示追従\n検証方法: 制約チェック関数による検証\n\nGeneral Mix（一般混合）:\n\n規模: 3 万プロンプト\nタスク: 上記 3 つのドメインと Chat の混合\n検証方法: ドメインに応じた検証方法\n\n\n\n\nDolci RL-Zero の全データは、評価ベンチマークとの重複を排除するため、厳格な Decontamination 処理が施されています。\n手法: decon パッケージによる 2 フェーズ処理\n\n検出フェーズ: 8-gram マッチングで重複を検出（閾値 50%）\nクラスタ拡張フェーズ: 類似サンプルのクラスタ全体を除去\n\nこれにより、RL 訓練データとベンチマークデータの完全な分離を保証しています。\n\n\n\n\nDolci のデータキュレーションは、以下のパイプラインで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│              Dolci Data Curation Pipeline                    │\n├──────────────────────────────────────────────────────────────┤\n│  Step 1: Source Selection                                    │\n│    └─&gt; Public datasets (OpenThoughts, WildChat, etc.)        │\n│        └─&gt; Model generation (QwQ-32B, DeepSeek R1)           │\n├──────────────────────────────────────────────────────────────┤\n│  Step 2: Heuristic Filtering                                 │\n│    └─&gt; Remove incomplete traces                              │\n│        └─&gt; Remove domain-specific errors                     │\n│            └─&gt; Remove excessive repetition                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 3: Topic Filtering                                     │\n│    └─&gt; OpenAI taxonomy classification                        │\n│        └─&gt; Remove off-topic samples                          │\n├──────────────────────────────────────────────────────────────┤\n│  Step 4: Difficulty Filtering                                │\n│    └─&gt; Select challenging prompts for RL                     │\n│        └─&gt; Balance difficulty distribution                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 5: Data Mixing                                         │\n│    └─&gt; Balance domain distribution                           │\n│        └─&gt; Optimize mix for target tasks                     │\n├──────────────────────────────────────────────────────────────┤\n│  Step 6: Decontamination                                     │\n│    └─&gt; 8-gram matching (50% threshold)                       │\n│        └─&gt; Cluster expansion                                 │\n│            └─&gt; Final Dolci datasets                          │\n└──────────────────────────────────────────────────────────────┘\n\n\nStep 1: Source Selection（ソース選択）:\n公開データセットと強力なモデルによる生成データを収集します。\nStep 2: Heuristic Filtering（ヒューリスティックフィルタリング）:\n明らかな低品質データを除去します。\n\n不完全な思考トレース\nドメイン固有エラー（数式の誤り、コード構文エラー）\n過度の繰り返し\n\nStep 3: Topic Filtering（トピックフィルタリング）:\nOpenAI taxonomy を使用して、トピック外のサンプルを除去します。\nStep 4: Difficulty Filtering（難易度フィルタリング）:\nRL 用に挑戦的なプロンプトを選択し、難易度分布をバランスします。\nStep 5: Data Mixing（データミキシング）:\nドメイン分布をバランスし、ターゲットタスクに最適化されたミックスを作成します。\nStep 6: Decontamination（評価データ汚染除去）:\n評価ベンチマークとの重複を完全に排除します。\n\n\n\n\nDolci データスイートは、以下の特徴を持ちます。\n\n\nすべてのデータソース、キュレーションパイプライン、処理コードを公開しています。\n公開内容:\n\n元のデータソースへの参照\nキュレーションスクリプト\nフィルタリング基準\nデータミキシング比率\n\n\n\n\n強力なモデルによる生成と厳格なフィルタリングにより、高品質を実現しています。\n品質保証の仕組み:\n\nモデル生成: QwQ-32B、DeepSeek R1 などの最先端モデルを使用\n複数段階フィルタリング: ヒューリスティック、トピック、難易度\nDecontamination: 評価データとの完全な分離\n\n\n\n\nMath、Code、Chat、Instruction Following、Safety など、幅広いドメインをカバーしています。\nドメイン分布:\n\n\n\nTable 4: Dolci のドメインカバレッジ\n\n\n\n\n\nドメイン\nThink SFT\nInstruct SFT\nRL-Zero\n\n\n\n\nMath\n85 万\n含む\n3 万\n\n\nCode\n55 万\n含む\n3 万\n\n\nChat\n45 万\n大部分\n含む\n\n\nIF\n45 万\n大部分\n3 万\n\n\nSafety\n9 万\n含む\n-\n\n\n\n\n\n\n\n\n\nSFT、DPO、RL の各ステージに最適化されたデータを提供しています。\n訓練パイプライン:\nBase Model\n    ↓\n  SFT (Dolci Think/Instruct SFT)\n    ↓\n  DPO (Dolci Think/Instruct DPO)\n    ↓\n  RL (Dolci Think/Instruct RL)\n    ↓\nFinal Model\n各ステージでデータの種類が異なります。\n\nSFT: 高品質な入力-出力ペア\nDPO: 品質差のある選好ペア\nRL: 検証可能な報酬を持つプロンプト\n\n\n\n\n\n\n\nNoteDelta Learning の詳細\n\n\n\n\n\nDelta Learning は、Dolci DPO データセットの作成に使用される重要な手法です。\n核心的洞察:\nDPO で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。絶対的な品質よりも、相対的な品質差が学習に重要です。\n実験結果:\n\n\n\n設定\nMATH スコア\n変化\n\n\n\n\nBase モデル\n45.2\n-\n\n\nQwen3-32B で SFT\n43.8\n-1.4\n\n\nQwen3-32B (chosen) vs Qwen3-0.6B (rejected) で DPO\n52.3\n+7.1\n\n\n\n同じ Qwen3-32B の出力でも、SFT では性能が低下するのに対し、弱いモデルとペアにして DPO すると大幅に改善します。\n応用:\nこの知見は、Dolci Think DPO と Dolci Instruct DPO の両方で活用されています。\n\n\n\n\n\n\n\nDolci は、Olmo 3 の Post-training に使用される包括的なデータスイートです。Think、Instruct、RL-Zero という 3 つの異なるモデルバリエーションをサポートし、SFT、DPO、RL の各訓練ステージに最適化されたデータを提供しています。\n主な貢献:\n\n完全オープン: すべてのデータソースとパイプラインを公開\n高品質: 強力なモデルによる生成と厳格なフィルタリング\n多様性: Math, Code, Chat, IF, Safety をカバー\n段階的訓練: SFT、DPO、RL に最適化\n研究価値: RL-Zero により完全オープンな RL ベンチマークを提供\n\nDolci は、完全にオープンな Post-training データセットとして、研究者が再現性の高い研究を行い、任意のステージから介入・カスタマイズできるようにしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#概要と目的",
    "href": "ja/olmo-3/08-dolci-dataset.html#概要と目的",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、Olmo 3 Base モデルを特定のタスクに特化させるための高品質なデータセットです。事前学習で獲得した幅広い知識を、実用的な能力（数学的推論、コーディング、指示追従、チャット）に変換することを目的としています。\nDolci の主な特徴:\n\n完全オープン: すべてのデータソースとキュレーションパイプラインを公開\n高品質: モデル生成データの厳格なフィルタリングと検証\n多様なドメイン: Math, Code, Chat, Instruction Following, Safety をカバー\n段階的訓練: SFT、DPO、RL の各ステージに最適化されたデータ",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-subsets",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-subsets",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、3 つの主要なサブセットから構成されています。\n┌──────────────────────────────────────────────────────────────┐\n│                     Dolci Data Suite                         │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Think                                                 │\n│    └─&gt; Think SFT (1.94M samples)                             │\n│        └─&gt; Think DPO (187K pairs)                            │\n│            └─&gt; Think RL (40K prompts)                        │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Instruct                                              │\n│    └─&gt; Instruct SFT (544K samples)                           │\n│        └─&gt; Instruct DPO (105K pairs)                         │\n│            └─&gt; Instruct RL (128K prompts)                    │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci RL-Zero                                               │\n│    └─&gt; Math (30K prompts)                                    │\n│    └─&gt; Code (30K prompts)                                    │\n│    └─&gt; IF (30K prompts)                                      │\n│    └─&gt; General Mix (30K prompts)                             │\n└──────────────────────────────────────────────────────────────┘\n各サブセットは、異なるモデルバリエーションの訓練パイプラインに対応しています。\nDolci Think: 段階的推論を行う思考型モデル用\nDolci Instruct: 簡潔で直接的な応答を生成するモデル用\nDolci RL-Zero: Base モデルから直接 RL で訓練するモデル用",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-think",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-think",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci Think は、最終回答を生成する前に段階的推論を行う思考型モデル（Olmo 3 Think）を訓練するためのデータセットです。\n\n\n規模: 約 194 万サンプル\n目的: モデルに思考トレースを生成する能力を教える\nデータソースの構成:\n\n\n\nTable 1: Dolci Think SFT のデータソース構成\n\n\n\n\n\nカテゴリ\n主なソース\nサンプル数\n\n\n\n\nMath\nOpenThoughts3+, SYNTHETIC-2-Verified\n約 85 万\n\n\nCode\nOpenThoughts3+, Dolci Think Python Algorithms\n約 55 万\n\n\nChat & IF\nWildChat, Persona IF, OpenAssistant\n約 45 万\n\n\nSafety\nCoCoNot, WildGuardMix, WildJailbreak\n約 9 万\n\n\nその他\nAya, TableGPT\n約 10 万\n\n\n\n\n\n\nデータ生成手法:\nDolci Think SFT のデータは、既存のプロンプトに対して強力なモデルで思考トレースを生成することで作成されています。\n使用モデル:\n\nMath / Code: QwQ-32B（思考型モデル）\nChat / Safety: DeepSeek R1（推論特化モデル）\n\nフィルタリング基準:\n\n不完全な思考トレース（途中で打ち切られたもの）を削除\nドメイン固有エラー（数式の誤り、コード構文エラー）を削除\n過度の繰り返しや冗長性を削除\nOpenAI taxonomy を使用したトピックフィルタリング\n\n\n\n\n規模: 約 18.7 万ペア\n目的: 思考トレースの品質を向上させる\nDelta Learning の原理:\nDPO（Direct Preference Optimization）で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。Dolci Think DPO では、強いモデルと弱いモデルのペアを使用して、明確な品質差を持つ選好データを作成しています。\nデータ生成設定:\n\n\n\nTable 2: Delta Learning のモデルペア\n\n\n\n\n\n役割\nモデル\n\n\n\n\nChosen (選択)\nQwen 3 32B (thinking mode)\n\n\nRejected (棄却)\nQwen 3 0.6B (thinking mode)\n\n\n\n\n\n\n主要な知見:\nSFT では改善しないデータでも、DPO では大幅に改善可能です。\n\nQwen3-32B の出力で SFT すると性能低下\n同じデータを弱いモデルとペアにして DPO すると大幅改善\n\nこれは、「絶対的な品質」よりも「相対的な品質差」が学習に重要であることを示しています。\n\n\n\n規模: 約 4 万プロンプト\n目的: RLVR（Reinforcement Learning with Verifiable Rewards）による性能向上\n特徴:\nDolci Think RL は、思考型モデルが苦手とする挑戦的なプロンプトを集めたデータセットです。\nドメイン:\n\nMath: AIME（アメリカ数学招待試験）レベルの高難度問題\nCode: LiveCodeBench などの実践的プログラミング課題\nReasoning: ZebraLogic などの論理パズル",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-instruct",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-instruct",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci Instruct は、思考トレースを生成せずに、簡潔で直接的な応答を生成するモデル（Olmo 3 Instruct）を訓練するためのデータセットです。\n\n\n規模: 約 54.4 万サンプル\n目的: 効率的で有用な応答を生成する能力を教える\n主要なデータソース:\n\nTulu 3 SFT: 多様な指示追従タスク\nFunction-calling データ: ツール使用と API 呼び出しのサンプル\nFlan: タスクフォーマット学習データ\n\nDolci Think SFT との違い:\n\n\n\nTable 3: Think と Instruct の SFT データの比較\n\n\n\n\n\n項目\nDolci Think SFT\nDolci Instruct SFT\n\n\n\n\n思考トレース\nあり\nなし\n\n\n応答スタイル\n段階的推論\n簡潔・直接的\n\n\nFunction-calling\nなし\nあり\n\n\nサンプル数\n194 万\n54.4 万\n\n\n\n\n\n\n\n\n\n規模: 約 10.5 万ペア\n目的: 簡潔性とユーザビリティの向上\n主要な改善点:\nMulti-turn preferences（複数ターン選好データ）:\n合成会話を生成し、複数ターンにわたる一貫した応答を学習します。\nLength control（応答長制御）:\nChosen と Rejected の長さ差を 100 トークン以下に制限することで、冗長性を抑えています。\n\nモデルが単に「長い応答」を学習するのを防ぐ\nユーザビリティを重視した簡潔な応答を促進\n\n\n\n\n規模: 約 12.8 万プロンプト\n目的: RLVR による核心能力のさらなる向上\nドメイン分布:\n\nInstruction Following: 複雑な指示の正確な追従\nChat: 多様な会話シナリオへの対応\nFunction-calling: ツール使用の精度向上\nKnowledge Recall: 知識の正確な想起",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-rl-zero",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-rl-zero",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci RL-Zero は、Base モデルから SFT/DPO を経由せずに、直接 RL で訓練するための特別なデータセットです。\n\n\n研究的価値:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能\n完全にオープンな RL ベンチマークを提供\n\n既存の課題:\n従来のオープンウェイトモデル（Llama 3、Qwen 2.5 など）は事前学習データを公開していないため、RL 研究が制限されていました。\nDolci RL-Zero により、データリークの影響を排除した明確なベンチマークが可能になります。\n\n\n\nDolci RL-Zero は、4 つの異なるドメインで構成されています。\nMath（数学）:\n\n規模: 3 万プロンプト\nタスク: GSM8K、MATH などの数学的推論問題\n検証方法: SymPy による数式比較\n\nCode（コーディング）:\n\n規模: 3 万プロンプト\nタスク: HumanEvalPlus、LiveCodeBench などのプログラミング課題\n検証方法: テストケースの実行と検証\n\nIF（Instruction Following）:\n\n規模: 3 万プロンプト\nタスク: IFEval、IFBench などの精密な指示追従\n検証方法: 制約チェック関数による検証\n\nGeneral Mix（一般混合）:\n\n規模: 3 万プロンプト\nタスク: 上記 3 つのドメインと Chat の混合\n検証方法: ドメインに応じた検証方法\n\n\n\n\nDolci RL-Zero の全データは、評価ベンチマークとの重複を排除するため、厳格な Decontamination 処理が施されています。\n手法: decon パッケージによる 2 フェーズ処理\n\n検出フェーズ: 8-gram マッチングで重複を検出（閾値 50%）\nクラスタ拡張フェーズ: 類似サンプルのクラスタ全体を除去\n\nこれにより、RL 訓練データとベンチマークデータの完全な分離を保証しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-pipeline",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-pipeline",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci のデータキュレーションは、以下のパイプラインで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│              Dolci Data Curation Pipeline                    │\n├──────────────────────────────────────────────────────────────┤\n│  Step 1: Source Selection                                    │\n│    └─&gt; Public datasets (OpenThoughts, WildChat, etc.)        │\n│        └─&gt; Model generation (QwQ-32B, DeepSeek R1)           │\n├──────────────────────────────────────────────────────────────┤\n│  Step 2: Heuristic Filtering                                 │\n│    └─&gt; Remove incomplete traces                              │\n│        └─&gt; Remove domain-specific errors                     │\n│            └─&gt; Remove excessive repetition                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 3: Topic Filtering                                     │\n│    └─&gt; OpenAI taxonomy classification                        │\n│        └─&gt; Remove off-topic samples                          │\n├──────────────────────────────────────────────────────────────┤\n│  Step 4: Difficulty Filtering                                │\n│    └─&gt; Select challenging prompts for RL                     │\n│        └─&gt; Balance difficulty distribution                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 5: Data Mixing                                         │\n│    └─&gt; Balance domain distribution                           │\n│        └─&gt; Optimize mix for target tasks                     │\n├──────────────────────────────────────────────────────────────┤\n│  Step 6: Decontamination                                     │\n│    └─&gt; 8-gram matching (50% threshold)                       │\n│        └─&gt; Cluster expansion                                 │\n│            └─&gt; Final Dolci datasets                          │\n└──────────────────────────────────────────────────────────────┘\n\n\nStep 1: Source Selection（ソース選択）:\n公開データセットと強力なモデルによる生成データを収集します。\nStep 2: Heuristic Filtering（ヒューリスティックフィルタリング）:\n明らかな低品質データを除去します。\n\n不完全な思考トレース\nドメイン固有エラー（数式の誤り、コード構文エラー）\n過度の繰り返し\n\nStep 3: Topic Filtering（トピックフィルタリング）:\nOpenAI taxonomy を使用して、トピック外のサンプルを除去します。\nStep 4: Difficulty Filtering（難易度フィルタリング）:\nRL 用に挑戦的なプロンプトを選択し、難易度分布をバランスします。\nStep 5: Data Mixing（データミキシング）:\nドメイン分布をバランスし、ターゲットタスクに最適化されたミックスを作成します。\nStep 6: Decontamination（評価データ汚染除去）:\n評価ベンチマークとの重複を完全に排除します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-features",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-features",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci データスイートは、以下の特徴を持ちます。\n\n\nすべてのデータソース、キュレーションパイプライン、処理コードを公開しています。\n公開内容:\n\n元のデータソースへの参照\nキュレーションスクリプト\nフィルタリング基準\nデータミキシング比率\n\n\n\n\n強力なモデルによる生成と厳格なフィルタリングにより、高品質を実現しています。\n品質保証の仕組み:\n\nモデル生成: QwQ-32B、DeepSeek R1 などの最先端モデルを使用\n複数段階フィルタリング: ヒューリスティック、トピック、難易度\nDecontamination: 評価データとの完全な分離\n\n\n\n\nMath、Code、Chat、Instruction Following、Safety など、幅広いドメインをカバーしています。\nドメイン分布:\n\n\n\nTable 4: Dolci のドメインカバレッジ\n\n\n\n\n\nドメイン\nThink SFT\nInstruct SFT\nRL-Zero\n\n\n\n\nMath\n85 万\n含む\n3 万\n\n\nCode\n55 万\n含む\n3 万\n\n\nChat\n45 万\n大部分\n含む\n\n\nIF\n45 万\n大部分\n3 万\n\n\nSafety\n9 万\n含む\n-\n\n\n\n\n\n\n\n\n\nSFT、DPO、RL の各ステージに最適化されたデータを提供しています。\n訓練パイプライン:\nBase Model\n    ↓\n  SFT (Dolci Think/Instruct SFT)\n    ↓\n  DPO (Dolci Think/Instruct DPO)\n    ↓\n  RL (Dolci Think/Instruct RL)\n    ↓\nFinal Model\n各ステージでデータの種類が異なります。\n\nSFT: 高品質な入力-出力ペア\nDPO: 品質差のある選好ペア\nRL: 検証可能な報酬を持つプロンプト\n\n\n\n\n\n\n\nNoteDelta Learning の詳細\n\n\n\n\n\nDelta Learning は、Dolci DPO データセットの作成に使用される重要な手法です。\n核心的洞察:\nDPO で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。絶対的な品質よりも、相対的な品質差が学習に重要です。\n実験結果:\n\n\n\n設定\nMATH スコア\n変化\n\n\n\n\nBase モデル\n45.2\n-\n\n\nQwen3-32B で SFT\n43.8\n-1.4\n\n\nQwen3-32B (chosen) vs Qwen3-0.6B (rejected) で DPO\n52.3\n+7.1\n\n\n\n同じ Qwen3-32B の出力でも、SFT では性能が低下するのに対し、弱いモデルとペアにして DPO すると大幅に改善します。\n応用:\nこの知見は、Dolci Think DPO と Dolci Instruct DPO の両方で活用されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#まとめ",
    "href": "ja/olmo-3/08-dolci-dataset.html#まとめ",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、Olmo 3 の Post-training に使用される包括的なデータスイートです。Think、Instruct、RL-Zero という 3 つの異なるモデルバリエーションをサポートし、SFT、DPO、RL の各訓練ステージに最適化されたデータを提供しています。\n主な貢献:\n\n完全オープン: すべてのデータソースとパイプラインを公開\n高品質: 強力なモデルによる生成と厳格なフィルタリング\n多様性: Math, Code, Chat, IF, Safety をカバー\n段階的訓練: SFT、DPO、RL に最適化\n研究価値: RL-Zero により完全オープンな RL ベンチマークを提供\n\nDolci は、完全にオープンな Post-training データセットとして、研究者が再現性の高い研究を行い、任意のステージから介入・カスタマイズできるようにしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html",
    "href": "ja/olmo-3/10-olmorl-grpo.html",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、思考型モデル（reasoning model）の強化学習訓練を効率化するために開発されたフレームワークです。Group Relative Policy Optimization（GRPO）をベースとし、Reinforcement Learning with Verifiable Rewards（RLVR）の手法を採用しています。\nPost-training の第3段階として、数学、コーディング、指示追従、一般的なチャットの複数ドメインにわたり、検証可能な報酬と LM-judge 報酬を組み合わせた強化学習を実施します。\n\n\nOlmoRL は、長い推論トレースを伴う強化学習の課題に対処するため、アルゴリズムとエンジニアリングインフラを密接に統合したシステムです。従来の数学とコードに限定されていた RLVR を、より幅広い検証可能なタスクへと拡張しています。\n主な特徴:\n\nアルゴリズムの改善: GRPO をベースに、DAPO や Dr GRPO などの最新の改善を統合\n大規模データセット: Dolci-Think-RL（約 100K プロンプト、4 ドメイン）\n効率的なインフラ: 長いシーケンス（最大 32K トークン）を効率的に処理する分散訓練システム\n4倍の高速化: OLMo 2 の RL インフラと比較して約 4 倍の高速化を達成\n\n\n\n\nOlmoRL の強化学習段階は、GRPO（Shao et al., 2024）をベースに構築され、DAPO（Yu et al., 2025）や Dr GRPO（Liu et al., 2025b）などの最新のアルゴリズム改善を統合しています。\n\n\nOlmoRL は、Vanilla GRPO に対して以下の改善を実施しています：\n1. Zero gradient signal filtering:\n\n報酬がすべて同一のグループ（advantage の標準偏差がゼロ）を除外\nゼロ勾配のサンプルでの訓練を回避（DAPO と同様）\n\n2. Active sampling:\n\nZero gradient filtering にもかかわらず、一貫したバッチサイズを維持\n動的サンプリングの改良版を実装（詳細は §4.4.3）\n\n3. Token-level loss:\n\nサンプルごとではなく、バッチ全体のトークン数で損失を正規化\n長さバイアスを回避\n\n4. No KL loss:\n\nKL 損失を削除（GLM-4.5、DAPO、Dr GRPO などと同様の実践）\nより制限の少ないポリシー更新を可能にし、過最適化や訓練の不安定化を引き起こさない\n\n5. Clip higher:\n\n上限クリッピング項を下限よりわずかに高く設定\nトークンに対するより大きな更新を可能にする（Yu et al., 2025）\n\n6. Truncated importance sampling:\n\n推論エンジンと訓練エンジン間の対数確率の差を調整\n切り詰められた重要度サンプリング比を損失に掛ける（Yao et al., 2025）\n\n7. No standard deviation normalization:\n\nAdvantage 計算時に標準偏差で正規化しない（Liu et al., 2025b）\n難易度バイアスを除去（報酬の標準偏差が低い問題の advantage が過度に増加するのを防止）\n\n\n\n\n最終的な目的関数は、token-level loss、truncated importance sampling、clip-higher、および advantage 計算における標準偏差正規化なしを含みます：\n\\[\nJ(\\theta) = \\frac{1}{\\sum_{i=1}^{G} |y_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|y_i|} \\min\\left(\\frac{\\pi(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}{\\pi_{\\text{vllm}}(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}, \\rho\\right) \\times \\min(r_{i,t} A_{i,t}, \\text{clip}(r_{i,t}, 1 - \\varepsilon_{\\text{low}}, 1 + \\varepsilon_{\\text{high}}) A_{i,t})\n\\]\nここで：\n\n\\(r_{i,t} = \\frac{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta)}{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta_{\\text{old}})}\\)\n\\(\\varepsilon_{\\text{low}}\\) と \\(\\varepsilon_{\\text{high}}\\) はクリッピングハイパーパラメータ\n\\(\\rho\\) は truncated importance sampling の上限値\nAdvantage \\(A_{i,t}\\) は、グループ \\(G\\) 内での相対報酬に基づいて計算：\n\n\\[\nA_{i,t} = r(x, y_i) - \\text{mean}(\\{r(x, y_i)\\}_{i=1}^{G})\n\\]\n\n\n\n\n\n\nNoteGRPO と PPO の比較\n\n\n\n\n\nGRPO は PPO の変種であり、主な違いは報酬の正規化方法にあります。\nPPO:\n\n報酬を全データセットまたはバッチ全体で正規化\nグローバルなベースラインを使用\n\nGRPO:\n\n同じプロンプトから生成された応答グループ内で報酬を正規化\nグループベースの相対的な品質評価により、学習の安定性が向上\n思考型モデルのように出力長が大きく異なる場合に特に有効\n\nOlmoRL の改善:\n\nVanilla GRPO に対して 7 つの主要な改善を実施\nZero gradient filtering と active sampling により、訓練の効率と安定性を大幅に向上\n\n\n\n\n\n\n\nOlmoRL は、OLMo 2 の数学ドメインを超えて、一般的なドメインへと検証可能な報酬を拡張しています。各ドメインに対して異なるカスタム検証器を使用します：\nMath（数学）:\n\nルールベースの検証器\n基本的な正規化を実行し、SymPy を使用して参照回答と比較\n参照回答と同じであれば 1、そうでなければ 0 を返す\n\nCode（コーディング）:\n\nテストケースベースの検証器\n応答に対して一連のテストケースを実行\n\n通過したテストケースの割合を報酬とする、または (b) すべてのテストケースを通過した場合に 1、そうでなければ 0 を返す\n\n\nInstruction-following（指示追従）:\n\nプロンプトからの一連の制約への準拠をチェックする関数セットを通して応答を渡す\nすべての制約が満たされている場合に 1、そうでなければ 0 を返す\n\nChat—reference（参照付きチャット）:\n\nground-truth 応答がある場合、LM judge を使用してモデルの応答を参照回答と比較\n応答の品質に基づいて [0, 1] のスコアを付与\n\nChat—open-ended（オープンエンドチャット）:\n\n参照回答なしで、LM judge を使用して応答の品質に基づいて [0, 1] のスコアを付与\n\n\n\n\n\nDolci-Think-RL は、4 つのドメイン（数学、コーディング、指示追従、一般的なチャット）にわたる約 100K のサンプルからなる大規模で多様なデータセットです。多様な推論タスクでの堅牢な RL を支援しながら、一般的な有用性を維持します。\n\n\n\n\n\n\n\n\n\n\n\nカテゴリ\nデータセット\nThink RL 用プロンプト数\nInstruct RL 用プロンプト数\n\n\n\n\nPrecise IF\nIF-RLVR\n30,186\n38,000\n\n\nMath\nOpen-Reasoner-Zero\n3,000\n14,000\n\n\n\nDAPO-Math\n2,584\n7,000\n\n\n\nAceReason-Math\n6,602\n-\n\n\n\nPolaris-Dataset\n-\n14,000\n\n\n\nKlearReasoner-MathSub\n3,000\n9,000\n\n\n\nOMEGA-train\n15,000\n20,000\n\n\nCoding\nAceCoder\n9,767\n20,000\n\n\n\nKlearReasoner-Code\n8,040\n-\n\n\n\nNemotron Post-training Code\n2,303\n-\n\n\n\nSYNTHETIC-2\n3,000\n-\n\n\nGeneral Chat\nTulu 3 SFT\n7,129\n18,955\n\n\n\nWildchat-4.8M\n7,129\n18,761\n\n\n\nMulti-Subject RLVR\n7,129\n12,234\n\n\n合計\n\n104,869\n171,950\n\n\n\n\n\n\nStep 1: プロンプトの調達:\n各ドメインから高品質なプロンプトを収集し、キュレーションします。\n\nMath: Open-Reasoner-Zero、DAPO-Math、AceReason-Math、KlearReasoner-MathSub、OMEGA など\nCoding: AceCoder、Klear-Reasoner Code、Nemotron Post-training Code、SYNTHETIC-2 など\nInstruction-following: IF-RLVR（最大 5 つの制約、IFEval と IFBench-Train からサンプリング）\nGeneral chat: Tülu 3 SFT、WildChat-4.8M、Multi-subject-RLVR\n\nStep 2: オフライン難易度フィルタリング:\n\nモデルの初期チェックポイントから各プロンプトに対して 8 つのロールアウトを生成\nモデルが容易に解決するサンプル（pass rate &gt; 62.5%）を除外\n温度 1.0、top-p 1.0 でサンプリング（RL 訓練時と一致）\n\nStep 3: データミキシング:\n\nドメイン固有の実験を実施し、最初の 500-1000 ステップで下流評価のトレンドを観察\n高品質なデータセットをアップウェイト\n各ドメインでほぼ同等のデータ量を使用（数学と指示追従にわずかに重点）\nOMEGA から特定のサブタスクをダウンサンプル\n\n\n\n\n\nOlmoRL は、長いシーケンスとより高速な全体的スループットを処理するために、強化学習インフラストラクチャに大幅な改善を加えました。\n\n\nRL での主要な技術的課題は、推論（ロールアウト）の管理です。最終モデルでは、最大 32K トークンの長さのロールアウトを生成し、平均 10K トークン以上になります。\nリソース配分（32B モデルの場合）:\n\n訓練: 8 H100 ノード\n推論: 20 ノード\nGPU 利用率: 推論が訓練の約 5 倍の計算量を使用\n\n自己回帰推論のコストが高いため、学習器は時間の 75% をデータ待ちに費やします。\n\n\n\n+------------------------------------------------------------------+\n|               OlmoRL Infrastructure Components                   |\n+------------------------------------------------------------------+\n|                                                                  |\n|  1. Fully Asynchronous Training                                 |\n|     - Centralized learner across multiple nodes (DeepSpeed)     |\n|     - Large pool of actors (independent vLLM instances)         |\n|     - Prompts queue & Results queue                             |\n|                                                                  |\n|  2. Continuous Batching                                         |\n|     - Remove compute waste for long generations                 |\n|     - Constantly enqueue new generations as each one finishes   |\n|     - Up to 54% compute savings vs static batching              |\n|                                                                  |\n|  3. Active Sampling                                             |\n|     - Continuously pull completions and resample prompts        |\n|     - Filter until desired batch size is reached                |\n|     - More efficient than dynamic oversampling (3x reduction)   |\n|                                                                  |\n|  4. Inflight Updates                                            |\n|     - Update weights without pausing generation engine          |\n|     - Thread-safe, no KV cache invalidation                     |\n|     - Up to 4x throughput increase                              |\n|                                                                  |\n+------------------------------------------------------------------+\nFully Asynchronous Training:\n\n中央集権的な学習器を複数ノードに分散（DeepSpeed 使用）\n大規模なアクタープール、それぞれ独立した vLLM インスタンスを実行\n学習器がプロンプトをキューに入れ、アクターに配信\nアクターが環境と相互作用し、結果をキューを通じて返す\n\nContinuous Batching:\n\n各生成が終了するたびに新しい生成を絶えずエンキュー\n静的バッチングと比較して、長い生成での計算の無駄を削減\nOlmo 3 では、32K 生成長で平均 14,628 トークン、最大 32K トークン\n静的バッチングでは最大 54% の計算が無駄になっていた\n\nActive Sampling:\n\nフィルタリングされたインスタンスを補償するため、継続的にアクターから補完を引き出し、プロンプトをキューに再サンプリング\nゼロ勾配でない補完の所望のバッチサイズに到達するまで、アクティブにサンプリングとフィルタリングを実施\nDAPO の動的サンプリング（3 倍のオーバーサンプリング）よりも効率的\n\nInflight Updates:\n\n各訓練ステップ後、生成エンジンを一時停止せずに重みを即座に更新\nスレッドセーフな生成フレームワークに依存し、KV キャッシュを無効化せずに生成を継続\n同じリソースで最大 4 倍の高速化を達成\n\n\n\n\n\n\n\n設定\n総トークン数 (Mtok)\nトークン/秒\nMFU (%)\nMBU (%)\n\n\n\n\nOLMo 2\n6.34\n881\n0.30\n12.90\n\n\n+ continuous batching\n7.02\n975\n0.33\n14.29\n\n\n+ better threading\n9.77\n1358\n0.46\n19.89\n\n\n+ inflight updates (Olmo 3)\n21.23\n2949\n1.01\n43.21\n\n\n\nInflight updates の追加が最も劇的な改善をもたらしています。\n\n\n\n\nOlmo 3.1 Think 32B は、拡張された OlmoRL 訓練を通じて、性能向上を示しました。Dolci Think RL データセットでの追加エポックにより、以下の改善が観察されました：\n性能向上:\n\nAIME 2024: +4 ポイント\nIFBench: +20 ポイント\n他のベンチマーク: 性能を維持\n\n拡張訓練により、より長い RL 訓練が汎化性能を向上させ、破局的忘却なしに安定した訓練が可能であることが確認されました。\n\n\n\n\n\nDelta Learning を用いた選好調整を先に実施してから RLVR を適用すると、SFT 単独よりも優れた全体的な性能を達成します。\n\n\n\n最終的な RL ミックスを DPO モデルで実行すると、SFT モデルで実行する場合よりも一貫して優れた性能を示します。\n主要な違い:\n\nRL が改善しない評価では、DPO モデルがしばしば優れた性能を発揮し、RL 訓練中もその優位性を維持（例: AlpacaEval）\nRL によって明示的にターゲットにされた評価では、DPO と SFT モデルの両方が同様の最終性能を達成（例: OMEGA）\nRL によってターゲットにされているが DPO からさらに改善が困難な評価では、SFT モデルが改善して DPO 性能に近づく（例: AIME 2025）\n\n\n\n\nRL 訓練中、すべてのドメインで報酬が着実に増加します（ただし、増加率は異なります）。指示追従データが最も着実に増加し、コード報酬が最もゆっくり増加します。\n\n\n\n\nOlmoRL は、GRPO をベースとした効率的な強化学習フレームワークであり、以下を実現しています：\n主要な貢献:\n\nアルゴリズムの改善: Vanilla GRPO に対して 7 つの重要な改善を実施\n大規模データセット: Dolci-Think-RL（100K プロンプト、4 ドメイン）\n効率的なインフラ: Continuous batching、active sampling、inflight updates により 4 倍の高速化\n複数ドメイン対応: Math、Code、Instruction-following、General chat\n拡張訓練: Olmo 3.1 Think 32B で 2300 ステップの拡張訓練により大幅な性能向上\n\nOlmoRL により、思考型モデルの訓練が大幅に効率化され、完全にオープンな RL 研究環境が提供されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-の概要",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-の概要",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、長い推論トレースを伴う強化学習の課題に対処するため、アルゴリズムとエンジニアリングインフラを密接に統合したシステムです。従来の数学とコードに限定されていた RLVR を、より幅広い検証可能なタスクへと拡張しています。\n主な特徴:\n\nアルゴリズムの改善: GRPO をベースに、DAPO や Dr GRPO などの最新の改善を統合\n大規模データセット: Dolci-Think-RL（約 100K プロンプト、4 ドメイン）\n効率的なインフラ: 長いシーケンス（最大 32K トークン）を効率的に処理する分散訓練システム\n4倍の高速化: OLMo 2 の RL インフラと比較して約 4 倍の高速化を達成",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-アルゴリズムの詳細",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-アルゴリズムの詳細",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL の強化学習段階は、GRPO（Shao et al., 2024）をベースに構築され、DAPO（Yu et al., 2025）や Dr GRPO（Liu et al., 2025b）などの最新のアルゴリズム改善を統合しています。\n\n\nOlmoRL は、Vanilla GRPO に対して以下の改善を実施しています：\n1. Zero gradient signal filtering:\n\n報酬がすべて同一のグループ（advantage の標準偏差がゼロ）を除外\nゼロ勾配のサンプルでの訓練を回避（DAPO と同様）\n\n2. Active sampling:\n\nZero gradient filtering にもかかわらず、一貫したバッチサイズを維持\n動的サンプリングの改良版を実装（詳細は §4.4.3）\n\n3. Token-level loss:\n\nサンプルごとではなく、バッチ全体のトークン数で損失を正規化\n長さバイアスを回避\n\n4. No KL loss:\n\nKL 損失を削除（GLM-4.5、DAPO、Dr GRPO などと同様の実践）\nより制限の少ないポリシー更新を可能にし、過最適化や訓練の不安定化を引き起こさない\n\n5. Clip higher:\n\n上限クリッピング項を下限よりわずかに高く設定\nトークンに対するより大きな更新を可能にする（Yu et al., 2025）\n\n6. Truncated importance sampling:\n\n推論エンジンと訓練エンジン間の対数確率の差を調整\n切り詰められた重要度サンプリング比を損失に掛ける（Yao et al., 2025）\n\n7. No standard deviation normalization:\n\nAdvantage 計算時に標準偏差で正規化しない（Liu et al., 2025b）\n難易度バイアスを除去（報酬の標準偏差が低い問題の advantage が過度に増加するのを防止）\n\n\n\n\n最終的な目的関数は、token-level loss、truncated importance sampling、clip-higher、および advantage 計算における標準偏差正規化なしを含みます：\n\\[\nJ(\\theta) = \\frac{1}{\\sum_{i=1}^{G} |y_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|y_i|} \\min\\left(\\frac{\\pi(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}{\\pi_{\\text{vllm}}(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}, \\rho\\right) \\times \\min(r_{i,t} A_{i,t}, \\text{clip}(r_{i,t}, 1 - \\varepsilon_{\\text{low}}, 1 + \\varepsilon_{\\text{high}}) A_{i,t})\n\\]\nここで：\n\n\\(r_{i,t} = \\frac{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta)}{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta_{\\text{old}})}\\)\n\\(\\varepsilon_{\\text{low}}\\) と \\(\\varepsilon_{\\text{high}}\\) はクリッピングハイパーパラメータ\n\\(\\rho\\) は truncated importance sampling の上限値\nAdvantage \\(A_{i,t}\\) は、グループ \\(G\\) 内での相対報酬に基づいて計算：\n\n\\[\nA_{i,t} = r(x, y_i) - \\text{mean}(\\{r(x, y_i)\\}_{i=1}^{G})\n\\]\n\n\n\n\n\n\nNoteGRPO と PPO の比較\n\n\n\n\n\nGRPO は PPO の変種であり、主な違いは報酬の正規化方法にあります。\nPPO:\n\n報酬を全データセットまたはバッチ全体で正規化\nグローバルなベースラインを使用\n\nGRPO:\n\n同じプロンプトから生成された応答グループ内で報酬を正規化\nグループベースの相対的な品質評価により、学習の安定性が向上\n思考型モデルのように出力長が大きく異なる場合に特に有効\n\nOlmoRL の改善:\n\nVanilla GRPO に対して 7 つの主要な改善を実施\nZero gradient filtering と active sampling により、訓練の効率と安定性を大幅に向上\n\n\n\n\n\n\n\nOlmoRL は、OLMo 2 の数学ドメインを超えて、一般的なドメインへと検証可能な報酬を拡張しています。各ドメインに対して異なるカスタム検証器を使用します：\nMath（数学）:\n\nルールベースの検証器\n基本的な正規化を実行し、SymPy を使用して参照回答と比較\n参照回答と同じであれば 1、そうでなければ 0 を返す\n\nCode（コーディング）:\n\nテストケースベースの検証器\n応答に対して一連のテストケースを実行\n\n通過したテストケースの割合を報酬とする、または (b) すべてのテストケースを通過した場合に 1、そうでなければ 0 を返す\n\n\nInstruction-following（指示追従）:\n\nプロンプトからの一連の制約への準拠をチェックする関数セットを通して応答を渡す\nすべての制約が満たされている場合に 1、そうでなければ 0 を返す\n\nChat—reference（参照付きチャット）:\n\nground-truth 応答がある場合、LM judge を使用してモデルの応答を参照回答と比較\n応答の品質に基づいて [0, 1] のスコアを付与\n\nChat—open-ended（オープンエンドチャット）:\n\n参照回答なしで、LM judge を使用して応答の品質に基づいて [0, 1] のスコアを付与",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#dolci-think-rl-データセット",
    "href": "ja/olmo-3/10-olmorl-grpo.html#dolci-think-rl-データセット",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Dolci-Think-RL は、4 つのドメイン（数学、コーディング、指示追従、一般的なチャット）にわたる約 100K のサンプルからなる大規模で多様なデータセットです。多様な推論タスクでの堅牢な RL を支援しながら、一般的な有用性を維持します。\n\n\n\n\n\n\n\n\n\n\n\nカテゴリ\nデータセット\nThink RL 用プロンプト数\nInstruct RL 用プロンプト数\n\n\n\n\nPrecise IF\nIF-RLVR\n30,186\n38,000\n\n\nMath\nOpen-Reasoner-Zero\n3,000\n14,000\n\n\n\nDAPO-Math\n2,584\n7,000\n\n\n\nAceReason-Math\n6,602\n-\n\n\n\nPolaris-Dataset\n-\n14,000\n\n\n\nKlearReasoner-MathSub\n3,000\n9,000\n\n\n\nOMEGA-train\n15,000\n20,000\n\n\nCoding\nAceCoder\n9,767\n20,000\n\n\n\nKlearReasoner-Code\n8,040\n-\n\n\n\nNemotron Post-training Code\n2,303\n-\n\n\n\nSYNTHETIC-2\n3,000\n-\n\n\nGeneral Chat\nTulu 3 SFT\n7,129\n18,955\n\n\n\nWildchat-4.8M\n7,129\n18,761\n\n\n\nMulti-Subject RLVR\n7,129\n12,234\n\n\n合計\n\n104,869\n171,950\n\n\n\n\n\n\nStep 1: プロンプトの調達:\n各ドメインから高品質なプロンプトを収集し、キュレーションします。\n\nMath: Open-Reasoner-Zero、DAPO-Math、AceReason-Math、KlearReasoner-MathSub、OMEGA など\nCoding: AceCoder、Klear-Reasoner Code、Nemotron Post-training Code、SYNTHETIC-2 など\nInstruction-following: IF-RLVR（最大 5 つの制約、IFEval と IFBench-Train からサンプリング）\nGeneral chat: Tülu 3 SFT、WildChat-4.8M、Multi-subject-RLVR\n\nStep 2: オフライン難易度フィルタリング:\n\nモデルの初期チェックポイントから各プロンプトに対して 8 つのロールアウトを生成\nモデルが容易に解決するサンプル（pass rate &gt; 62.5%）を除外\n温度 1.0、top-p 1.0 でサンプリング（RL 訓練時と一致）\n\nStep 3: データミキシング:\n\nドメイン固有の実験を実施し、最初の 500-1000 ステップで下流評価のトレンドを観察\n高品質なデータセットをアップウェイト\n各ドメインでほぼ同等のデータ量を使用（数学と指示追従にわずかに重点）\nOMEGA から特定のサブタスクをダウンサンプル",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-インフラストラクチャ",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-インフラストラクチャ",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、長いシーケンスとより高速な全体的スループットを処理するために、強化学習インフラストラクチャに大幅な改善を加えました。\n\n\nRL での主要な技術的課題は、推論（ロールアウト）の管理です。最終モデルでは、最大 32K トークンの長さのロールアウトを生成し、平均 10K トークン以上になります。\nリソース配分（32B モデルの場合）:\n\n訓練: 8 H100 ノード\n推論: 20 ノード\nGPU 利用率: 推論が訓練の約 5 倍の計算量を使用\n\n自己回帰推論のコストが高いため、学習器は時間の 75% をデータ待ちに費やします。\n\n\n\n+------------------------------------------------------------------+\n|               OlmoRL Infrastructure Components                   |\n+------------------------------------------------------------------+\n|                                                                  |\n|  1. Fully Asynchronous Training                                 |\n|     - Centralized learner across multiple nodes (DeepSpeed)     |\n|     - Large pool of actors (independent vLLM instances)         |\n|     - Prompts queue & Results queue                             |\n|                                                                  |\n|  2. Continuous Batching                                         |\n|     - Remove compute waste for long generations                 |\n|     - Constantly enqueue new generations as each one finishes   |\n|     - Up to 54% compute savings vs static batching              |\n|                                                                  |\n|  3. Active Sampling                                             |\n|     - Continuously pull completions and resample prompts        |\n|     - Filter until desired batch size is reached                |\n|     - More efficient than dynamic oversampling (3x reduction)   |\n|                                                                  |\n|  4. Inflight Updates                                            |\n|     - Update weights without pausing generation engine          |\n|     - Thread-safe, no KV cache invalidation                     |\n|     - Up to 4x throughput increase                              |\n|                                                                  |\n+------------------------------------------------------------------+\nFully Asynchronous Training:\n\n中央集権的な学習器を複数ノードに分散（DeepSpeed 使用）\n大規模なアクタープール、それぞれ独立した vLLM インスタンスを実行\n学習器がプロンプトをキューに入れ、アクターに配信\nアクターが環境と相互作用し、結果をキューを通じて返す\n\nContinuous Batching:\n\n各生成が終了するたびに新しい生成を絶えずエンキュー\n静的バッチングと比較して、長い生成での計算の無駄を削減\nOlmo 3 では、32K 生成長で平均 14,628 トークン、最大 32K トークン\n静的バッチングでは最大 54% の計算が無駄になっていた\n\nActive Sampling:\n\nフィルタリングされたインスタンスを補償するため、継続的にアクターから補完を引き出し、プロンプトをキューに再サンプリング\nゼロ勾配でない補完の所望のバッチサイズに到達するまで、アクティブにサンプリングとフィルタリングを実施\nDAPO の動的サンプリング（3 倍のオーバーサンプリング）よりも効率的\n\nInflight Updates:\n\n各訓練ステップ後、生成エンジンを一時停止せずに重みを即座に更新\nスレッドセーフな生成フレームワークに依存し、KV キャッシュを無効化せずに生成を継続\n同じリソースで最大 4 倍の高速化を達成\n\n\n\n\n\n\n\n設定\n総トークン数 (Mtok)\nトークン/秒\nMFU (%)\nMBU (%)\n\n\n\n\nOLMo 2\n6.34\n881\n0.30\n12.90\n\n\n+ continuous batching\n7.02\n975\n0.33\n14.29\n\n\n+ better threading\n9.77\n1358\n0.46\n19.89\n\n\n+ inflight updates (Olmo 3)\n21.23\n2949\n1.01\n43.21\n\n\n\nInflight updates の追加が最も劇的な改善をもたらしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmo-3.1-think-32b-の拡張訓練",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmo-3.1-think-32b-の拡張訓練",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Olmo 3.1 Think 32B は、拡張された OlmoRL 訓練を通じて、性能向上を示しました。Dolci Think RL データセットでの追加エポックにより、以下の改善が観察されました：\n性能向上:\n\nAIME 2024: +4 ポイント\nIFBench: +20 ポイント\n他のベンチマーク: 性能を維持\n\n拡張訓練により、より長い RL 訓練が汎化性能を向上させ、破局的忘却なしに安定した訓練が可能であることが確認されました。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#主要な発見",
    "href": "ja/olmo-3/10-olmorl-grpo.html#主要な発見",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Delta Learning を用いた選好調整を先に実施してから RLVR を適用すると、SFT 単独よりも優れた全体的な性能を達成します。\n\n\n\n最終的な RL ミックスを DPO モデルで実行すると、SFT モデルで実行する場合よりも一貫して優れた性能を示します。\n主要な違い:\n\nRL が改善しない評価では、DPO モデルがしばしば優れた性能を発揮し、RL 訓練中もその優位性を維持（例: AlpacaEval）\nRL によって明示的にターゲットにされた評価では、DPO と SFT モデルの両方が同様の最終性能を達成（例: OMEGA）\nRL によってターゲットにされているが DPO からさらに改善が困難な評価では、SFT モデルが改善して DPO 性能に近づく（例: AIME 2025）\n\n\n\n\nRL 訓練中、すべてのドメインで報酬が着実に増加します（ただし、増加率は異なります）。指示追従データが最も着実に増加し、コード報酬が最もゆっくり増加します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#まとめ",
    "href": "ja/olmo-3/10-olmorl-grpo.html#まとめ",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、GRPO をベースとした効率的な強化学習フレームワークであり、以下を実現しています：\n主要な貢献:\n\nアルゴリズムの改善: Vanilla GRPO に対して 7 つの重要な改善を実施\n大規模データセット: Dolci-Think-RL（100K プロンプト、4 ドメイン）\n効率的なインフラ: Continuous batching、active sampling、inflight updates により 4 倍の高速化\n複数ドメイン対応: Math、Code、Instruction-following、General chat\n拡張訓練: Olmo 3.1 Think 32B で 2300 ステップの拡張訓練により大幅な性能向上\n\nOlmoRL により、思考型モデルの訓練が大幅に効率化され、完全にオープンな RL 研究環境が提供されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  }
]