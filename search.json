[
  {
    "objectID": "ja/pdlt/index.html",
    "href": "ja/pdlt/index.html",
    "title": "The Principles of Deep Learning Theory",
    "section": "",
    "text": "The Principles of Deep Learning Theory (PDLT) は、物理学の有効理論（Effective Theory）のアプローチを深層学習に適用し、深層ニューラルネットワークの動作原理を理論的に解明する教科書である。\n著者の Daniel A. Roberts と Sho Yaida は、統計力学や場の理論で用いられる手法（繰り込み群、臨界性解析、ガウス過程など）を駆使して、深層学習の「なぜ動くのか」を first principles から説明する。\n論文: arXiv:2106.10165\nWeb サイト: deeplearningtheory.com",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory"
    ]
  },
  {
    "objectID": "ja/pdlt/index.html#主要なトピック",
    "href": "ja/pdlt/index.html#主要なトピック",
    "title": "The Principles of Deep Learning Theory",
    "section": "主要なトピック",
    "text": "主要なトピック\n\n有効理論アプローチ: 物理学の手法を深層学習に適用\n繰り込み群フロー (RG Flow): 層を通る信号の変化を理論的に追跡\n臨界性 (Criticality): 適切な初期化の理論的基盤\nNeural Tangent Kernel: 無限幅極限と有限幅の理論的区別\n表現学習: 深層学習の本質的能力の理論的説明\n普遍性クラス: 活性化関数の分類と選択の指針",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory"
    ]
  },
  {
    "objectID": "ja/pdlt/index.html#目次",
    "href": "ja/pdlt/index.html#目次",
    "title": "The Principles of Deep Learning Theory",
    "section": "目次",
    "text": "目次\n\n全体像\nEffective Theory Approach\nRG Flow\nCriticality\nNeural Tangent Kernel\nInfinite Width Limit\nRepresentation Learning\nUniversality Classes",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html",
    "href": "ja/pdlt/06-representation-learning.html",
    "title": "Representation Learning",
    "section": "",
    "text": "Representation Learning（表現学習）は、ニューラルネットワークが学習を通じて内部表現を変化させ、タスクに適した特徴を獲得する現象です。深層学習理論において、表現学習の有無は無限幅と有限幅のニューラルネットワークを決定的に区別する重要な性質です。\n\n\n無限幅のニューラルネットワークでは、表現学習が発生しません。Chapter 6.3.3 および Chapter 10.1.2 で示されるように、無限幅極限では以下の二つの制約が生じます。\n\n\n無限幅では、ネットワーク出力の各成分が独立に更新されます。例えば MSE 損失の場合、\\(i\\) 番目の出力 \\(z^{(L)}_i\\) の更新は \\(i\\) 番目の誤差因子のみに依存します。\n\\[\nd\\bar{z}^{(L)}_{i;\\delta} = -\\eta \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(L)}_{\\delta\\tilde{\\alpha}} \\epsilon_{i;\\tilde{\\alpha}}\n\\]\nこれにより、出力成分間の相関を学習することができません。\n\n\n\n最終隠れ層 \\(L-1\\) の事前活性化 \\(z^{(L-1)}_j\\) の更新分布を解析すると、その平均と分散はともに \\(1/n\\) で抑制されます。\n更新の平均:\n\\[\n\\mathbb{E}\\left[ d\\bar{z}^{(L-1)}_{j;\\delta} \\right] = O(1/n)\n\\]\n更新の共分散:\n\\[\n\\text{Cov}\\left[ d\\bar{z}^{(L-1)}_{j_1;\\delta_1}, d\\bar{z}^{(L-1)}_{j_2;\\delta_2} \\right] = O(1/n)\n\\]\n無限幅極限（\\(n \\to \\infty\\)）では、これらの量はゼロに漸近します。したがって、学習前後で隠れ層の分布は変化せず、表現学習が発生しません。\n\n\n\n\n\n\nImportant無限幅ネットワークの限界\n\n\n\n無限幅ニューラルネットワークは、本質的に線形モデルおよびカーネル法と等価です（Chapter 10.4）。これらは以下の制約を持ちます。\n\n固定された特徴表現（frozen features）\n線形な関数近似\nカーネルトリックに基づく予測\n\n実用的な深層学習モデルの柔軟性と表現力を理解するには、有限幅への拡張が不可欠です。\n\n\n\n\n\n\n有限幅のニューラルネットワークでは、表現学習が発生します。Chapter 6.4 および Chapter 11 で詳述されるように、有限幅では非ガウス相互作用が表現学習を可能にします。\n\n\n有限幅ニューラルネットワークは、神経連想（neural association）に対する帰納的バイアスを自動的に持ちます。これは Donald Hebb の古典的原理に基づきます。\n\nAny two cells or systems of cells that are repeatedly active at the same time will tend to become “associated,” so that activity in one facilitates activity in the other.\n— Donald Hebb, The Organization of Behavior (1949)\n\n有限幅では、ニューロン間の非ガウス相互作用により、一つの事前活性化 \\(z^{(\\ell)}_1\\) が別の事前活性化 \\(z^{(\\ell)}_2\\) に影響を与えます。この影響は条件付き分布 \\(p(z^{(\\ell)}_2 | \\hat{z}^{(\\ell)}_1)\\) で表現されます。\n無限幅では \\(p(z^{(\\ell)}_2 | \\hat{z}^{(\\ell)}_1) = p(z^{(\\ell)}_2)\\) となり、ニューロン間に関連性はありません。しかし有限幅では、この条件付き分布が事前分布から逸脱し、ヘブ学習が自然に生じます。\n\n\n\n\n\n\nNoteHebbian Learning の生物学的背景\n\n\n\n\n\nヘブ学習は、もともと生物学的ニューロンのシナプス可塑性を説明するために提案されました。「同時に発火するニューロンは結合する（Cells that fire together, wire together）」という原則は、以下の神経科学的現象と関連します。\n\nLong-Term Potentiation (LTP): シナプス結合の長期的な強化\nSpike-Timing-Dependent Plasticity (STDP): スパイクのタイミングに依存した可塑性\nNeural Association: 記憶と学習における神経回路の形成\n\n人工ニューラルネットワークにおいても、有限幅でこの原理が自然に現れることは、生物学的妥当性の観点から興味深い性質です。\n\n\n\n\n\n\n有限幅では、学習によって出力成分間の相関が生成されます。Chapter 6.4.2 で示されるように、ベイズ事後分布の平均 \\(p(z^{(L)}_B | y_A)\\) を計算すると、層内のニューロン相互作用により非自明な相関が生じます。\n\n\n\n最終隠れ層の事前活性化の事後分布 \\(p(z^{(L-1)}_B | y_A)\\) は、事前分布からの非ゼロのシフトを示します。これは層間相互作用により生じ、有限幅における表現学習の存在を示します（Chapter 6.4.3）。\n\n\n\n\nDifferential of the Neural Tangent Kernel (dNTK) は、表現学習を定量化する中心的な量です（Chapter 11）。\n\n\ndNTK は、NTK の訓練データに対する微分として定義されます（Chapter 11.1）。\n\n\n\nChapter 11.2 では、dNTK の再帰方程式（Renormalization Group flow）が導出されます。\n第1層: dNTK はゼロです（11.2.1）。\n第2層: dNTK が非ゼロになります（11.2.2）。\n深い層: dNTK は層とともに成長します（11.2.3）。\nこの成長は、深いネットワークほど表現学習能力が高いことを示唆します。\n\n\n\nChapter 11.3 では、dNTK の臨界解析が行われます。\nScale-Invariant Universality Class (11.3.1):\nReLU などのスケール不変活性化関数に対して、dNTK は特定の臨界指数で成長します。\n\\(K^* = 0\\) Universality Class (11.3.2):\ntanh や sin などの活性化関数に対して、異なる臨界挙動を示します。\n\n\n\n\n以下の表は、無限幅と有限幅におけるニューラルネットワークの性質を比較します。\n\n\n\n\n\n\n\n\nProperty\nInfinite Width\nFinite Width\n\n\n\n\nNTK Behavior\nFrozen (constant)\nEvolves during training\n\n\nOutput Correlation\nIndependent (No Wiring)\nCorrelated (Wiring)\n\n\nHidden Representation\nFixed (No Learning)\nChanges (Learning)\n\n\nHebbian Association\nAbsent\nPresent\n\n\nModel Class\nLinear / Kernel Method\nNonlinear Model\n\n\ndNTK\nZero\nNonzero (growing)\n\n\n\n\n\n\n\n\n\nTipKernel vs. Nearly-Kernel Methods\n\n\n\nChapter 11.4 では、有限幅ニューラルネットワークを非線形モデルおよびNearly-Kernel Methodsとして解釈する枠組みが提示されます（11.4.1, 11.4.2）。\n\n無限幅: カーネル法（Kernel Methods） — 固定カーネルに基づく線形予測\n有限幅: Nearly-Kernel Methods — カーネルが学習中に微小に進化\n\n有限幅ネットワークは「ほぼカーネル法」として動作しますが、dNTK による微小な非線形性が表現学習を可能にします（11.4.3）。\n\n\n\n\n\n表現学習は、深層学習が従来の機械学習手法を凌駕する主要な理由の一つです。\n\n\n有限幅ネットワークは、訓練データに基づいて内部表現を調整し、タスクに特化した特徴を学習します。\n\n\n\n深い層ほど抽象的な特徴を学習し、浅い層は低レベルの特徴を捉えます。dNTK の深さ依存性はこの性質を反映します。\n\n\n\n適切な表現を学習することで、訓練データに見られないパターンにも対応できる汎化能力が向上します。\n\n\n\n\nRepresentation Learning（表現学習） は、深層学習理論における中心的概念です。\n\n無限幅: 表現学習は発生せず、ネットワークはカーネル法に帰着（Chapter 6.3, 10）\n有限幅: Hebbian learning により表現学習が発生（Chapter 6.4）\ndNTK: 表現学習を定量化する重要な量（Chapter 11）\n理論的意義: 実用的な深層学習モデルの理解には有限幅理論が不可欠\n\n表現学習の理解は、深層ニューラルネットワークがなぜ強力なのかを説明する鍵となります。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#無限幅での表現学習の欠如",
    "href": "ja/pdlt/06-representation-learning.html#無限幅での表現学習の欠如",
    "title": "Representation Learning",
    "section": "",
    "text": "無限幅のニューラルネットワークでは、表現学習が発生しません。Chapter 6.3.3 および Chapter 10.1.2 で示されるように、無限幅極限では以下の二つの制約が生じます。\n\n\n無限幅では、ネットワーク出力の各成分が独立に更新されます。例えば MSE 損失の場合、\\(i\\) 番目の出力 \\(z^{(L)}_i\\) の更新は \\(i\\) 番目の誤差因子のみに依存します。\n\\[\nd\\bar{z}^{(L)}_{i;\\delta} = -\\eta \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(L)}_{\\delta\\tilde{\\alpha}} \\epsilon_{i;\\tilde{\\alpha}}\n\\]\nこれにより、出力成分間の相関を学習することができません。\n\n\n\n最終隠れ層 \\(L-1\\) の事前活性化 \\(z^{(L-1)}_j\\) の更新分布を解析すると、その平均と分散はともに \\(1/n\\) で抑制されます。\n更新の平均:\n\\[\n\\mathbb{E}\\left[ d\\bar{z}^{(L-1)}_{j;\\delta} \\right] = O(1/n)\n\\]\n更新の共分散:\n\\[\n\\text{Cov}\\left[ d\\bar{z}^{(L-1)}_{j_1;\\delta_1}, d\\bar{z}^{(L-1)}_{j_2;\\delta_2} \\right] = O(1/n)\n\\]\n無限幅極限（\\(n \\to \\infty\\)）では、これらの量はゼロに漸近します。したがって、学習前後で隠れ層の分布は変化せず、表現学習が発生しません。\n\n\n\n\n\n\nImportant無限幅ネットワークの限界\n\n\n\n無限幅ニューラルネットワークは、本質的に線形モデルおよびカーネル法と等価です（Chapter 10.4）。これらは以下の制約を持ちます。\n\n固定された特徴表現（frozen features）\n線形な関数近似\nカーネルトリックに基づく予測\n\n実用的な深層学習モデルの柔軟性と表現力を理解するには、有限幅への拡張が不可欠です。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#有限幅での表現学習の発生",
    "href": "ja/pdlt/06-representation-learning.html#有限幅での表現学習の発生",
    "title": "Representation Learning",
    "section": "",
    "text": "有限幅のニューラルネットワークでは、表現学習が発生します。Chapter 6.4 および Chapter 11 で詳述されるように、有限幅では非ガウス相互作用が表現学習を可能にします。\n\n\n有限幅ニューラルネットワークは、神経連想（neural association）に対する帰納的バイアスを自動的に持ちます。これは Donald Hebb の古典的原理に基づきます。\n\nAny two cells or systems of cells that are repeatedly active at the same time will tend to become “associated,” so that activity in one facilitates activity in the other.\n— Donald Hebb, The Organization of Behavior (1949)\n\n有限幅では、ニューロン間の非ガウス相互作用により、一つの事前活性化 \\(z^{(\\ell)}_1\\) が別の事前活性化 \\(z^{(\\ell)}_2\\) に影響を与えます。この影響は条件付き分布 \\(p(z^{(\\ell)}_2 | \\hat{z}^{(\\ell)}_1)\\) で表現されます。\n無限幅では \\(p(z^{(\\ell)}_2 | \\hat{z}^{(\\ell)}_1) = p(z^{(\\ell)}_2)\\) となり、ニューロン間に関連性はありません。しかし有限幅では、この条件付き分布が事前分布から逸脱し、ヘブ学習が自然に生じます。\n\n\n\n\n\n\nNoteHebbian Learning の生物学的背景\n\n\n\n\n\nヘブ学習は、もともと生物学的ニューロンのシナプス可塑性を説明するために提案されました。「同時に発火するニューロンは結合する（Cells that fire together, wire together）」という原則は、以下の神経科学的現象と関連します。\n\nLong-Term Potentiation (LTP): シナプス結合の長期的な強化\nSpike-Timing-Dependent Plasticity (STDP): スパイクのタイミングに依存した可塑性\nNeural Association: 記憶と学習における神経回路の形成\n\n人工ニューラルネットワークにおいても、有限幅でこの原理が自然に現れることは、生物学的妥当性の観点から興味深い性質です。\n\n\n\n\n\n\n有限幅では、学習によって出力成分間の相関が生成されます。Chapter 6.4.2 で示されるように、ベイズ事後分布の平均 \\(p(z^{(L)}_B | y_A)\\) を計算すると、層内のニューロン相互作用により非自明な相関が生じます。\n\n\n\n最終隠れ層の事前活性化の事後分布 \\(p(z^{(L-1)}_B | y_A)\\) は、事前分布からの非ゼロのシフトを示します。これは層間相互作用により生じ、有限幅における表現学習の存在を示します（Chapter 6.4.3）。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#dntk-の役割",
    "href": "ja/pdlt/06-representation-learning.html#dntk-の役割",
    "title": "Representation Learning",
    "section": "",
    "text": "Differential of the Neural Tangent Kernel (dNTK) は、表現学習を定量化する中心的な量です（Chapter 11）。\n\n\ndNTK は、NTK の訓練データに対する微分として定義されます（Chapter 11.1）。\n\n\n\nChapter 11.2 では、dNTK の再帰方程式（Renormalization Group flow）が導出されます。\n第1層: dNTK はゼロです（11.2.1）。\n第2層: dNTK が非ゼロになります（11.2.2）。\n深い層: dNTK は層とともに成長します（11.2.3）。\nこの成長は、深いネットワークほど表現学習能力が高いことを示唆します。\n\n\n\nChapter 11.3 では、dNTK の臨界解析が行われます。\nScale-Invariant Universality Class (11.3.1):\nReLU などのスケール不変活性化関数に対して、dNTK は特定の臨界指数で成長します。\n\\(K^* = 0\\) Universality Class (11.3.2):\ntanh や sin などの活性化関数に対して、異なる臨界挙動を示します。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#無限幅と有限幅の比較",
    "href": "ja/pdlt/06-representation-learning.html#無限幅と有限幅の比較",
    "title": "Representation Learning",
    "section": "",
    "text": "以下の表は、無限幅と有限幅におけるニューラルネットワークの性質を比較します。\n\n\n\n\n\n\n\n\nProperty\nInfinite Width\nFinite Width\n\n\n\n\nNTK Behavior\nFrozen (constant)\nEvolves during training\n\n\nOutput Correlation\nIndependent (No Wiring)\nCorrelated (Wiring)\n\n\nHidden Representation\nFixed (No Learning)\nChanges (Learning)\n\n\nHebbian Association\nAbsent\nPresent\n\n\nModel Class\nLinear / Kernel Method\nNonlinear Model\n\n\ndNTK\nZero\nNonzero (growing)\n\n\n\n\n\n\n\n\n\nTipKernel vs. Nearly-Kernel Methods\n\n\n\nChapter 11.4 では、有限幅ニューラルネットワークを非線形モデルおよびNearly-Kernel Methodsとして解釈する枠組みが提示されます（11.4.1, 11.4.2）。\n\n無限幅: カーネル法（Kernel Methods） — 固定カーネルに基づく線形予測\n有限幅: Nearly-Kernel Methods — カーネルが学習中に微小に進化\n\n有限幅ネットワークは「ほぼカーネル法」として動作しますが、dNTK による微小な非線形性が表現学習を可能にします（11.4.3）。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#表現学習の重要性",
    "href": "ja/pdlt/06-representation-learning.html#表現学習の重要性",
    "title": "Representation Learning",
    "section": "",
    "text": "表現学習は、深層学習が従来の機械学習手法を凌駕する主要な理由の一つです。\n\n\n有限幅ネットワークは、訓練データに基づいて内部表現を調整し、タスクに特化した特徴を学習します。\n\n\n\n深い層ほど抽象的な特徴を学習し、浅い層は低レベルの特徴を捉えます。dNTK の深さ依存性はこの性質を反映します。\n\n\n\n適切な表現を学習することで、訓練データに見られないパターンにも対応できる汎化能力が向上します。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/06-representation-learning.html#まとめ",
    "href": "ja/pdlt/06-representation-learning.html#まとめ",
    "title": "Representation Learning",
    "section": "",
    "text": "Representation Learning（表現学習） は、深層学習理論における中心的概念です。\n\n無限幅: 表現学習は発生せず、ネットワークはカーネル法に帰着（Chapter 6.3, 10）\n有限幅: Hebbian learning により表現学習が発生（Chapter 6.4）\ndNTK: 表現学習を定量化する重要な量（Chapter 11）\n理論的意義: 実用的な深層学習モデルの理解には有限幅理論が不可欠\n\n表現学習の理解は、深層ニューラルネットワークがなぜ強力なのかを説明する鍵となります。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Representation Learning"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html",
    "href": "ja/pdlt/04-neural-tangent-kernel.html",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "Neural Tangent Kernel（NTK、ニューラル接核）は、勾配降下法による深層学習の訓練ダイナミクスを理解するための重要な理論的ツールです。NTK は、訓練における微小なパラメータ更新がネットワーク出力に与える影響を特徴付けます。\n\n\nニューラルネットワークの出力 \\(z_i\\) をパラメータ \\(\\theta\\) の関数として考えたとき、勾配降下法による更新は次のように表されます：\n\\[\nz_i(\\theta(t+1)) - z_i(\\theta(t)) = -\\eta \\sum_{\\mu,\\nu} \\lambda_{\\mu\\nu} \\frac{dz_i}{d\\theta_\\mu} \\frac{\\partial \\mathcal{L}}{\\partial z_j} \\frac{dz_j}{d\\theta_\\nu}\n\\]\nここで、Neural Tangent Kernel は次のように定義されます：\n\\[\nH_{i_1 i_2; \\alpha_1 \\alpha_2} \\equiv \\sum_{\\mu,\\nu} \\lambda_{\\mu\\nu} \\frac{dz_{i_1;\\alpha_1}}{d\\theta_\\mu} \\frac{dz_{i_2;\\alpha_2}}{d\\theta_\\nu}\n\\]\nこの NTK \\(H\\) は、訓練データ \\(\\alpha_1, \\alpha_2\\) に対するネットワーク出力の勾配の内積として解釈でき、訓練中の出力変化を支配します。\n\n\n多層パーセプトロン（MLP）では、各層 \\(\\ell\\) における NTK \\(H^{(\\ell)}\\) を定義できます。これにより、隠れ層の表現がどのように変化するかも追跡できます。\n層ごとの NTK は次の再帰的な関係式を満たします：\n\\[\n\\hat{H}^{(\\ell+1)}_{i_1 i_2; \\alpha_1 \\alpha_2} = \\delta_{i_1 i_2} \\left[ \\lambda_b^{(\\ell+1)} + \\lambda_W^{(\\ell+1)} \\langle \\sigma_{\\alpha_1} \\sigma_{\\alpha_2} \\rangle_{K^{(\\ell)}} \\right] + C_W \\langle \\sigma'_{\\alpha_1} \\sigma'_{\\alpha_2} \\rangle_{K^{(\\ell)}} \\hat{H}^{(\\ell)}_{i_1 i_2; \\alpha_1 \\alpha_2}\n\\]\nここで、\\(\\sigma\\) は活性化関数、\\(K^{(\\ell)}\\) はカーネル、\\(\\lambda_b, \\lambda_W\\) は学習率です。\n\n\n\n\nネットワーク幅 \\(n \\to \\infty\\) の極限では、NTK は確定的になります。つまり、ネットワークの異なるインスタンス間で NTK が自己平均化し、ランダム性が消失します。\nこの極限で NTK は Frozen NTK（凍結 NTK） \\(\\Theta^{(\\ell)}\\) に収束します：\n\\[\n\\Theta^{(\\ell+1)} = \\lambda_b^{(\\ell+1)} + \\lambda_W^{(\\ell+1)} g(K^{(\\ell)}) + \\chi_\\perp(K^{(\\ell)}) \\Theta^{(\\ell)}\n\\]\nここで：\n\n\\(g(K) = \\langle \\sigma(z) \\sigma(z) \\rangle_K\\) は活性化関数の2点相関\n\\(\\chi_\\perp(K) = C_W \\langle \\sigma'(z) \\sigma'(z) \\rangle_K\\) は横方向感受率\n\n\n\n無限幅ネットワークでは以下の性質が成り立ちます：\n\nNo Wiring（配線なし）: 出力の各成分 \\(z_i^{(L)}\\) が独立に更新され、成分間の相関が学習されない\nNo Representation Learning（表現学習なし）: 隠れ層の表現が訓練中に本質的に変化しない\nAlgorithm Independence（アルゴリズム独立性）: 勾配降下法でもニュートン法でも、同じ Frozen NTK によって訓練ダイナミクスが支配される\n\nこれらの制限は、無限幅ネットワークが本質的に線形モデルとして振る舞うことを意味します。\n\n\n\n\n\n\n無限幅極限では、NTK は訓練中に不変です。これを「凍結（frozen）」と呼ぶ理由です。\n初期化時の Frozen NTK \\(\\Theta^{(\\ell)}\\) は訓練を通じて変化せず、ネットワーク出力の更新は次のように線形化されます：\n\\[\n\\bar{z}^{(\\ell)}_{i;\\delta}(t=1) - z^{(\\ell)}_{i;\\delta}(t=0) = -\\eta \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(\\ell)}_{\\delta\\tilde{\\alpha}} \\frac{d\\mathcal{L}_\\mathcal{A}}{dz^{(\\ell)}_{i;\\tilde{\\alpha}}}\n\\]\nこの線形性により、多段階訓練後の予測も閉形式で表現できます（Kernel Prediction）。\n\n\n\n幅が有限 \\(n &lt; \\infty\\) の場合、NTK は訓練中に変化します。これを NTK の解凍（defrosting） と呼びます。\n有限幅補正は \\(1/n\\) 展開で表され、次のような項が現れます：\n\nNTK の平均: \\(H^{(\\ell)} = \\Theta^{(\\ell)} + \\frac{1}{n} H^{\\{1\\}(\\ell)} + O(1/n^2)\\)\nNTK のゆらぎ: ネットワークインスタンス間で NTK が確率的に変動\n前活性化との交差相関: NTK と前活性化 \\(z^{(\\ell)}\\) の間に相関が生じる\n\nこれらの有限幅効果により、表現学習（Representation Learning） が可能になります。\n\n\n\n\n\n\nChapter 8 では、NTK の統計的性質を層ごとに再帰的に計算する手法を導入します。\n主要な結果:\n\n第1層: NTK は確定的（ゆらぎなし）\n\\[\n\\hat{H}^{(1)} = \\lambda_b^{(1)} + \\lambda_W^{(1)} \\frac{1}{n_0} \\sum_{j=1}^{n_0} x_j^2\n\\]\n第2層: NTK にゆらぎが出現し、前活性化との交差相関が生じる\n深い層: ゆらぎと交差相関が層を重ねるごとに蓄積される\n\n再帰関係式は次のようになります：\n\nNTK 平均の再帰: (8.63) 式\nNTK-前活性化交差相関の再帰: (8.77), (8.79) 式\nNTK 分散の再帰: (8.89), (8.97) 式\n\nこれらの再帰は、前活性化の RG フロー（Chapter 4）と同様の構造を持ちます。\n\n\n\nChapter 9 では、初期化時の NTK の臨界性（criticality）と普遍性（universality）を解析します。\n臨界性解析:\n初期化ハイパーパラメータ \\(C_b, C_W\\) と学習率 \\(\\lambda_b^{(\\ell)}, \\lambda_W^{(\\ell)}\\) を適切に調整することで、深いネットワークでも安定した訓練が可能になります。\n普遍性クラス:\n\nScale-Invariant 普遍性クラス（ReLU など）: 深さ対幅の比 \\(L/n\\) が重要なパラメータ\n\\(K^* = 0\\) 普遍性クラス（tanh など）: 異なるスケーリング挙動\n\n勾配消失・爆発問題の解決:\n臨界性の概念により、深層ネットワークでの勾配消失・勾配爆発問題が解決されます。適切な初期化（\\(C_W = 1\\) など）と学習率のスケーリングにより、勾配が層を伝播する際に指数的に減衰・増幅することを防ぎます。\n\n\n\nChapter 10 では、無限幅ネットワークの訓練を完全に解析します。\n小さなステップ（§10.1）:\n初期化直後の1ステップ更新では、Frozen NTK \\(\\Theta^{(\\ell)}\\) が支配的で、更新は学習率 \\(\\eta\\) について線形です。\n大きなジャンプ（§10.2）:\n完全に訓練されたネットワークの閉形式解が得られます。これは、1回のニュートン法でも多段階勾配降下法でも同じです（アルゴリズム独立性）。\nカーネル予測:\nテストデータ \\(x_{\\dot{\\beta}}\\) に対する予測は次の形になります：\n\\[\n\\bar{z}^{(L)}_{i;\\dot{\\beta}} = z^{(L)}_{i;\\dot{\\beta}}(t=0) + \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(L)}_{\\dot{\\beta}\\tilde{\\alpha}} \\cdot (\\text{訓練誤差の関数})\n\\]\n予測が初期出力、Frozen NTK、訓練データのみで決定されることがわかります。\n汎化（§10.3）:\n汎化誤差は Bias-Variance トレードオフ として分解されます：\n\nBias（バイアス）: アンサンブル平均予測と真の値のずれ\nVariance（分散）: ネットワークインスタンス間の予測のばらつき\n\n臨界性により、このトレードオフが最適化されます。\n線形モデルとの関連（§10.4）:\n無限幅ネットワークは、ランダム特徴に基づく線形モデルと等価です。従来のカーネル法との対応が明確になります。\n\n\n\n\n\n\nNoteKernel Methods との関連\n\n\n\n\n\nカーネル法（Kernel Methods） は、機械学習における古典的な手法で、データを高次元特徴空間に写像し、その空間での線形分離を行います。\n\n\nカーネル関数 \\(K(x, x')\\) は、特徴写像 \\(\\phi(x)\\) の内積として定義されます：\n\\[\nK(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\n\\]\n典型的なカーネル：\n\n線形カーネル: \\(K(x, x') = x^\\top x'\\)\n多項式カーネル: \\(K(x, x') = (x^\\top x' + c)^d\\)\nRBF カーネル: \\(K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\\)\n\n\n\n\n無限幅ニューラルネットワークの Frozen NTK \\(\\Theta^{(L)}\\) は、カーネル法のカーネル関数と同じ役割を果たします。\n具体的には、無限幅ネットワークの予測は次の形になります：\n\\[\nf(x) = f_0(x) + \\sum_{\\alpha \\in \\mathcal{A}} \\Theta^{(L)}(x, x_\\alpha) \\cdot (\\text{誤差})\n\\]\nこれは、カーネル法の Kernel Ridge Regression と同型です：\n\\[\nf(x) = \\sum_{\\alpha \\in \\mathcal{A}} \\alpha_\\alpha K(x, x_\\alpha)\n\\]\n\n\n\n無限幅ネットワークは、次の理由でカーネル法と同様の制限を持ちます：\n\n特徴が固定: 訓練中に内部表現（隠れ層）が変化しない\n線形モデル: パラメータに関して線形な予測を行う\nカーネルの固定: Frozen NTK は訓練を通じて不変\n\nこれらの制限を超えるには、有限幅ネットワークの解析が不可欠です（Chapter 11 で扱います）。\n\n\n\n有限幅では、NTK が訓練中に変化し（解凍）、適応的なカーネルとして振る舞います。これにより、真の表現学習が可能になります。\n\n\n\n\n\n\n\n\nNeural Tangent Kernel（NTK）は、深層学習の訓練ダイナミクスを理解するための強力な理論的枠組みです。\n\n無限幅極限: NTK は確定的（Frozen）で、ネットワークは線形モデルとして振る舞う\n有限幅: NTK が確率的にゆらぎ、訓練中に変化することで表現学習が可能になる\n臨界性: 適切なハイパーパラメータチューニングにより、深いネットワークでも安定した訓練が実現\nカーネル法との関連: 無限幅ネットワークは従来のカーネル法と等価だが、有限幅では適応的カーネルとして振る舞う\n\nNTK 理論は、深層学習の「なぜ機能するのか」を理論的に解明する上で中心的な役割を果たしています。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html#ntk-とは何か",
    "href": "ja/pdlt/04-neural-tangent-kernel.html#ntk-とは何か",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "ニューラルネットワークの出力 \\(z_i\\) をパラメータ \\(\\theta\\) の関数として考えたとき、勾配降下法による更新は次のように表されます：\n\\[\nz_i(\\theta(t+1)) - z_i(\\theta(t)) = -\\eta \\sum_{\\mu,\\nu} \\lambda_{\\mu\\nu} \\frac{dz_i}{d\\theta_\\mu} \\frac{\\partial \\mathcal{L}}{\\partial z_j} \\frac{dz_j}{d\\theta_\\nu}\n\\]\nここで、Neural Tangent Kernel は次のように定義されます：\n\\[\nH_{i_1 i_2; \\alpha_1 \\alpha_2} \\equiv \\sum_{\\mu,\\nu} \\lambda_{\\mu\\nu} \\frac{dz_{i_1;\\alpha_1}}{d\\theta_\\mu} \\frac{dz_{i_2;\\alpha_2}}{d\\theta_\\nu}\n\\]\nこの NTK \\(H\\) は、訓練データ \\(\\alpha_1, \\alpha_2\\) に対するネットワーク出力の勾配の内積として解釈でき、訓練中の出力変化を支配します。\n\n\n多層パーセプトロン（MLP）では、各層 \\(\\ell\\) における NTK \\(H^{(\\ell)}\\) を定義できます。これにより、隠れ層の表現がどのように変化するかも追跡できます。\n層ごとの NTK は次の再帰的な関係式を満たします：\n\\[\n\\hat{H}^{(\\ell+1)}_{i_1 i_2; \\alpha_1 \\alpha_2} = \\delta_{i_1 i_2} \\left[ \\lambda_b^{(\\ell+1)} + \\lambda_W^{(\\ell+1)} \\langle \\sigma_{\\alpha_1} \\sigma_{\\alpha_2} \\rangle_{K^{(\\ell)}} \\right] + C_W \\langle \\sigma'_{\\alpha_1} \\sigma'_{\\alpha_2} \\rangle_{K^{(\\ell)}} \\hat{H}^{(\\ell)}_{i_1 i_2; \\alpha_1 \\alpha_2}\n\\]\nここで、\\(\\sigma\\) は活性化関数、\\(K^{(\\ell)}\\) はカーネル、\\(\\lambda_b, \\lambda_W\\) は学習率です。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html#無限幅極限での-ntk-の性質",
    "href": "ja/pdlt/04-neural-tangent-kernel.html#無限幅極限での-ntk-の性質",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "ネットワーク幅 \\(n \\to \\infty\\) の極限では、NTK は確定的になります。つまり、ネットワークの異なるインスタンス間で NTK が自己平均化し、ランダム性が消失します。\nこの極限で NTK は Frozen NTK（凍結 NTK） \\(\\Theta^{(\\ell)}\\) に収束します：\n\\[\n\\Theta^{(\\ell+1)} = \\lambda_b^{(\\ell+1)} + \\lambda_W^{(\\ell+1)} g(K^{(\\ell)}) + \\chi_\\perp(K^{(\\ell)}) \\Theta^{(\\ell)}\n\\]\nここで：\n\n\\(g(K) = \\langle \\sigma(z) \\sigma(z) \\rangle_K\\) は活性化関数の2点相関\n\\(\\chi_\\perp(K) = C_W \\langle \\sigma'(z) \\sigma'(z) \\rangle_K\\) は横方向感受率\n\n\n\n無限幅ネットワークでは以下の性質が成り立ちます：\n\nNo Wiring（配線なし）: 出力の各成分 \\(z_i^{(L)}\\) が独立に更新され、成分間の相関が学習されない\nNo Representation Learning（表現学習なし）: 隠れ層の表現が訓練中に本質的に変化しない\nAlgorithm Independence（アルゴリズム独立性）: 勾配降下法でもニュートン法でも、同じ Frozen NTK によって訓練ダイナミクスが支配される\n\nこれらの制限は、無限幅ネットワークが本質的に線形モデルとして振る舞うことを意味します。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html#訓練中の-ntk-の変化",
    "href": "ja/pdlt/04-neural-tangent-kernel.html#訓練中の-ntk-の変化",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "無限幅極限では、NTK は訓練中に不変です。これを「凍結（frozen）」と呼ぶ理由です。\n初期化時の Frozen NTK \\(\\Theta^{(\\ell)}\\) は訓練を通じて変化せず、ネットワーク出力の更新は次のように線形化されます：\n\\[\n\\bar{z}^{(\\ell)}_{i;\\delta}(t=1) - z^{(\\ell)}_{i;\\delta}(t=0) = -\\eta \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(\\ell)}_{\\delta\\tilde{\\alpha}} \\frac{d\\mathcal{L}_\\mathcal{A}}{dz^{(\\ell)}_{i;\\tilde{\\alpha}}}\n\\]\nこの線形性により、多段階訓練後の予測も閉形式で表現できます（Kernel Prediction）。\n\n\n\n幅が有限 \\(n &lt; \\infty\\) の場合、NTK は訓練中に変化します。これを NTK の解凍（defrosting） と呼びます。\n有限幅補正は \\(1/n\\) 展開で表され、次のような項が現れます：\n\nNTK の平均: \\(H^{(\\ell)} = \\Theta^{(\\ell)} + \\frac{1}{n} H^{\\{1\\}(\\ell)} + O(1/n^2)\\)\nNTK のゆらぎ: ネットワークインスタンス間で NTK が確率的に変動\n前活性化との交差相関: NTK と前活性化 \\(z^{(\\ell)}\\) の間に相関が生じる\n\nこれらの有限幅効果により、表現学習（Representation Learning） が可能になります。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html#chapter-8-9-10-の要点",
    "href": "ja/pdlt/04-neural-tangent-kernel.html#chapter-8-9-10-の要点",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "Chapter 8 では、NTK の統計的性質を層ごとに再帰的に計算する手法を導入します。\n主要な結果:\n\n第1層: NTK は確定的（ゆらぎなし）\n\\[\n\\hat{H}^{(1)} = \\lambda_b^{(1)} + \\lambda_W^{(1)} \\frac{1}{n_0} \\sum_{j=1}^{n_0} x_j^2\n\\]\n第2層: NTK にゆらぎが出現し、前活性化との交差相関が生じる\n深い層: ゆらぎと交差相関が層を重ねるごとに蓄積される\n\n再帰関係式は次のようになります：\n\nNTK 平均の再帰: (8.63) 式\nNTK-前活性化交差相関の再帰: (8.77), (8.79) 式\nNTK 分散の再帰: (8.89), (8.97) 式\n\nこれらの再帰は、前活性化の RG フロー（Chapter 4）と同様の構造を持ちます。\n\n\n\nChapter 9 では、初期化時の NTK の臨界性（criticality）と普遍性（universality）を解析します。\n臨界性解析:\n初期化ハイパーパラメータ \\(C_b, C_W\\) と学習率 \\(\\lambda_b^{(\\ell)}, \\lambda_W^{(\\ell)}\\) を適切に調整することで、深いネットワークでも安定した訓練が可能になります。\n普遍性クラス:\n\nScale-Invariant 普遍性クラス（ReLU など）: 深さ対幅の比 \\(L/n\\) が重要なパラメータ\n\\(K^* = 0\\) 普遍性クラス（tanh など）: 異なるスケーリング挙動\n\n勾配消失・爆発問題の解決:\n臨界性の概念により、深層ネットワークでの勾配消失・勾配爆発問題が解決されます。適切な初期化（\\(C_W = 1\\) など）と学習率のスケーリングにより、勾配が層を伝播する際に指数的に減衰・増幅することを防ぎます。\n\n\n\nChapter 10 では、無限幅ネットワークの訓練を完全に解析します。\n小さなステップ（§10.1）:\n初期化直後の1ステップ更新では、Frozen NTK \\(\\Theta^{(\\ell)}\\) が支配的で、更新は学習率 \\(\\eta\\) について線形です。\n大きなジャンプ（§10.2）:\n完全に訓練されたネットワークの閉形式解が得られます。これは、1回のニュートン法でも多段階勾配降下法でも同じです（アルゴリズム独立性）。\nカーネル予測:\nテストデータ \\(x_{\\dot{\\beta}}\\) に対する予測は次の形になります：\n\\[\n\\bar{z}^{(L)}_{i;\\dot{\\beta}} = z^{(L)}_{i;\\dot{\\beta}}(t=0) + \\sum_{\\tilde{\\alpha} \\in \\mathcal{A}} \\Theta^{(L)}_{\\dot{\\beta}\\tilde{\\alpha}} \\cdot (\\text{訓練誤差の関数})\n\\]\n予測が初期出力、Frozen NTK、訓練データのみで決定されることがわかります。\n汎化（§10.3）:\n汎化誤差は Bias-Variance トレードオフ として分解されます：\n\nBias（バイアス）: アンサンブル平均予測と真の値のずれ\nVariance（分散）: ネットワークインスタンス間の予測のばらつき\n\n臨界性により、このトレードオフが最適化されます。\n線形モデルとの関連（§10.4）:\n無限幅ネットワークは、ランダム特徴に基づく線形モデルと等価です。従来のカーネル法との対応が明確になります。\n\n\n\n\n\n\nNoteKernel Methods との関連\n\n\n\n\n\nカーネル法（Kernel Methods） は、機械学習における古典的な手法で、データを高次元特徴空間に写像し、その空間での線形分離を行います。\n\n\nカーネル関数 \\(K(x, x')\\) は、特徴写像 \\(\\phi(x)\\) の内積として定義されます：\n\\[\nK(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\n\\]\n典型的なカーネル：\n\n線形カーネル: \\(K(x, x') = x^\\top x'\\)\n多項式カーネル: \\(K(x, x') = (x^\\top x' + c)^d\\)\nRBF カーネル: \\(K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\\)\n\n\n\n\n無限幅ニューラルネットワークの Frozen NTK \\(\\Theta^{(L)}\\) は、カーネル法のカーネル関数と同じ役割を果たします。\n具体的には、無限幅ネットワークの予測は次の形になります：\n\\[\nf(x) = f_0(x) + \\sum_{\\alpha \\in \\mathcal{A}} \\Theta^{(L)}(x, x_\\alpha) \\cdot (\\text{誤差})\n\\]\nこれは、カーネル法の Kernel Ridge Regression と同型です：\n\\[\nf(x) = \\sum_{\\alpha \\in \\mathcal{A}} \\alpha_\\alpha K(x, x_\\alpha)\n\\]\n\n\n\n無限幅ネットワークは、次の理由でカーネル法と同様の制限を持ちます：\n\n特徴が固定: 訓練中に内部表現（隠れ層）が変化しない\n線形モデル: パラメータに関して線形な予測を行う\nカーネルの固定: Frozen NTK は訓練を通じて不変\n\nこれらの制限を超えるには、有限幅ネットワークの解析が不可欠です（Chapter 11 で扱います）。\n\n\n\n有限幅では、NTK が訓練中に変化し（解凍）、適応的なカーネルとして振る舞います。これにより、真の表現学習が可能になります。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/04-neural-tangent-kernel.html#まとめ",
    "href": "ja/pdlt/04-neural-tangent-kernel.html#まとめ",
    "title": "Neural Tangent Kernel",
    "section": "",
    "text": "Neural Tangent Kernel（NTK）は、深層学習の訓練ダイナミクスを理解するための強力な理論的枠組みです。\n\n無限幅極限: NTK は確定的（Frozen）で、ネットワークは線形モデルとして振る舞う\n有限幅: NTK が確率的にゆらぎ、訓練中に変化することで表現学習が可能になる\n臨界性: 適切なハイパーパラメータチューニングにより、深いネットワークでも安定した訓練が実現\nカーネル法との関連: 無限幅ネットワークは従来のカーネル法と等価だが、有限幅では適応的カーネルとして振る舞う\n\nNTK 理論は、深層学習の「なぜ機能するのか」を理論的に解明する上で中心的な役割を果たしています。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Neural Tangent Kernel"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html",
    "href": "ja/pdlt/02-rg-flow.html",
    "title": "RG Flow",
    "section": "",
    "text": "Renormalization Group Flow (繰り込み群フロー, RG Flow) は、深層ニューラルネットワークにおける層ごとの表現の変化を理解するための理論的枠組みです。物理学における繰り込み群の概念を深層学習に応用し、ネットワークが入力データから出力へと情報を段階的に変換していく過程を形式化します。\n\n\n繰り込み群 (Renormalization Group, RG) は元々物理学で発展した概念で、スケールの異なる階層において系の振る舞いがどのように変化するかを記述する手法です。微視的な自由度を積分消去 (marginalize out) することで、より粗視化された有効理論を導出します。\n\n\n\n\n\n\nNote物理学における繰り込み群\n\n\n\n\n\n物理学では、繰り込み群は複雑な相互作用系を理解するための強力な手法として確立されています。\n基本的なアイデア:\n\n微視的スケールから巨視的スケールへと観測スケールを変化させる\n各スケールで有効な相互作用パラメータがどのように「流れる」かを追跡\n微視的自由度を積分消去し、粗視化された記述を得る\n\n深層学習との類似:\n\n物理学: 微視的スケール → 巨視的スケール\n深層学習: 入力層 → 隠れ層 → 出力層\n\nどちらも、細かい情報を段階的に粗視化し、より高次の抽象的な記述へと変換していくプロセスです。\n\n\n\n\n\n\n深層ニューラルネットワークでは、RG Flow は以下のプロセスを形式化します。\n層ごとの変換:\n┌──────────────────────────────────────────────────┐\n│  Layer l -&gt; Layer l+1                            │\n│                                                  │\n│  Preactivations z^(l) -&gt; z^(l+1)                 │\n│                                                  │\n│  Distribution p(z^(l)) -&gt; p(z^(l+1))             │\n└──────────────────────────────────────────────────┘\n各層において、前層の preactivation（活性化前の値）の分布を積分消去し、次層の preactivation の分布を導出します。このプロセスは、物理学における繰り込み変換に対応します。\n\n\n\n\n\n初期化時、第1層の preactivation はガウス分布に従います (Chapter 4.1)。\n定義:\n\\[\nz^{(1)}_{i;\\alpha} = b^{(1)}_i + \\sum_{j=1}^{n_0} W^{(1)}_{ij} x_{j;\\alpha}\n\\]\nここで、バイアス \\(b^{(1)}\\) と重み \\(W^{(1)}\\) は独立なガウス分布から初期化されます。\n分布:\n\\[\np(z^{(1)} | \\mathcal{D}) = \\frac{1}{Z} e^{-S(z^{(1)})}\n\\]\n作用 (action) は2次形式:\n\\[\nS(z^{(1)}) = \\frac{1}{2} \\sum_{i=1}^{n_1} \\sum_{\\alpha_1, \\alpha_2 \\in \\mathcal{D}} G^{\\alpha_1 \\alpha_2}_{(1)} z^{(1)}_{i;\\alpha_1} z^{(1)}_{i;\\alpha_2}\n\\]\n\\(G^{(1)}_{\\alpha_1 \\alpha_2}\\) は第1層の計量 (metric) で、サンプル間の相関を表します。\n重要な性質:\n\nすべての奇数次相関関数がゼロ\n4次以上の連結相関関数がゼロ（Wick の定理が成立）\n完全にガウス的\n\n\n\n\n第2層では、非ガウス性 (non-Gaussianity) が出現します (Chapter 4.2)。活性化関数が非線形であるため、4次連結相関関数が非ゼロになります。\n分布の変化:\n\\[\np(z^{(2)} | \\mathcal{D}) \\approx \\frac{1}{Z} \\exp\\left( -S_2(z^{(2)}) - S_4(z^{(2)}) + O(1/n^2) \\right)\n\\]\nここで:\n\n\\(S_2\\): 2次作用（ガウス部分）\n\\(S_4\\): 4次作用（非ガウス補正、\\(O(1/n)\\) で抑制）\n\nLarge-\\(n\\) expansion:\nネットワーク幅 \\(n\\) が大きいとき、非ガウス性は \\(1/n\\) で抑制されます。この性質により、有限幅ネットワークの振る舞いを体系的に展開できます。\n\n\n\n第3層以降、非ガウス性は層ごとに蓄積していきます (Chapter 4.3)。\n再帰的構造:\n各層 \\(\\ell\\) から層 \\(\\ell+1\\) への遷移は、以下の2つの寄与を含みます。\n\n第 \\(\\ell\\) 層から継承された非ガウス性\n第 \\(\\ell\\) 層から第 \\(\\ell+1\\) 層への遷移で新たに生成された非ガウス性\n\n深さと幅の比:\n非ガウス性の蓄積は、深さ \\(L\\) と幅 \\(n\\) の比 \\(L/n\\) によって特徴づけられます。ネットワークが well-behaved であるためには、この比が適切に制御される必要があります (depth-to-width ratio)。\n流れの図示:\ngraph LR\n    A[Layer 1&lt;br/&gt;Gaussian] --&gt; B[Layer 2&lt;br/&gt;Weak non-Gaussian]\n    B --&gt; C[Layer 3&lt;br/&gt;Moderate non-Gaussian]\n    C --&gt; D[Layer L&lt;br/&gt;Accumulated non-Gaussian]\n\n    style A fill:#e1f5ff\n    style B fill:#ffe1f5\n    style D fill:#ffe1e1\n\n\n\n\nNeural Tangent Kernel (NTK) もまた、層ごとに「流れ」ます (Chapter 8)。NTK は勾配降下法による学習のダイナミクスを支配する量です。\n\n\n定義:\n\\[\n\\hat{H}^{(\\ell)}_{i_1 i_2; \\alpha_1 \\alpha_2} \\equiv \\sum_{\\mu, \\nu} \\lambda^{\\mu\\nu} \\frac{dz^{(\\ell)}_{i_1;\\alpha_1}}{d\\theta^\\mu} \\frac{dz^{(\\ell)}_{i_2;\\alpha_2}}{d\\theta^\\nu}\n\\]\nここで、\\(\\lambda^{\\mu\\nu}\\) は学習率テンソル、\\(\\theta^\\mu\\) はモデルパラメータです。\n\n\n\n第 \\(\\ell\\) 層の NTK から第 \\(\\ell+1\\) 層の NTK への遷移は、以下の再帰式で記述されます (Chapter 8.0)。\nForward equation:\n\\[\n\\hat{H}^{(\\ell+1)}_{i_1 i_2; \\alpha_1 \\alpha_2} = \\delta_{i_1 i_2} \\left( \\lambda^{(\\ell+1)}_b + \\lambda^{(\\ell+1)}_W V^{(\\ell)}_{\\alpha_1 \\alpha_2} \\right) + \\sum_{j_1, j_2} \\frac{dz^{(\\ell+1)}_{i_1;\\alpha_1}}{dz^{(\\ell)}_{j_1;\\alpha_1}} \\frac{dz^{(\\ell+1)}_{i_2;\\alpha_2}}{dz^{(\\ell)}_{j_2;\\alpha_2}} \\hat{H}^{(\\ell)}_{j_1 j_2; \\alpha_1 \\alpha_2}\n\\]\nこの式は:\n\n第1項: 第 \\(\\ell+1\\) 層のパラメータからの直接寄与\n第2項: 第 \\(\\ell\\) 層以前からの寄与（連鎖律による）\n\n\n\n\n初期化時、NTK は確率的な量です (Chapter 8.1-8.3)。\n第1層: NTK は決定論的（ゆらぎなし）\n第2層: NTK にゆらぎが出現\n深い層: NTK のゆらぎが蓄積\nPreactivation の RG Flow と完全に並行して、NTK の統計も再帰的に計算できます。\n\n\n\n\nRG Flow の定式化により、以下が可能になります。\n理論的洞察:\n\nネットワークの深さ \\(L\\) と幅 \\(n\\) の役割の明確化\n臨界性 (criticality) の概念の拡張\n有限幅効果の体系的な理解\n\n実用的示唆:\n\n初期化スキームの設計原理\n深さと幅のバランスの指針\n勾配消失・爆発問題の理解\n\n\n\n\n\n\n\nTipRG Flow と表現学習\n\n\n\nRG Flow は、深層ニューラルネットワークがデータの表現を段階的に抽象化していくプロセスを、厳密な数学的枠組みで記述します。\n直感的理解:\n\n入力層: 細粒度の生データ\n隠れ層: 中間的な特徴表現（段階的に抽象化）\n出力層: 粗視化された最終表現\n\nこのプロセスは、物理学における微視的記述から巨視的記述への移行と本質的に同じ構造を持っています。\n\n\n\n\n\nRenormalization Group Flow は、深層学習の理論的理解において中心的な役割を果たします。\n主要な結果:\n\n第1層はガウス分布、第2層以降で非ガウス性が出現・蓄積\n非ガウス性は \\(1/n\\) 展開で体系的に制御可能\nPreactivation と NTK の両方が層ごとに「流れ」る\n深さと幅の比 \\(L/n\\) が重要なパラメータ\n\n次のステップ:\n\nRG Flow の方程式を解き、臨界性を解析 (Chapter 5)\nNTK の統計を用いて学習ダイナミクスを理解 (Chapter 9, 10)\n表現学習の出現条件を明らかにする (Chapter 11)\n\nRG Flow は、深層学習の「有効理論」(effective theory) を構築するための基盤となる概念です。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#繰り込み群とは",
    "href": "ja/pdlt/02-rg-flow.html#繰り込み群とは",
    "title": "RG Flow",
    "section": "",
    "text": "繰り込み群 (Renormalization Group, RG) は元々物理学で発展した概念で、スケールの異なる階層において系の振る舞いがどのように変化するかを記述する手法です。微視的な自由度を積分消去 (marginalize out) することで、より粗視化された有効理論を導出します。\n\n\n\n\n\n\nNote物理学における繰り込み群\n\n\n\n\n\n物理学では、繰り込み群は複雑な相互作用系を理解するための強力な手法として確立されています。\n基本的なアイデア:\n\n微視的スケールから巨視的スケールへと観測スケールを変化させる\n各スケールで有効な相互作用パラメータがどのように「流れる」かを追跡\n微視的自由度を積分消去し、粗視化された記述を得る\n\n深層学習との類似:\n\n物理学: 微視的スケール → 巨視的スケール\n深層学習: 入力層 → 隠れ層 → 出力層\n\nどちらも、細かい情報を段階的に粗視化し、より高次の抽象的な記述へと変換していくプロセスです。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#深層学習における-rg-flow",
    "href": "ja/pdlt/02-rg-flow.html#深層学習における-rg-flow",
    "title": "RG Flow",
    "section": "",
    "text": "深層ニューラルネットワークでは、RG Flow は以下のプロセスを形式化します。\n層ごとの変換:\n┌──────────────────────────────────────────────────┐\n│  Layer l -&gt; Layer l+1                            │\n│                                                  │\n│  Preactivations z^(l) -&gt; z^(l+1)                 │\n│                                                  │\n│  Distribution p(z^(l)) -&gt; p(z^(l+1))             │\n└──────────────────────────────────────────────────┘\n各層において、前層の preactivation（活性化前の値）の分布を積分消去し、次層の preactivation の分布を導出します。このプロセスは、物理学における繰り込み変換に対応します。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#preactivation-の-rg-flow",
    "href": "ja/pdlt/02-rg-flow.html#preactivation-の-rg-flow",
    "title": "RG Flow",
    "section": "",
    "text": "初期化時、第1層の preactivation はガウス分布に従います (Chapter 4.1)。\n定義:\n\\[\nz^{(1)}_{i;\\alpha} = b^{(1)}_i + \\sum_{j=1}^{n_0} W^{(1)}_{ij} x_{j;\\alpha}\n\\]\nここで、バイアス \\(b^{(1)}\\) と重み \\(W^{(1)}\\) は独立なガウス分布から初期化されます。\n分布:\n\\[\np(z^{(1)} | \\mathcal{D}) = \\frac{1}{Z} e^{-S(z^{(1)})}\n\\]\n作用 (action) は2次形式:\n\\[\nS(z^{(1)}) = \\frac{1}{2} \\sum_{i=1}^{n_1} \\sum_{\\alpha_1, \\alpha_2 \\in \\mathcal{D}} G^{\\alpha_1 \\alpha_2}_{(1)} z^{(1)}_{i;\\alpha_1} z^{(1)}_{i;\\alpha_2}\n\\]\n\\(G^{(1)}_{\\alpha_1 \\alpha_2}\\) は第1層の計量 (metric) で、サンプル間の相関を表します。\n重要な性質:\n\nすべての奇数次相関関数がゼロ\n4次以上の連結相関関数がゼロ（Wick の定理が成立）\n完全にガウス的\n\n\n\n\n第2層では、非ガウス性 (non-Gaussianity) が出現します (Chapter 4.2)。活性化関数が非線形であるため、4次連結相関関数が非ゼロになります。\n分布の変化:\n\\[\np(z^{(2)} | \\mathcal{D}) \\approx \\frac{1}{Z} \\exp\\left( -S_2(z^{(2)}) - S_4(z^{(2)}) + O(1/n^2) \\right)\n\\]\nここで:\n\n\\(S_2\\): 2次作用（ガウス部分）\n\\(S_4\\): 4次作用（非ガウス補正、\\(O(1/n)\\) で抑制）\n\nLarge-\\(n\\) expansion:\nネットワーク幅 \\(n\\) が大きいとき、非ガウス性は \\(1/n\\) で抑制されます。この性質により、有限幅ネットワークの振る舞いを体系的に展開できます。\n\n\n\n第3層以降、非ガウス性は層ごとに蓄積していきます (Chapter 4.3)。\n再帰的構造:\n各層 \\(\\ell\\) から層 \\(\\ell+1\\) への遷移は、以下の2つの寄与を含みます。\n\n第 \\(\\ell\\) 層から継承された非ガウス性\n第 \\(\\ell\\) 層から第 \\(\\ell+1\\) 層への遷移で新たに生成された非ガウス性\n\n深さと幅の比:\n非ガウス性の蓄積は、深さ \\(L\\) と幅 \\(n\\) の比 \\(L/n\\) によって特徴づけられます。ネットワークが well-behaved であるためには、この比が適切に制御される必要があります (depth-to-width ratio)。\n流れの図示:\ngraph LR\n    A[Layer 1&lt;br/&gt;Gaussian] --&gt; B[Layer 2&lt;br/&gt;Weak non-Gaussian]\n    B --&gt; C[Layer 3&lt;br/&gt;Moderate non-Gaussian]\n    C --&gt; D[Layer L&lt;br/&gt;Accumulated non-Gaussian]\n\n    style A fill:#e1f5ff\n    style B fill:#ffe1f5\n    style D fill:#ffe1e1",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#ntk-の-rg-flow",
    "href": "ja/pdlt/02-rg-flow.html#ntk-の-rg-flow",
    "title": "RG Flow",
    "section": "",
    "text": "Neural Tangent Kernel (NTK) もまた、層ごとに「流れ」ます (Chapter 8)。NTK は勾配降下法による学習のダイナミクスを支配する量です。\n\n\n定義:\n\\[\n\\hat{H}^{(\\ell)}_{i_1 i_2; \\alpha_1 \\alpha_2} \\equiv \\sum_{\\mu, \\nu} \\lambda^{\\mu\\nu} \\frac{dz^{(\\ell)}_{i_1;\\alpha_1}}{d\\theta^\\mu} \\frac{dz^{(\\ell)}_{i_2;\\alpha_2}}{d\\theta^\\nu}\n\\]\nここで、\\(\\lambda^{\\mu\\nu}\\) は学習率テンソル、\\(\\theta^\\mu\\) はモデルパラメータです。\n\n\n\n第 \\(\\ell\\) 層の NTK から第 \\(\\ell+1\\) 層の NTK への遷移は、以下の再帰式で記述されます (Chapter 8.0)。\nForward equation:\n\\[\n\\hat{H}^{(\\ell+1)}_{i_1 i_2; \\alpha_1 \\alpha_2} = \\delta_{i_1 i_2} \\left( \\lambda^{(\\ell+1)}_b + \\lambda^{(\\ell+1)}_W V^{(\\ell)}_{\\alpha_1 \\alpha_2} \\right) + \\sum_{j_1, j_2} \\frac{dz^{(\\ell+1)}_{i_1;\\alpha_1}}{dz^{(\\ell)}_{j_1;\\alpha_1}} \\frac{dz^{(\\ell+1)}_{i_2;\\alpha_2}}{dz^{(\\ell)}_{j_2;\\alpha_2}} \\hat{H}^{(\\ell)}_{j_1 j_2; \\alpha_1 \\alpha_2}\n\\]\nこの式は:\n\n第1項: 第 \\(\\ell+1\\) 層のパラメータからの直接寄与\n第2項: 第 \\(\\ell\\) 層以前からの寄与（連鎖律による）\n\n\n\n\n初期化時、NTK は確率的な量です (Chapter 8.1-8.3)。\n第1層: NTK は決定論的（ゆらぎなし）\n第2層: NTK にゆらぎが出現\n深い層: NTK のゆらぎが蓄積\nPreactivation の RG Flow と完全に並行して、NTK の統計も再帰的に計算できます。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#rg-flow-の意義",
    "href": "ja/pdlt/02-rg-flow.html#rg-flow-の意義",
    "title": "RG Flow",
    "section": "",
    "text": "RG Flow の定式化により、以下が可能になります。\n理論的洞察:\n\nネットワークの深さ \\(L\\) と幅 \\(n\\) の役割の明確化\n臨界性 (criticality) の概念の拡張\n有限幅効果の体系的な理解\n\n実用的示唆:\n\n初期化スキームの設計原理\n深さと幅のバランスの指針\n勾配消失・爆発問題の理解\n\n\n\n\n\n\n\nTipRG Flow と表現学習\n\n\n\nRG Flow は、深層ニューラルネットワークがデータの表現を段階的に抽象化していくプロセスを、厳密な数学的枠組みで記述します。\n直感的理解:\n\n入力層: 細粒度の生データ\n隠れ層: 中間的な特徴表現（段階的に抽象化）\n出力層: 粗視化された最終表現\n\nこのプロセスは、物理学における微視的記述から巨視的記述への移行と本質的に同じ構造を持っています。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/02-rg-flow.html#まとめ",
    "href": "ja/pdlt/02-rg-flow.html#まとめ",
    "title": "RG Flow",
    "section": "",
    "text": "Renormalization Group Flow は、深層学習の理論的理解において中心的な役割を果たします。\n主要な結果:\n\n第1層はガウス分布、第2層以降で非ガウス性が出現・蓄積\n非ガウス性は \\(1/n\\) 展開で体系的に制御可能\nPreactivation と NTK の両方が層ごとに「流れ」る\n深さと幅の比 \\(L/n\\) が重要なパラメータ\n\n次のステップ:\n\nRG Flow の方程式を解き、臨界性を解析 (Chapter 5)\nNTK の統計を用いて学習ダイナミクスを理解 (Chapter 9, 10)\n表現学習の出現条件を明らかにする (Chapter 11)\n\nRG Flow は、深層学習の「有効理論」(effective theory) を構築するための基盤となる概念です。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "RG Flow"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html",
    "href": "ja/pdlt/00-overview.html",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "本まとめは、Daniel A. Roberts と Sho Yaida による “The Principles of Deep Learning Theory” (PDLT) の主要な概念と理論的枠組みを解説するものである。\n原著 PDLT は、物理学の有効理論（Effective Theory）のアプローチを深層学習に適用し、深層ニューラルネットワークの動作原理を理論的に解明することを目指した画期的な教科書である。著者らは、統計力学や場の理論で用いられる手法（繰り込み群、臨界性解析、ガウス過程など）を駆使して、深層学習の「なぜ動くのか」を first principles から説明している。\n原著論文: arXiv:2106.10165\n原著 Web サイト: deeplearningtheory.com\n\n\n\n\n\n原著は、深層学習を理解するために、物理学の effective theory framework を採用している。これは、多数の素子（ニューロン）から構成される系の巨視的な振る舞いを、統計的性質から理解するアプローチである。\n\n詳細: Effective Theory Approach\n\n熱力学が蒸気機関の macroscopic な性質を記述し、統計力学がその microscopic な起源を説明したように、原著は深層ニューラルネットワークの macroscopic な計算能力を、microscopic なパラメータの統計的性質から導出する。\n\n\n\n原著は以下の知識を持つ読者を対象としている：\n\n線形代数\n多変量微積分\n確率論の基礎\n\n物理学のバックグラウンドは必須ではなく、必要な数学的道具は原著 Chapter 1 “Pretraining” で丁寧に説明されている。\n\n\n\n原著の重要な特徴は、理論研究でよく扱われる理想化されたモデル（single-hidden-layer networks や完全な無限幅極限）を出発点としつつも、最終的には実際に使われる深層ニューラルネットワークの理論的理解を目指している点である。\n特に、deep multilayer perceptrons（深層多層パーセプトロン）を中心に、有限幅（finite width）のネットワークの挙動を解析する。\n\n\n\n\n\n\nNote他の深層学習理論書との違い\n\n\n\n\n\n従来の深層学習理論書は、以下のいずれかに偏る傾向がある：\n\n実践書: アルゴリズムと実装に焦点を当てるが、理論的根拠は限定的\n数学的厳密性重視: 厳密な証明を重視するが、単純化された設定（single-hidden-layer など）に限定\n漸近解析: 無限幅極限など数学的に扱いやすい極限に焦点\n\n原著の独自性:\n\n実際に使われる深層ネットワーク（finite depth, finite width）の理論\n物理学の有効理論による直感的かつ定量的な理解\n計算の詳細を省略せず、読者が自ら拡張できる枠組みを提供\n\n\n\n\n\n\n\n\n本まとめでは、原著で展開される以下の主要な概念を解説する。\n\n\n原著では、深層ニューラルネットワークの各層を通る信号の変化を、繰り込み群フロー（Renormalization Group Flow, RG Flow）として捉える。\n\n詳細: RG Flow\n\n入力層から出力層に向かって、preactivation の分布がどのように変化していくかを追跡し、その effective theory を構築する。\n\n\n\nニューラルネットワークの初期化における臨界性（Criticality）は、訓練の成功に重要な役割を果たす。\n\n詳細: Criticality\n\n適切な初期化により、信号が層を通じて指数的に増大（exploding）または減衰（vanishing）することなく、臨界点（critical point）に保たれる。\n\n\n\n\n\n\nTip実践への応用: 初期化戦略\n\n\n\n\n\n原著の臨界性理論は、以下の実践的初期化手法の理論的基盤を提供する：\n\nHe initialization (Kaiming He, 2015): ReLU 系活性化関数に最適（Scale-Invariant Class）\nXavier initialization (Glorot & Bengio, 2010): tanh 系活性化関数に適合（K* = 0 Class）\n活性化関数の選択: 普遍性クラスに応じた初期化パラメータの調整\n\nこれらの経験則が「なぜ動くのか」を、原著は臨界性の観点から理論的に説明している。\n\n\n\n\n\n\n無限幅極限において、ニューラルネットワークの訓練はカーネル法と等価になる。このカーネルを Neural Tangent Kernel (NTK) と呼ぶ。\n\n詳細: Neural Tangent Kernel\n\nNTK は訓練中に固定され、ネットワークは線形モデルとして振る舞う。しかし、有限幅では NTK が変化し、表現学習（Representation Learning）が可能になる。\n\n\n\n原著における重要な区別：\n\n無限幅（Infinite Width）: ニューロン数 \\(n \\to \\infty\\) の極限\n\nガウス過程に収束\nNTK が固定され、線形モデルに帰着\n表現学習が起こらない\n\n有限幅（Finite Width）: 実際のニューラルネットワーク\n\nNTK が訓練中に変化\n非線形性が残り、表現学習が可能\nHebbian learning により、層間の相関が形成される\n\n\n\n詳細: Infinite Width Limit\n\n\n\n\n有限幅のニューラルネットワークでは、訓練によって内部表現が変化する。これが表現学習（Representation Learning）であり、深層学習の本質的な能力である。\n\n詳細: Representation Learning\n\n無限幅極限では表現学習が消失するため、実際の深層学習を理解するには有限幅の解析が不可欠である。\n\n\n\n異なる活性化関数（ReLU, tanh, GELU など）は、臨界性の解析において普遍性クラス（Universality Classes）に分類される。\n\n詳細: Universality Classes\n\n主要な普遍性クラス：\n\nScale-Invariant Universality Class: ReLU, leaky ReLU など\nK* = 0 Universality Class: tanh, sin など\nHalf-Stable Universality Classes: SWISH, GELU など\n\n同じ普遍性クラスに属する活性化関数は、深層極限において同様の統計的性質を示す。\n\n\n\n\n原著は以下のように構成されている。本まとめでは、これらの章から主要な概念を抽出して解説する。\n\n\nChapter 0: Initialization - Effective Theory Approach の導入 - 深層学習における理論の必要性\nChapter 1: Pretraining - Gaussian integrals - 確率論と統計の数学的基礎 - Nearly-Gaussian distributions\nChapter 2: Neural Networks - 関数近似としてのニューラルネットワーク - 活性化関数 - アンサンブル\nChapter 3: Effective Theory of Deep Linear Networks at Initialization - Deep linear networks の解析 - Criticality の導入 - Fluctuations と Chaos\nChapter 4: RG Flow of Preactivations - 第1層: Gaussian 分布 - 第2層: Non-Gaussianity の発生 - 深層: Non-Gaussianity の蓄積 - Marginalization rules\nChapter 5: Effective Theory of Preactivations at Initialization - Kernel の criticality 解析 - 普遍性クラスの分類 - Fluctuations の解析\n\n\n\nChapter 6: Bayesian Learning - ベイズ推論とニューラルネットワーク - 無限幅での推論（表現学習なし） - 有限幅での推論（Hebbian learning、表現学習あり）\nChapter 7: Gradient-Based Learning - 教師あり学習 - 勾配降下法と関数近似\nChapter 8: RG Flow of the Neural Tangent Kernel - NTK の forward equation - 層ごとの NTK の発展 - NTK の平均と分散\nChapter 9: Effective Theory of the NTK at Initialization - NTK の criticality 解析 - 普遍性クラスごとの NTK の性質\nChapter 10: Kernel Learning - 無限幅ネットワークの線形モデルとしての性質 - Generalization と bias-variance tradeoff - Kernel methods との関連\nChapter 11: Representation Learning - NTK の微分 (dNTK) - 有限幅ネットワークの非線形性 - Nearly-kernel methods\nChapter ∞: The End of Training - 有限幅での訓練 - 多数ステップの勾配降下 - 有限幅での予測\n\n\n\nAppendix A: Information in Deep Learning - Entropy と mutual information - 無限幅での情報理論的解析 - 有限幅での最適アスペクト比\nAppendix B: Residual Learning - Residual networks (ResNets) への拡張 - Residual MLP の criticality 解析\n\n\n\n\n原著が導出した主要な理論的結果を以下にまとめる。\n\n\n原著は、適切な初期化（例: He initialization, Xavier initialization）がネットワークを臨界点に保ち、gradient の exploding/vanishing を防ぐことを理論的に証明した。これにより、経験的に知られていた初期化手法の理論的根拠が明らかになった。\n\n\n\n原著は、無限幅極限においてニューラルネットワークが Gaussian process に収束し、NTK が訓練中に固定されることを示した。これは線形モデルに帰着し、表現学習が起こらない。\nこの結果は、深層学習の本質を理解するには有限幅の理論が不可欠であることを示す重要な洞察である。\n\n\n\n原著は、有限幅では NTK が訓練中に変化し（dNTK \\(\\neq 0\\)）、これが表現学習を可能にすることを明らかにした。この変化は \\(1/n\\) のオーダーで起こり（\\(n\\) はニューロン数）、深さとともに蓄積される。\nこの理論により、深層学習が単なるカーネル法を超える能力を持つ理由が説明される。\n\n\n\n原著の普遍性クラスの解析により、異なる活性化関数がどのように振る舞うかを予測できるようになった。例えば：\n\nReLU (Scale-Invariant Class): 深層ネットワークでも安定\ntanh (K* = 0 Class): 深層で情報が減衰しやすい\n\nこの知見は、アーキテクチャ設計の理論的指針を提供する。\n\n\n\n\n原著 PDLT は、深層学習を「経験的に動くがなぜかわからない」状態から、「物理学的に理解できる」状態へと引き上げることを目指した画期的な著作である。\n原著が採用する物理学の effective theory アプローチは、以下の利点をもたらす：\n\n予測可能性: 初期化やアーキテクチャの選択が訓練にどう影響するかを理論的に予測\n普遍性: 特定のモデルに依存しない、一般的な原理の発見\n拡張性: MLPs 以外のアーキテクチャ（ResNets, Transformers など）への拡張可能性\n\n原著は、深層学習の理論研究における新しいパラダイムを提示し、今後の理論的・実践的進展の基盤となることが期待される。\n\n\n\n\n\n\nImportant深層学習理論の現状と課題\n\n\n\n\n\n原著が解決した問題:\n\n深層ニューラルネットワークの初期化時の統計的性質の完全な特徴づけ\n無限幅と有限幅の理論的区別の明確化\n表現学習の理論的基盤の提供\n\n残された課題:\n\nアーキテクチャの拡張: Transformers, Diffusion Models などへの理論的拡張\n訓練ダイナミクスの完全な理解: 有限ステップでの収束性、学習率の最適化\n汎化の理論: なぜ過剰パラメータ化されたネットワークが汎化するのか\n実データへの適用: 理論と実験の定量的比較の更なる精緻化\n\n原著 PDLT はこれらの課題に取り組むための理論的枠組みを提供している。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#概要",
    "href": "ja/pdlt/00-overview.html#概要",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "本まとめは、Daniel A. Roberts と Sho Yaida による “The Principles of Deep Learning Theory” (PDLT) の主要な概念と理論的枠組みを解説するものである。\n原著 PDLT は、物理学の有効理論（Effective Theory）のアプローチを深層学習に適用し、深層ニューラルネットワークの動作原理を理論的に解明することを目指した画期的な教科書である。著者らは、統計力学や場の理論で用いられる手法（繰り込み群、臨界性解析、ガウス過程など）を駆使して、深層学習の「なぜ動くのか」を first principles から説明している。\n原著論文: arXiv:2106.10165\n原著 Web サイト: deeplearningtheory.com",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#原著の特徴",
    "href": "ja/pdlt/00-overview.html#原著の特徴",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "原著は、深層学習を理解するために、物理学の effective theory framework を採用している。これは、多数の素子（ニューロン）から構成される系の巨視的な振る舞いを、統計的性質から理解するアプローチである。\n\n詳細: Effective Theory Approach\n\n熱力学が蒸気機関の macroscopic な性質を記述し、統計力学がその microscopic な起源を説明したように、原著は深層ニューラルネットワークの macroscopic な計算能力を、microscopic なパラメータの統計的性質から導出する。\n\n\n\n原著は以下の知識を持つ読者を対象としている：\n\n線形代数\n多変量微積分\n確率論の基礎\n\n物理学のバックグラウンドは必須ではなく、必要な数学的道具は原著 Chapter 1 “Pretraining” で丁寧に説明されている。\n\n\n\n原著の重要な特徴は、理論研究でよく扱われる理想化されたモデル（single-hidden-layer networks や完全な無限幅極限）を出発点としつつも、最終的には実際に使われる深層ニューラルネットワークの理論的理解を目指している点である。\n特に、deep multilayer perceptrons（深層多層パーセプトロン）を中心に、有限幅（finite width）のネットワークの挙動を解析する。\n\n\n\n\n\n\nNote他の深層学習理論書との違い\n\n\n\n\n\n従来の深層学習理論書は、以下のいずれかに偏る傾向がある：\n\n実践書: アルゴリズムと実装に焦点を当てるが、理論的根拠は限定的\n数学的厳密性重視: 厳密な証明を重視するが、単純化された設定（single-hidden-layer など）に限定\n漸近解析: 無限幅極限など数学的に扱いやすい極限に焦点\n\n原著の独自性:\n\n実際に使われる深層ネットワーク（finite depth, finite width）の理論\n物理学の有効理論による直感的かつ定量的な理解\n計算の詳細を省略せず、読者が自ら拡張できる枠組みを提供",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#原著の主要な概念",
    "href": "ja/pdlt/00-overview.html#原著の主要な概念",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "本まとめでは、原著で展開される以下の主要な概念を解説する。\n\n\n原著では、深層ニューラルネットワークの各層を通る信号の変化を、繰り込み群フロー（Renormalization Group Flow, RG Flow）として捉える。\n\n詳細: RG Flow\n\n入力層から出力層に向かって、preactivation の分布がどのように変化していくかを追跡し、その effective theory を構築する。\n\n\n\nニューラルネットワークの初期化における臨界性（Criticality）は、訓練の成功に重要な役割を果たす。\n\n詳細: Criticality\n\n適切な初期化により、信号が層を通じて指数的に増大（exploding）または減衰（vanishing）することなく、臨界点（critical point）に保たれる。\n\n\n\n\n\n\nTip実践への応用: 初期化戦略\n\n\n\n\n\n原著の臨界性理論は、以下の実践的初期化手法の理論的基盤を提供する：\n\nHe initialization (Kaiming He, 2015): ReLU 系活性化関数に最適（Scale-Invariant Class）\nXavier initialization (Glorot & Bengio, 2010): tanh 系活性化関数に適合（K* = 0 Class）\n活性化関数の選択: 普遍性クラスに応じた初期化パラメータの調整\n\nこれらの経験則が「なぜ動くのか」を、原著は臨界性の観点から理論的に説明している。\n\n\n\n\n\n\n無限幅極限において、ニューラルネットワークの訓練はカーネル法と等価になる。このカーネルを Neural Tangent Kernel (NTK) と呼ぶ。\n\n詳細: Neural Tangent Kernel\n\nNTK は訓練中に固定され、ネットワークは線形モデルとして振る舞う。しかし、有限幅では NTK が変化し、表現学習（Representation Learning）が可能になる。\n\n\n\n原著における重要な区別：\n\n無限幅（Infinite Width）: ニューロン数 \\(n \\to \\infty\\) の極限\n\nガウス過程に収束\nNTK が固定され、線形モデルに帰着\n表現学習が起こらない\n\n有限幅（Finite Width）: 実際のニューラルネットワーク\n\nNTK が訓練中に変化\n非線形性が残り、表現学習が可能\nHebbian learning により、層間の相関が形成される\n\n\n\n詳細: Infinite Width Limit\n\n\n\n\n有限幅のニューラルネットワークでは、訓練によって内部表現が変化する。これが表現学習（Representation Learning）であり、深層学習の本質的な能力である。\n\n詳細: Representation Learning\n\n無限幅極限では表現学習が消失するため、実際の深層学習を理解するには有限幅の解析が不可欠である。\n\n\n\n異なる活性化関数（ReLU, tanh, GELU など）は、臨界性の解析において普遍性クラス（Universality Classes）に分類される。\n\n詳細: Universality Classes\n\n主要な普遍性クラス：\n\nScale-Invariant Universality Class: ReLU, leaky ReLU など\nK* = 0 Universality Class: tanh, sin など\nHalf-Stable Universality Classes: SWISH, GELU など\n\n同じ普遍性クラスに属する活性化関数は、深層極限において同様の統計的性質を示す。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#原著の構成",
    "href": "ja/pdlt/00-overview.html#原著の構成",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "原著は以下のように構成されている。本まとめでは、これらの章から主要な概念を抽出して解説する。\n\n\nChapter 0: Initialization - Effective Theory Approach の導入 - 深層学習における理論の必要性\nChapter 1: Pretraining - Gaussian integrals - 確率論と統計の数学的基礎 - Nearly-Gaussian distributions\nChapter 2: Neural Networks - 関数近似としてのニューラルネットワーク - 活性化関数 - アンサンブル\nChapter 3: Effective Theory of Deep Linear Networks at Initialization - Deep linear networks の解析 - Criticality の導入 - Fluctuations と Chaos\nChapter 4: RG Flow of Preactivations - 第1層: Gaussian 分布 - 第2層: Non-Gaussianity の発生 - 深層: Non-Gaussianity の蓄積 - Marginalization rules\nChapter 5: Effective Theory of Preactivations at Initialization - Kernel の criticality 解析 - 普遍性クラスの分類 - Fluctuations の解析\n\n\n\nChapter 6: Bayesian Learning - ベイズ推論とニューラルネットワーク - 無限幅での推論（表現学習なし） - 有限幅での推論（Hebbian learning、表現学習あり）\nChapter 7: Gradient-Based Learning - 教師あり学習 - 勾配降下法と関数近似\nChapter 8: RG Flow of the Neural Tangent Kernel - NTK の forward equation - 層ごとの NTK の発展 - NTK の平均と分散\nChapter 9: Effective Theory of the NTK at Initialization - NTK の criticality 解析 - 普遍性クラスごとの NTK の性質\nChapter 10: Kernel Learning - 無限幅ネットワークの線形モデルとしての性質 - Generalization と bias-variance tradeoff - Kernel methods との関連\nChapter 11: Representation Learning - NTK の微分 (dNTK) - 有限幅ネットワークの非線形性 - Nearly-kernel methods\nChapter ∞: The End of Training - 有限幅での訓練 - 多数ステップの勾配降下 - 有限幅での予測\n\n\n\nAppendix A: Information in Deep Learning - Entropy と mutual information - 無限幅での情報理論的解析 - 有限幅での最適アスペクト比\nAppendix B: Residual Learning - Residual networks (ResNets) への拡張 - Residual MLP の criticality 解析",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#原著の主要な結果",
    "href": "ja/pdlt/00-overview.html#原著の主要な結果",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "原著が導出した主要な理論的結果を以下にまとめる。\n\n\n原著は、適切な初期化（例: He initialization, Xavier initialization）がネットワークを臨界点に保ち、gradient の exploding/vanishing を防ぐことを理論的に証明した。これにより、経験的に知られていた初期化手法の理論的根拠が明らかになった。\n\n\n\n原著は、無限幅極限においてニューラルネットワークが Gaussian process に収束し、NTK が訓練中に固定されることを示した。これは線形モデルに帰着し、表現学習が起こらない。\nこの結果は、深層学習の本質を理解するには有限幅の理論が不可欠であることを示す重要な洞察である。\n\n\n\n原著は、有限幅では NTK が訓練中に変化し（dNTK \\(\\neq 0\\)）、これが表現学習を可能にすることを明らかにした。この変化は \\(1/n\\) のオーダーで起こり（\\(n\\) はニューロン数）、深さとともに蓄積される。\nこの理論により、深層学習が単なるカーネル法を超える能力を持つ理由が説明される。\n\n\n\n原著の普遍性クラスの解析により、異なる活性化関数がどのように振る舞うかを予測できるようになった。例えば：\n\nReLU (Scale-Invariant Class): 深層ネットワークでも安定\ntanh (K* = 0 Class): 深層で情報が減衰しやすい\n\nこの知見は、アーキテクチャ設計の理論的指針を提供する。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/pdlt/00-overview.html#原著の意義",
    "href": "ja/pdlt/00-overview.html#原著の意義",
    "title": "The Principles of Deep Learning Theory まとめ",
    "section": "",
    "text": "原著 PDLT は、深層学習を「経験的に動くがなぜかわからない」状態から、「物理学的に理解できる」状態へと引き上げることを目指した画期的な著作である。\n原著が採用する物理学の effective theory アプローチは、以下の利点をもたらす：\n\n予測可能性: 初期化やアーキテクチャの選択が訓練にどう影響するかを理論的に予測\n普遍性: 特定のモデルに依存しない、一般的な原理の発見\n拡張性: MLPs 以外のアーキテクチャ（ResNets, Transformers など）への拡張可能性\n\n原著は、深層学習の理論研究における新しいパラダイムを提示し、今後の理論的・実践的進展の基盤となることが期待される。\n\n\n\n\n\n\nImportant深層学習理論の現状と課題\n\n\n\n\n\n原著が解決した問題:\n\n深層ニューラルネットワークの初期化時の統計的性質の完全な特徴づけ\n無限幅と有限幅の理論的区別の明確化\n表現学習の理論的基盤の提供\n\n残された課題:\n\nアーキテクチャの拡張: Transformers, Diffusion Models などへの理論的拡張\n訓練ダイナミクスの完全な理解: 有限ステップでの収束性、学習率の最適化\n汎化の理論: なぜ過剰パラメータ化されたネットワークが汎化するのか\n実データへの適用: 理論と実験の定量的比較の更なる精緻化\n\n原著 PDLT はこれらの課題に取り組むための理論的枠組みを提供している。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html",
    "href": "ja/olmo-3/10-olmorl-grpo.html",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、思考型モデル（reasoning model）の強化学習訓練を効率化するために開発されたフレームワークです。Group Relative Policy Optimization（GRPO）をベースとし、Reinforcement Learning with Verifiable Rewards（RLVR）の手法を採用しています。\nPost-training の第3段階として、数学、コーディング、指示追従、一般的なチャットの複数ドメインにわたり、検証可能な報酬と LM-judge 報酬を組み合わせた強化学習を実施します。\n\n\nOlmoRL は、長い推論トレースを伴う強化学習の課題に対処するため、アルゴリズムとエンジニアリングインフラを密接に統合したシステムです。従来の数学とコードに限定されていた RLVR を、より幅広い検証可能なタスクへと拡張しています。\n主な特徴:\n\nアルゴリズムの改善: GRPO をベースに、DAPO や Dr GRPO などの最新の改善を統合\n大規模データセット: Dolci-Think-RL（約 100K プロンプト、4 ドメイン）\n効率的なインフラ: 長いシーケンス（最大 32K トークン）を効率的に処理する分散訓練システム\n4倍の高速化: OLMo 2 の RL インフラと比較して約 4 倍の高速化を達成\n\n\n\n\nOlmoRL の強化学習段階は、GRPO（Shao et al., 2024）をベースに構築され、DAPO（Yu et al., 2025）や Dr GRPO（Liu et al., 2025b）などの最新のアルゴリズム改善を統合しています。\n\n\nOlmoRL は、Vanilla GRPO に対して以下の改善を実施しています：\n1. Zero gradient signal filtering:\n\n報酬がすべて同一のグループ（advantage の標準偏差がゼロ）を除外\nゼロ勾配のサンプルでの訓練を回避（DAPO と同様）\n\n2. Active sampling:\n\nZero gradient filtering にもかかわらず、一貫したバッチサイズを維持\n動的サンプリングの改良版を実装（詳細は §4.4.3）\n\n3. Token-level loss:\n\nサンプルごとではなく、バッチ全体のトークン数で損失を正規化\n長さバイアスを回避\n\n4. No KL loss:\n\nKL 損失を削除（GLM-4.5、DAPO、Dr GRPO などと同様の実践）\nより制限の少ないポリシー更新を可能にし、過最適化や訓練の不安定化を引き起こさない\n\n5. Clip higher:\n\n上限クリッピング項を下限よりわずかに高く設定\nトークンに対するより大きな更新を可能にする（Yu et al., 2025）\n\n6. Truncated importance sampling:\n\n推論エンジンと訓練エンジン間の対数確率の差を調整\n切り詰められた重要度サンプリング比を損失に掛ける（Yao et al., 2025）\n\n7. No standard deviation normalization:\n\nAdvantage 計算時に標準偏差で正規化しない（Liu et al., 2025b）\n難易度バイアスを除去（報酬の標準偏差が低い問題の advantage が過度に増加するのを防止）\n\n\n\n\n最終的な目的関数は、token-level loss、truncated importance sampling、clip-higher、および advantage 計算における標準偏差正規化なしを含みます：\n\\[\nJ(\\theta) = \\frac{1}{\\sum_{i=1}^{G} |y_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|y_i|} \\min\\left(\\frac{\\pi(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}{\\pi_{\\text{vllm}}(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}, \\rho\\right) \\times \\min(r_{i,t} A_{i,t}, \\text{clip}(r_{i,t}, 1 - \\varepsilon_{\\text{low}}, 1 + \\varepsilon_{\\text{high}}) A_{i,t})\n\\]\nここで：\n\n\\(r_{i,t} = \\frac{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta)}{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta_{\\text{old}})}\\)\n\\(\\varepsilon_{\\text{low}}\\) と \\(\\varepsilon_{\\text{high}}\\) はクリッピングハイパーパラメータ\n\\(\\rho\\) は truncated importance sampling の上限値\nAdvantage \\(A_{i,t}\\) は、グループ \\(G\\) 内での相対報酬に基づいて計算：\n\n\\[\nA_{i,t} = r(x, y_i) - \\text{mean}(\\{r(x, y_i)\\}_{i=1}^{G})\n\\]\n\n\n\n\n\n\nNoteGRPO と PPO の比較\n\n\n\n\n\nGRPO は PPO の変種であり、主な違いは報酬の正規化方法にあります。\nPPO:\n\n報酬を全データセットまたはバッチ全体で正規化\nグローバルなベースラインを使用\n\nGRPO:\n\n同じプロンプトから生成された応答グループ内で報酬を正規化\nグループベースの相対的な品質評価により、学習の安定性が向上\n思考型モデルのように出力長が大きく異なる場合に特に有効\n\nOlmoRL の改善:\n\nVanilla GRPO に対して 7 つの主要な改善を実施\nZero gradient filtering と active sampling により、訓練の効率と安定性を大幅に向上\n\n\n\n\n\n\n\nOlmoRL は、OLMo 2 の数学ドメインを超えて、一般的なドメインへと検証可能な報酬を拡張しています。各ドメインに対して異なるカスタム検証器を使用します：\nMath（数学）:\n\nルールベースの検証器\n基本的な正規化を実行し、SymPy を使用して参照回答と比較\n参照回答と同じであれば 1、そうでなければ 0 を返す\n\nCode（コーディング）:\n\nテストケースベースの検証器\n応答に対して一連のテストケースを実行\n\n通過したテストケースの割合を報酬とする、または (b) すべてのテストケースを通過した場合に 1、そうでなければ 0 を返す\n\n\nInstruction-following（指示追従）:\n\nプロンプトからの一連の制約への準拠をチェックする関数セットを通して応答を渡す\nすべての制約が満たされている場合に 1、そうでなければ 0 を返す\n\nChat—reference（参照付きチャット）:\n\nground-truth 応答がある場合、LM judge を使用してモデルの応答を参照回答と比較\n応答の品質に基づいて [0, 1] のスコアを付与\n\nChat—open-ended（オープンエンドチャット）:\n\n参照回答なしで、LM judge を使用して応答の品質に基づいて [0, 1] のスコアを付与\n\n\n\n\n\nDolci-Think-RL は、4 つのドメイン（数学、コーディング、指示追従、一般的なチャット）にわたる約 100K のサンプルからなる大規模で多様なデータセットです。多様な推論タスクでの堅牢な RL を支援しながら、一般的な有用性を維持します。\n\n\n\n\n\n\n\n\n\n\n\nカテゴリ\nデータセット\nThink RL 用プロンプト数\nInstruct RL 用プロンプト数\n\n\n\n\nPrecise IF\nIF-RLVR\n30,186\n38,000\n\n\nMath\nOpen-Reasoner-Zero\n3,000\n14,000\n\n\n\nDAPO-Math\n2,584\n7,000\n\n\n\nAceReason-Math\n6,602\n-\n\n\n\nPolaris-Dataset\n-\n14,000\n\n\n\nKlearReasoner-MathSub\n3,000\n9,000\n\n\n\nOMEGA-train\n15,000\n20,000\n\n\nCoding\nAceCoder\n9,767\n20,000\n\n\n\nKlearReasoner-Code\n8,040\n-\n\n\n\nNemotron Post-training Code\n2,303\n-\n\n\n\nSYNTHETIC-2\n3,000\n-\n\n\nGeneral Chat\nTulu 3 SFT\n7,129\n18,955\n\n\n\nWildchat-4.8M\n7,129\n18,761\n\n\n\nMulti-Subject RLVR\n7,129\n12,234\n\n\n合計\n\n104,869\n171,950\n\n\n\n\n\n\nStep 1: プロンプトの調達:\n各ドメインから高品質なプロンプトを収集し、キュレーションします。\n\nMath: Open-Reasoner-Zero、DAPO-Math、AceReason-Math、KlearReasoner-MathSub、OMEGA など\nCoding: AceCoder、Klear-Reasoner Code、Nemotron Post-training Code、SYNTHETIC-2 など\nInstruction-following: IF-RLVR（最大 5 つの制約、IFEval と IFBench-Train からサンプリング）\nGeneral chat: Tülu 3 SFT、WildChat-4.8M、Multi-subject-RLVR\n\nStep 2: オフライン難易度フィルタリング:\n\nモデルの初期チェックポイントから各プロンプトに対して 8 つのロールアウトを生成\nモデルが容易に解決するサンプル（pass rate &gt; 62.5%）を除外\n温度 1.0、top-p 1.0 でサンプリング（RL 訓練時と一致）\n\nStep 3: データミキシング:\n\nドメイン固有の実験を実施し、最初の 500-1000 ステップで下流評価のトレンドを観察\n高品質なデータセットをアップウェイト\n各ドメインでほぼ同等のデータ量を使用（数学と指示追従にわずかに重点）\nOMEGA から特定のサブタスクをダウンサンプル\n\n\n\n\n\nOlmoRL は、長いシーケンスとより高速な全体的スループットを処理するために、強化学習インフラストラクチャに大幅な改善を加えました。\n\n\nRL での主要な技術的課題は、推論（ロールアウト）の管理です。最終モデルでは、最大 32K トークンの長さのロールアウトを生成し、平均 10K トークン以上になります。\nリソース配分（32B モデルの場合）:\n\n訓練: 8 H100 ノード\n推論: 20 ノード\nGPU 利用率: 推論が訓練の約 5 倍の計算量を使用\n\n自己回帰推論のコストが高いため、学習器は時間の 75% をデータ待ちに費やします。\n\n\n\n+------------------------------------------------------------------+\n|               OlmoRL Infrastructure Components                   |\n+------------------------------------------------------------------+\n|                                                                  |\n|  1. Fully Asynchronous Training                                 |\n|     - Centralized learner across multiple nodes (DeepSpeed)     |\n|     - Large pool of actors (independent vLLM instances)         |\n|     - Prompts queue & Results queue                             |\n|                                                                  |\n|  2. Continuous Batching                                         |\n|     - Remove compute waste for long generations                 |\n|     - Constantly enqueue new generations as each one finishes   |\n|     - Up to 54% compute savings vs static batching              |\n|                                                                  |\n|  3. Active Sampling                                             |\n|     - Continuously pull completions and resample prompts        |\n|     - Filter until desired batch size is reached                |\n|     - More efficient than dynamic oversampling (3x reduction)   |\n|                                                                  |\n|  4. Inflight Updates                                            |\n|     - Update weights without pausing generation engine          |\n|     - Thread-safe, no KV cache invalidation                     |\n|     - Up to 4x throughput increase                              |\n|                                                                  |\n+------------------------------------------------------------------+\nFully Asynchronous Training:\n\n中央集権的な学習器を複数ノードに分散（DeepSpeed 使用）\n大規模なアクタープール、それぞれ独立した vLLM インスタンスを実行\n学習器がプロンプトをキューに入れ、アクターに配信\nアクターが環境と相互作用し、結果をキューを通じて返す\n\nContinuous Batching:\n\n各生成が終了するたびに新しい生成を絶えずエンキュー\n静的バッチングと比較して、長い生成での計算の無駄を削減\nOlmo 3 では、32K 生成長で平均 14,628 トークン、最大 32K トークン\n静的バッチングでは最大 54% の計算が無駄になっていた\n\nActive Sampling:\n\nフィルタリングされたインスタンスを補償するため、継続的にアクターから補完を引き出し、プロンプトをキューに再サンプリング\nゼロ勾配でない補完の所望のバッチサイズに到達するまで、アクティブにサンプリングとフィルタリングを実施\nDAPO の動的サンプリング（3 倍のオーバーサンプリング）よりも効率的\n\nInflight Updates:\n\n各訓練ステップ後、生成エンジンを一時停止せずに重みを即座に更新\nスレッドセーフな生成フレームワークに依存し、KV キャッシュを無効化せずに生成を継続\n同じリソースで最大 4 倍の高速化を達成\n\n\n\n\n\n\n\n設定\n総トークン数 (Mtok)\nトークン/秒\nMFU (%)\nMBU (%)\n\n\n\n\nOLMo 2\n6.34\n881\n0.30\n12.90\n\n\n+ continuous batching\n7.02\n975\n0.33\n14.29\n\n\n+ better threading\n9.77\n1358\n0.46\n19.89\n\n\n+ inflight updates (Olmo 3)\n21.23\n2949\n1.01\n43.21\n\n\n\nInflight updates の追加が最も劇的な改善をもたらしています。\n\n\n\n\nOlmo 3.1 Think 32B は、拡張された OlmoRL 訓練を通じて、性能向上を示しました。Dolci Think RL データセットでの追加エポックにより、以下の改善が観察されました：\n性能向上:\n\nAIME 2024: +4 ポイント\nIFBench: +20 ポイント\n他のベンチマーク: 性能を維持\n\n拡張訓練により、より長い RL 訓練が汎化性能を向上させ、破局的忘却なしに安定した訓練が可能であることが確認されました。\n\n\n\n\n\nDelta Learning を用いた選好調整を先に実施してから RLVR を適用すると、SFT 単独よりも優れた全体的な性能を達成します。\n\n\n\n最終的な RL ミックスを DPO モデルで実行すると、SFT モデルで実行する場合よりも一貫して優れた性能を示します。\n主要な違い:\n\nRL が改善しない評価では、DPO モデルがしばしば優れた性能を発揮し、RL 訓練中もその優位性を維持（例: AlpacaEval）\nRL によって明示的にターゲットにされた評価では、DPO と SFT モデルの両方が同様の最終性能を達成（例: OMEGA）\nRL によってターゲットにされているが DPO からさらに改善が困難な評価では、SFT モデルが改善して DPO 性能に近づく（例: AIME 2025）\n\n\n\n\nRL 訓練中、すべてのドメインで報酬が着実に増加します（ただし、増加率は異なります）。指示追従データが最も着実に増加し、コード報酬が最もゆっくり増加します。\n\n\n\n\nOlmoRL は、GRPO をベースとした効率的な強化学習フレームワークであり、以下を実現しています：\n主要な貢献:\n\nアルゴリズムの改善: Vanilla GRPO に対して 7 つの重要な改善を実施\n大規模データセット: Dolci-Think-RL（100K プロンプト、4 ドメイン）\n効率的なインフラ: Continuous batching、active sampling、inflight updates により 4 倍の高速化\n複数ドメイン対応: Math、Code、Instruction-following、General chat\n拡張訓練: Olmo 3.1 Think 32B で 2300 ステップの拡張訓練により大幅な性能向上\n\nOlmoRL により、思考型モデルの訓練が大幅に効率化され、完全にオープンな RL 研究環境が提供されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-の概要",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-の概要",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、長い推論トレースを伴う強化学習の課題に対処するため、アルゴリズムとエンジニアリングインフラを密接に統合したシステムです。従来の数学とコードに限定されていた RLVR を、より幅広い検証可能なタスクへと拡張しています。\n主な特徴:\n\nアルゴリズムの改善: GRPO をベースに、DAPO や Dr GRPO などの最新の改善を統合\n大規模データセット: Dolci-Think-RL（約 100K プロンプト、4 ドメイン）\n効率的なインフラ: 長いシーケンス（最大 32K トークン）を効率的に処理する分散訓練システム\n4倍の高速化: OLMo 2 の RL インフラと比較して約 4 倍の高速化を達成",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-アルゴリズムの詳細",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-アルゴリズムの詳細",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL の強化学習段階は、GRPO（Shao et al., 2024）をベースに構築され、DAPO（Yu et al., 2025）や Dr GRPO（Liu et al., 2025b）などの最新のアルゴリズム改善を統合しています。\n\n\nOlmoRL は、Vanilla GRPO に対して以下の改善を実施しています：\n1. Zero gradient signal filtering:\n\n報酬がすべて同一のグループ（advantage の標準偏差がゼロ）を除外\nゼロ勾配のサンプルでの訓練を回避（DAPO と同様）\n\n2. Active sampling:\n\nZero gradient filtering にもかかわらず、一貫したバッチサイズを維持\n動的サンプリングの改良版を実装（詳細は §4.4.3）\n\n3. Token-level loss:\n\nサンプルごとではなく、バッチ全体のトークン数で損失を正規化\n長さバイアスを回避\n\n4. No KL loss:\n\nKL 損失を削除（GLM-4.5、DAPO、Dr GRPO などと同様の実践）\nより制限の少ないポリシー更新を可能にし、過最適化や訓練の不安定化を引き起こさない\n\n5. Clip higher:\n\n上限クリッピング項を下限よりわずかに高く設定\nトークンに対するより大きな更新を可能にする（Yu et al., 2025）\n\n6. Truncated importance sampling:\n\n推論エンジンと訓練エンジン間の対数確率の差を調整\n切り詰められた重要度サンプリング比を損失に掛ける（Yao et al., 2025）\n\n7. No standard deviation normalization:\n\nAdvantage 計算時に標準偏差で正規化しない（Liu et al., 2025b）\n難易度バイアスを除去（報酬の標準偏差が低い問題の advantage が過度に増加するのを防止）\n\n\n\n\n最終的な目的関数は、token-level loss、truncated importance sampling、clip-higher、および advantage 計算における標準偏差正規化なしを含みます：\n\\[\nJ(\\theta) = \\frac{1}{\\sum_{i=1}^{G} |y_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|y_i|} \\min\\left(\\frac{\\pi(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}{\\pi_{\\text{vllm}}(y_{i,t} | x, y_{i,&lt;t}; \\theta_{\\text{old}})}, \\rho\\right) \\times \\min(r_{i,t} A_{i,t}, \\text{clip}(r_{i,t}, 1 - \\varepsilon_{\\text{low}}, 1 + \\varepsilon_{\\text{high}}) A_{i,t})\n\\]\nここで：\n\n\\(r_{i,t} = \\frac{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta)}{\\pi(y_{i,t}|x,y_{i,&lt;t};\\theta_{\\text{old}})}\\)\n\\(\\varepsilon_{\\text{low}}\\) と \\(\\varepsilon_{\\text{high}}\\) はクリッピングハイパーパラメータ\n\\(\\rho\\) は truncated importance sampling の上限値\nAdvantage \\(A_{i,t}\\) は、グループ \\(G\\) 内での相対報酬に基づいて計算：\n\n\\[\nA_{i,t} = r(x, y_i) - \\text{mean}(\\{r(x, y_i)\\}_{i=1}^{G})\n\\]\n\n\n\n\n\n\nNoteGRPO と PPO の比較\n\n\n\n\n\nGRPO は PPO の変種であり、主な違いは報酬の正規化方法にあります。\nPPO:\n\n報酬を全データセットまたはバッチ全体で正規化\nグローバルなベースラインを使用\n\nGRPO:\n\n同じプロンプトから生成された応答グループ内で報酬を正規化\nグループベースの相対的な品質評価により、学習の安定性が向上\n思考型モデルのように出力長が大きく異なる場合に特に有効\n\nOlmoRL の改善:\n\nVanilla GRPO に対して 7 つの主要な改善を実施\nZero gradient filtering と active sampling により、訓練の効率と安定性を大幅に向上\n\n\n\n\n\n\n\nOlmoRL は、OLMo 2 の数学ドメインを超えて、一般的なドメインへと検証可能な報酬を拡張しています。各ドメインに対して異なるカスタム検証器を使用します：\nMath（数学）:\n\nルールベースの検証器\n基本的な正規化を実行し、SymPy を使用して参照回答と比較\n参照回答と同じであれば 1、そうでなければ 0 を返す\n\nCode（コーディング）:\n\nテストケースベースの検証器\n応答に対して一連のテストケースを実行\n\n通過したテストケースの割合を報酬とする、または (b) すべてのテストケースを通過した場合に 1、そうでなければ 0 を返す\n\n\nInstruction-following（指示追従）:\n\nプロンプトからの一連の制約への準拠をチェックする関数セットを通して応答を渡す\nすべての制約が満たされている場合に 1、そうでなければ 0 を返す\n\nChat—reference（参照付きチャット）:\n\nground-truth 応答がある場合、LM judge を使用してモデルの応答を参照回答と比較\n応答の品質に基づいて [0, 1] のスコアを付与\n\nChat—open-ended（オープンエンドチャット）:\n\n参照回答なしで、LM judge を使用して応答の品質に基づいて [0, 1] のスコアを付与",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#dolci-think-rl-データセット",
    "href": "ja/olmo-3/10-olmorl-grpo.html#dolci-think-rl-データセット",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Dolci-Think-RL は、4 つのドメイン（数学、コーディング、指示追従、一般的なチャット）にわたる約 100K のサンプルからなる大規模で多様なデータセットです。多様な推論タスクでの堅牢な RL を支援しながら、一般的な有用性を維持します。\n\n\n\n\n\n\n\n\n\n\n\nカテゴリ\nデータセット\nThink RL 用プロンプト数\nInstruct RL 用プロンプト数\n\n\n\n\nPrecise IF\nIF-RLVR\n30,186\n38,000\n\n\nMath\nOpen-Reasoner-Zero\n3,000\n14,000\n\n\n\nDAPO-Math\n2,584\n7,000\n\n\n\nAceReason-Math\n6,602\n-\n\n\n\nPolaris-Dataset\n-\n14,000\n\n\n\nKlearReasoner-MathSub\n3,000\n9,000\n\n\n\nOMEGA-train\n15,000\n20,000\n\n\nCoding\nAceCoder\n9,767\n20,000\n\n\n\nKlearReasoner-Code\n8,040\n-\n\n\n\nNemotron Post-training Code\n2,303\n-\n\n\n\nSYNTHETIC-2\n3,000\n-\n\n\nGeneral Chat\nTulu 3 SFT\n7,129\n18,955\n\n\n\nWildchat-4.8M\n7,129\n18,761\n\n\n\nMulti-Subject RLVR\n7,129\n12,234\n\n\n合計\n\n104,869\n171,950\n\n\n\n\n\n\nStep 1: プロンプトの調達:\n各ドメインから高品質なプロンプトを収集し、キュレーションします。\n\nMath: Open-Reasoner-Zero、DAPO-Math、AceReason-Math、KlearReasoner-MathSub、OMEGA など\nCoding: AceCoder、Klear-Reasoner Code、Nemotron Post-training Code、SYNTHETIC-2 など\nInstruction-following: IF-RLVR（最大 5 つの制約、IFEval と IFBench-Train からサンプリング）\nGeneral chat: Tülu 3 SFT、WildChat-4.8M、Multi-subject-RLVR\n\nStep 2: オフライン難易度フィルタリング:\n\nモデルの初期チェックポイントから各プロンプトに対して 8 つのロールアウトを生成\nモデルが容易に解決するサンプル（pass rate &gt; 62.5%）を除外\n温度 1.0、top-p 1.0 でサンプリング（RL 訓練時と一致）\n\nStep 3: データミキシング:\n\nドメイン固有の実験を実施し、最初の 500-1000 ステップで下流評価のトレンドを観察\n高品質なデータセットをアップウェイト\n各ドメインでほぼ同等のデータ量を使用（数学と指示追従にわずかに重点）\nOMEGA から特定のサブタスクをダウンサンプル",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmorl-インフラストラクチャ",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmorl-インフラストラクチャ",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、長いシーケンスとより高速な全体的スループットを処理するために、強化学習インフラストラクチャに大幅な改善を加えました。\n\n\nRL での主要な技術的課題は、推論（ロールアウト）の管理です。最終モデルでは、最大 32K トークンの長さのロールアウトを生成し、平均 10K トークン以上になります。\nリソース配分（32B モデルの場合）:\n\n訓練: 8 H100 ノード\n推論: 20 ノード\nGPU 利用率: 推論が訓練の約 5 倍の計算量を使用\n\n自己回帰推論のコストが高いため、学習器は時間の 75% をデータ待ちに費やします。\n\n\n\n+------------------------------------------------------------------+\n|               OlmoRL Infrastructure Components                   |\n+------------------------------------------------------------------+\n|                                                                  |\n|  1. Fully Asynchronous Training                                 |\n|     - Centralized learner across multiple nodes (DeepSpeed)     |\n|     - Large pool of actors (independent vLLM instances)         |\n|     - Prompts queue & Results queue                             |\n|                                                                  |\n|  2. Continuous Batching                                         |\n|     - Remove compute waste for long generations                 |\n|     - Constantly enqueue new generations as each one finishes   |\n|     - Up to 54% compute savings vs static batching              |\n|                                                                  |\n|  3. Active Sampling                                             |\n|     - Continuously pull completions and resample prompts        |\n|     - Filter until desired batch size is reached                |\n|     - More efficient than dynamic oversampling (3x reduction)   |\n|                                                                  |\n|  4. Inflight Updates                                            |\n|     - Update weights without pausing generation engine          |\n|     - Thread-safe, no KV cache invalidation                     |\n|     - Up to 4x throughput increase                              |\n|                                                                  |\n+------------------------------------------------------------------+\nFully Asynchronous Training:\n\n中央集権的な学習器を複数ノードに分散（DeepSpeed 使用）\n大規模なアクタープール、それぞれ独立した vLLM インスタンスを実行\n学習器がプロンプトをキューに入れ、アクターに配信\nアクターが環境と相互作用し、結果をキューを通じて返す\n\nContinuous Batching:\n\n各生成が終了するたびに新しい生成を絶えずエンキュー\n静的バッチングと比較して、長い生成での計算の無駄を削減\nOlmo 3 では、32K 生成長で平均 14,628 トークン、最大 32K トークン\n静的バッチングでは最大 54% の計算が無駄になっていた\n\nActive Sampling:\n\nフィルタリングされたインスタンスを補償するため、継続的にアクターから補完を引き出し、プロンプトをキューに再サンプリング\nゼロ勾配でない補完の所望のバッチサイズに到達するまで、アクティブにサンプリングとフィルタリングを実施\nDAPO の動的サンプリング（3 倍のオーバーサンプリング）よりも効率的\n\nInflight Updates:\n\n各訓練ステップ後、生成エンジンを一時停止せずに重みを即座に更新\nスレッドセーフな生成フレームワークに依存し、KV キャッシュを無効化せずに生成を継続\n同じリソースで最大 4 倍の高速化を達成\n\n\n\n\n\n\n\n設定\n総トークン数 (Mtok)\nトークン/秒\nMFU (%)\nMBU (%)\n\n\n\n\nOLMo 2\n6.34\n881\n0.30\n12.90\n\n\n+ continuous batching\n7.02\n975\n0.33\n14.29\n\n\n+ better threading\n9.77\n1358\n0.46\n19.89\n\n\n+ inflight updates (Olmo 3)\n21.23\n2949\n1.01\n43.21\n\n\n\nInflight updates の追加が最も劇的な改善をもたらしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#olmo-3.1-think-32b-の拡張訓練",
    "href": "ja/olmo-3/10-olmorl-grpo.html#olmo-3.1-think-32b-の拡張訓練",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Olmo 3.1 Think 32B は、拡張された OlmoRL 訓練を通じて、性能向上を示しました。Dolci Think RL データセットでの追加エポックにより、以下の改善が観察されました：\n性能向上:\n\nAIME 2024: +4 ポイント\nIFBench: +20 ポイント\n他のベンチマーク: 性能を維持\n\n拡張訓練により、より長い RL 訓練が汎化性能を向上させ、破局的忘却なしに安定した訓練が可能であることが確認されました。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#主要な発見",
    "href": "ja/olmo-3/10-olmorl-grpo.html#主要な発見",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "Delta Learning を用いた選好調整を先に実施してから RLVR を適用すると、SFT 単独よりも優れた全体的な性能を達成します。\n\n\n\n最終的な RL ミックスを DPO モデルで実行すると、SFT モデルで実行する場合よりも一貫して優れた性能を示します。\n主要な違い:\n\nRL が改善しない評価では、DPO モデルがしばしば優れた性能を発揮し、RL 訓練中もその優位性を維持（例: AlpacaEval）\nRL によって明示的にターゲットにされた評価では、DPO と SFT モデルの両方が同様の最終性能を達成（例: OMEGA）\nRL によってターゲットにされているが DPO からさらに改善が困難な評価では、SFT モデルが改善して DPO 性能に近づく（例: AIME 2025）\n\n\n\n\nRL 訓練中、すべてのドメインで報酬が着実に増加します（ただし、増加率は異なります）。指示追従データが最も着実に増加し、コード報酬が最もゆっくり増加します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/10-olmorl-grpo.html#まとめ",
    "href": "ja/olmo-3/10-olmorl-grpo.html#まとめ",
    "title": "OlmoRL / GRPO: 効率的な強化学習",
    "section": "",
    "text": "OlmoRL は、GRPO をベースとした効率的な強化学習フレームワークであり、以下を実現しています：\n主要な貢献:\n\nアルゴリズムの改善: Vanilla GRPO に対して 7 つの重要な改善を実施\n大規模データセット: Dolci-Think-RL（100K プロンプト、4 ドメイン）\n効率的なインフラ: Continuous batching、active sampling、inflight updates により 4 倍の高速化\n複数ドメイン対応: Math、Code、Instruction-following、General chat\n拡張訓練: Olmo 3.1 Think 32B で 2300 ステップの拡張訓練により大幅な性能向上\n\nOlmoRL により、思考型モデルの訓練が大幅に効率化され、完全にオープンな RL 研究環境が提供されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "OlmoRL / GRPO"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html",
    "href": "ja/olmo-3/08-dolci-dataset.html",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci (Dolma Instruct) は、Olmo 3 の Post-training（後訓練）に使用される包括的なデータスイートです。Think（思考型）、Instruct（指示追従型）、RL-Zero（Base から直接 RL）という 3 つの異なるモデルバリエーションをサポートするため、複数のサブセットから構成されています。\n\n\nDolci は、Olmo 3 Base モデルを特定のタスクに特化させるための高品質なデータセットです。事前学習で獲得した幅広い知識を、実用的な能力（数学的推論、コーディング、指示追従、チャット）に変換することを目的としています。\nDolci の主な特徴:\n\n完全オープン: すべてのデータソースとキュレーションパイプラインを公開\n高品質: モデル生成データの厳格なフィルタリングと検証\n多様なドメイン: Math, Code, Chat, Instruction Following, Safety をカバー\n段階的訓練: SFT、DPO、RL の各ステージに最適化されたデータ\n\n\n\n\nDolci は、3 つの主要なサブセットから構成されています。\n┌──────────────────────────────────────────────────────────────┐\n│                     Dolci Data Suite                         │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Think                                                 │\n│    └─&gt; Think SFT (1.94M samples)                             │\n│        └─&gt; Think DPO (187K pairs)                            │\n│            └─&gt; Think RL (40K prompts)                        │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Instruct                                              │\n│    └─&gt; Instruct SFT (544K samples)                           │\n│        └─&gt; Instruct DPO (105K pairs)                         │\n│            └─&gt; Instruct RL (128K prompts)                    │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci RL-Zero                                               │\n│    └─&gt; Math (30K prompts)                                    │\n│    └─&gt; Code (30K prompts)                                    │\n│    └─&gt; IF (30K prompts)                                      │\n│    └─&gt; General Mix (30K prompts)                             │\n└──────────────────────────────────────────────────────────────┘\n各サブセットは、異なるモデルバリエーションの訓練パイプラインに対応しています。\nDolci Think: 段階的推論を行う思考型モデル用\nDolci Instruct: 簡潔で直接的な応答を生成するモデル用\nDolci RL-Zero: Base モデルから直接 RL で訓練するモデル用\n\n\n\nDolci Think は、最終回答を生成する前に段階的推論を行う思考型モデル（Olmo 3 Think）を訓練するためのデータセットです。\n\n\n規模: 約 194 万サンプル\n目的: モデルに思考トレースを生成する能力を教える\nデータソースの構成:\n\n\n\nTable 1: Dolci Think SFT のデータソース構成\n\n\n\n\n\nカテゴリ\n主なソース\nサンプル数\n\n\n\n\nMath\nOpenThoughts3+, SYNTHETIC-2-Verified\n約 85 万\n\n\nCode\nOpenThoughts3+, Dolci Think Python Algorithms\n約 55 万\n\n\nChat & IF\nWildChat, Persona IF, OpenAssistant\n約 45 万\n\n\nSafety\nCoCoNot, WildGuardMix, WildJailbreak\n約 9 万\n\n\nその他\nAya, TableGPT\n約 10 万\n\n\n\n\n\n\nデータ生成手法:\nDolci Think SFT のデータは、既存のプロンプトに対して強力なモデルで思考トレースを生成することで作成されています。\n使用モデル:\n\nMath / Code: QwQ-32B（思考型モデル）\nChat / Safety: DeepSeek R1（推論特化モデル）\n\nフィルタリング基準:\n\n不完全な思考トレース（途中で打ち切られたもの）を削除\nドメイン固有エラー（数式の誤り、コード構文エラー）を削除\n過度の繰り返しや冗長性を削除\nOpenAI taxonomy を使用したトピックフィルタリング\n\n\n\n\n規模: 約 18.7 万ペア\n目的: 思考トレースの品質を向上させる\nDelta Learning の原理:\nDPO（Direct Preference Optimization）で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。Dolci Think DPO では、強いモデルと弱いモデルのペアを使用して、明確な品質差を持つ選好データを作成しています。\nデータ生成設定:\n\n\n\nTable 2: Delta Learning のモデルペア\n\n\n\n\n\n役割\nモデル\n\n\n\n\nChosen (選択)\nQwen 3 32B (thinking mode)\n\n\nRejected (棄却)\nQwen 3 0.6B (thinking mode)\n\n\n\n\n\n\n主要な知見:\nSFT では改善しないデータでも、DPO では大幅に改善可能です。\n\nQwen3-32B の出力で SFT すると性能低下\n同じデータを弱いモデルとペアにして DPO すると大幅改善\n\nこれは、「絶対的な品質」よりも「相対的な品質差」が学習に重要であることを示しています。\n\n\n\n規模: 約 4 万プロンプト\n目的: RLVR（Reinforcement Learning with Verifiable Rewards）による性能向上\n特徴:\nDolci Think RL は、思考型モデルが苦手とする挑戦的なプロンプトを集めたデータセットです。\nドメイン:\n\nMath: AIME（アメリカ数学招待試験）レベルの高難度問題\nCode: LiveCodeBench などの実践的プログラミング課題\nReasoning: ZebraLogic などの論理パズル\n\n\n\n\n\nDolci Instruct は、思考トレースを生成せずに、簡潔で直接的な応答を生成するモデル（Olmo 3 Instruct）を訓練するためのデータセットです。\n\n\n規模: 約 54.4 万サンプル\n目的: 効率的で有用な応答を生成する能力を教える\n主要なデータソース:\n\nTulu 3 SFT: 多様な指示追従タスク\nFunction-calling データ: ツール使用と API 呼び出しのサンプル\nFlan: タスクフォーマット学習データ\n\nDolci Think SFT との違い:\n\n\n\nTable 3: Think と Instruct の SFT データの比較\n\n\n\n\n\n項目\nDolci Think SFT\nDolci Instruct SFT\n\n\n\n\n思考トレース\nあり\nなし\n\n\n応答スタイル\n段階的推論\n簡潔・直接的\n\n\nFunction-calling\nなし\nあり\n\n\nサンプル数\n194 万\n54.4 万\n\n\n\n\n\n\n\n\n\n規模: 約 10.5 万ペア\n目的: 簡潔性とユーザビリティの向上\n主要な改善点:\nMulti-turn preferences（複数ターン選好データ）:\n合成会話を生成し、複数ターンにわたる一貫した応答を学習します。\nLength control（応答長制御）:\nChosen と Rejected の長さ差を 100 トークン以下に制限することで、冗長性を抑えています。\n\nモデルが単に「長い応答」を学習するのを防ぐ\nユーザビリティを重視した簡潔な応答を促進\n\n\n\n\n規模: 約 12.8 万プロンプト\n目的: RLVR による核心能力のさらなる向上\nドメイン分布:\n\nInstruction Following: 複雑な指示の正確な追従\nChat: 多様な会話シナリオへの対応\nFunction-calling: ツール使用の精度向上\nKnowledge Recall: 知識の正確な想起\n\n\n\n\n\nDolci RL-Zero は、Base モデルから SFT/DPO を経由せずに、直接 RL で訓練するための特別なデータセットです。\n\n\n研究的価値:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能\n完全にオープンな RL ベンチマークを提供\n\n既存の課題:\n従来のオープンウェイトモデル（Llama 3、Qwen 2.5 など）は事前学習データを公開していないため、RL 研究が制限されていました。\nDolci RL-Zero により、データリークの影響を排除した明確なベンチマークが可能になります。\n\n\n\nDolci RL-Zero は、4 つの異なるドメインで構成されています。\nMath（数学）:\n\n規模: 3 万プロンプト\nタスク: GSM8K、MATH などの数学的推論問題\n検証方法: SymPy による数式比較\n\nCode（コーディング）:\n\n規模: 3 万プロンプト\nタスク: HumanEvalPlus、LiveCodeBench などのプログラミング課題\n検証方法: テストケースの実行と検証\n\nIF（Instruction Following）:\n\n規模: 3 万プロンプト\nタスク: IFEval、IFBench などの精密な指示追従\n検証方法: 制約チェック関数による検証\n\nGeneral Mix（一般混合）:\n\n規模: 3 万プロンプト\nタスク: 上記 3 つのドメインと Chat の混合\n検証方法: ドメインに応じた検証方法\n\n\n\n\nDolci RL-Zero の全データは、評価ベンチマークとの重複を排除するため、厳格な Decontamination 処理が施されています。\n手法: decon パッケージによる 2 フェーズ処理\n\n検出フェーズ: 8-gram マッチングで重複を検出（閾値 50%）\nクラスタ拡張フェーズ: 類似サンプルのクラスタ全体を除去\n\nこれにより、RL 訓練データとベンチマークデータの完全な分離を保証しています。\n\n\n\n\nDolci のデータキュレーションは、以下のパイプラインで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│              Dolci Data Curation Pipeline                    │\n├──────────────────────────────────────────────────────────────┤\n│  Step 1: Source Selection                                    │\n│    └─&gt; Public datasets (OpenThoughts, WildChat, etc.)        │\n│        └─&gt; Model generation (QwQ-32B, DeepSeek R1)           │\n├──────────────────────────────────────────────────────────────┤\n│  Step 2: Heuristic Filtering                                 │\n│    └─&gt; Remove incomplete traces                              │\n│        └─&gt; Remove domain-specific errors                     │\n│            └─&gt; Remove excessive repetition                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 3: Topic Filtering                                     │\n│    └─&gt; OpenAI taxonomy classification                        │\n│        └─&gt; Remove off-topic samples                          │\n├──────────────────────────────────────────────────────────────┤\n│  Step 4: Difficulty Filtering                                │\n│    └─&gt; Select challenging prompts for RL                     │\n│        └─&gt; Balance difficulty distribution                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 5: Data Mixing                                         │\n│    └─&gt; Balance domain distribution                           │\n│        └─&gt; Optimize mix for target tasks                     │\n├──────────────────────────────────────────────────────────────┤\n│  Step 6: Decontamination                                     │\n│    └─&gt; 8-gram matching (50% threshold)                       │\n│        └─&gt; Cluster expansion                                 │\n│            └─&gt; Final Dolci datasets                          │\n└──────────────────────────────────────────────────────────────┘\n\n\nStep 1: Source Selection（ソース選択）:\n公開データセットと強力なモデルによる生成データを収集します。\nStep 2: Heuristic Filtering（ヒューリスティックフィルタリング）:\n明らかな低品質データを除去します。\n\n不完全な思考トレース\nドメイン固有エラー（数式の誤り、コード構文エラー）\n過度の繰り返し\n\nStep 3: Topic Filtering（トピックフィルタリング）:\nOpenAI taxonomy を使用して、トピック外のサンプルを除去します。\nStep 4: Difficulty Filtering（難易度フィルタリング）:\nRL 用に挑戦的なプロンプトを選択し、難易度分布をバランスします。\nStep 5: Data Mixing（データミキシング）:\nドメイン分布をバランスし、ターゲットタスクに最適化されたミックスを作成します。\nStep 6: Decontamination（評価データ汚染除去）:\n評価ベンチマークとの重複を完全に排除します。\n\n\n\n\nDolci データスイートは、以下の特徴を持ちます。\n\n\nすべてのデータソース、キュレーションパイプライン、処理コードを公開しています。\n公開内容:\n\n元のデータソースへの参照\nキュレーションスクリプト\nフィルタリング基準\nデータミキシング比率\n\n\n\n\n強力なモデルによる生成と厳格なフィルタリングにより、高品質を実現しています。\n品質保証の仕組み:\n\nモデル生成: QwQ-32B、DeepSeek R1 などの最先端モデルを使用\n複数段階フィルタリング: ヒューリスティック、トピック、難易度\nDecontamination: 評価データとの完全な分離\n\n\n\n\nMath、Code、Chat、Instruction Following、Safety など、幅広いドメインをカバーしています。\nドメイン分布:\n\n\n\nTable 4: Dolci のドメインカバレッジ\n\n\n\n\n\nドメイン\nThink SFT\nInstruct SFT\nRL-Zero\n\n\n\n\nMath\n85 万\n含む\n3 万\n\n\nCode\n55 万\n含む\n3 万\n\n\nChat\n45 万\n大部分\n含む\n\n\nIF\n45 万\n大部分\n3 万\n\n\nSafety\n9 万\n含む\n-\n\n\n\n\n\n\n\n\n\nSFT、DPO、RL の各ステージに最適化されたデータを提供しています。\n訓練パイプライン:\nBase Model\n    ↓\n  SFT (Dolci Think/Instruct SFT)\n    ↓\n  DPO (Dolci Think/Instruct DPO)\n    ↓\n  RL (Dolci Think/Instruct RL)\n    ↓\nFinal Model\n各ステージでデータの種類が異なります。\n\nSFT: 高品質な入力-出力ペア\nDPO: 品質差のある選好ペア\nRL: 検証可能な報酬を持つプロンプト\n\n\n\n\n\n\n\nNoteDelta Learning の詳細\n\n\n\n\n\nDelta Learning は、Dolci DPO データセットの作成に使用される重要な手法です。\n核心的洞察:\nDPO で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。絶対的な品質よりも、相対的な品質差が学習に重要です。\n実験結果:\n\n\n\n設定\nMATH スコア\n変化\n\n\n\n\nBase モデル\n45.2\n-\n\n\nQwen3-32B で SFT\n43.8\n-1.4\n\n\nQwen3-32B (chosen) vs Qwen3-0.6B (rejected) で DPO\n52.3\n+7.1\n\n\n\n同じ Qwen3-32B の出力でも、SFT では性能が低下するのに対し、弱いモデルとペアにして DPO すると大幅に改善します。\n応用:\nこの知見は、Dolci Think DPO と Dolci Instruct DPO の両方で活用されています。\n\n\n\n\n\n\n\nDolci は、Olmo 3 の Post-training に使用される包括的なデータスイートです。Think、Instruct、RL-Zero という 3 つの異なるモデルバリエーションをサポートし、SFT、DPO、RL の各訓練ステージに最適化されたデータを提供しています。\n主な貢献:\n\n完全オープン: すべてのデータソースとパイプラインを公開\n高品質: 強力なモデルによる生成と厳格なフィルタリング\n多様性: Math, Code, Chat, IF, Safety をカバー\n段階的訓練: SFT、DPO、RL に最適化\n研究価値: RL-Zero により完全オープンな RL ベンチマークを提供\n\nDolci は、完全にオープンな Post-training データセットとして、研究者が再現性の高い研究を行い、任意のステージから介入・カスタマイズできるようにしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#概要と目的",
    "href": "ja/olmo-3/08-dolci-dataset.html#概要と目的",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、Olmo 3 Base モデルを特定のタスクに特化させるための高品質なデータセットです。事前学習で獲得した幅広い知識を、実用的な能力（数学的推論、コーディング、指示追従、チャット）に変換することを目的としています。\nDolci の主な特徴:\n\n完全オープン: すべてのデータソースとキュレーションパイプラインを公開\n高品質: モデル生成データの厳格なフィルタリングと検証\n多様なドメイン: Math, Code, Chat, Instruction Following, Safety をカバー\n段階的訓練: SFT、DPO、RL の各ステージに最適化されたデータ",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-subsets",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-subsets",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、3 つの主要なサブセットから構成されています。\n┌──────────────────────────────────────────────────────────────┐\n│                     Dolci Data Suite                         │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Think                                                 │\n│    └─&gt; Think SFT (1.94M samples)                             │\n│        └─&gt; Think DPO (187K pairs)                            │\n│            └─&gt; Think RL (40K prompts)                        │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci Instruct                                              │\n│    └─&gt; Instruct SFT (544K samples)                           │\n│        └─&gt; Instruct DPO (105K pairs)                         │\n│            └─&gt; Instruct RL (128K prompts)                    │\n├──────────────────────────────────────────────────────────────┤\n│  Dolci RL-Zero                                               │\n│    └─&gt; Math (30K prompts)                                    │\n│    └─&gt; Code (30K prompts)                                    │\n│    └─&gt; IF (30K prompts)                                      │\n│    └─&gt; General Mix (30K prompts)                             │\n└──────────────────────────────────────────────────────────────┘\n各サブセットは、異なるモデルバリエーションの訓練パイプラインに対応しています。\nDolci Think: 段階的推論を行う思考型モデル用\nDolci Instruct: 簡潔で直接的な応答を生成するモデル用\nDolci RL-Zero: Base モデルから直接 RL で訓練するモデル用",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-think",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-think",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci Think は、最終回答を生成する前に段階的推論を行う思考型モデル（Olmo 3 Think）を訓練するためのデータセットです。\n\n\n規模: 約 194 万サンプル\n目的: モデルに思考トレースを生成する能力を教える\nデータソースの構成:\n\n\n\nTable 1: Dolci Think SFT のデータソース構成\n\n\n\n\n\nカテゴリ\n主なソース\nサンプル数\n\n\n\n\nMath\nOpenThoughts3+, SYNTHETIC-2-Verified\n約 85 万\n\n\nCode\nOpenThoughts3+, Dolci Think Python Algorithms\n約 55 万\n\n\nChat & IF\nWildChat, Persona IF, OpenAssistant\n約 45 万\n\n\nSafety\nCoCoNot, WildGuardMix, WildJailbreak\n約 9 万\n\n\nその他\nAya, TableGPT\n約 10 万\n\n\n\n\n\n\nデータ生成手法:\nDolci Think SFT のデータは、既存のプロンプトに対して強力なモデルで思考トレースを生成することで作成されています。\n使用モデル:\n\nMath / Code: QwQ-32B（思考型モデル）\nChat / Safety: DeepSeek R1（推論特化モデル）\n\nフィルタリング基準:\n\n不完全な思考トレース（途中で打ち切られたもの）を削除\nドメイン固有エラー（数式の誤り、コード構文エラー）を削除\n過度の繰り返しや冗長性を削除\nOpenAI taxonomy を使用したトピックフィルタリング\n\n\n\n\n規模: 約 18.7 万ペア\n目的: 思考トレースの品質を向上させる\nDelta Learning の原理:\nDPO（Direct Preference Optimization）で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。Dolci Think DPO では、強いモデルと弱いモデルのペアを使用して、明確な品質差を持つ選好データを作成しています。\nデータ生成設定:\n\n\n\nTable 2: Delta Learning のモデルペア\n\n\n\n\n\n役割\nモデル\n\n\n\n\nChosen (選択)\nQwen 3 32B (thinking mode)\n\n\nRejected (棄却)\nQwen 3 0.6B (thinking mode)\n\n\n\n\n\n\n主要な知見:\nSFT では改善しないデータでも、DPO では大幅に改善可能です。\n\nQwen3-32B の出力で SFT すると性能低下\n同じデータを弱いモデルとペアにして DPO すると大幅改善\n\nこれは、「絶対的な品質」よりも「相対的な品質差」が学習に重要であることを示しています。\n\n\n\n規模: 約 4 万プロンプト\n目的: RLVR（Reinforcement Learning with Verifiable Rewards）による性能向上\n特徴:\nDolci Think RL は、思考型モデルが苦手とする挑戦的なプロンプトを集めたデータセットです。\nドメイン:\n\nMath: AIME（アメリカ数学招待試験）レベルの高難度問題\nCode: LiveCodeBench などの実践的プログラミング課題\nReasoning: ZebraLogic などの論理パズル",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-instruct",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-instruct",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci Instruct は、思考トレースを生成せずに、簡潔で直接的な応答を生成するモデル（Olmo 3 Instruct）を訓練するためのデータセットです。\n\n\n規模: 約 54.4 万サンプル\n目的: 効率的で有用な応答を生成する能力を教える\n主要なデータソース:\n\nTulu 3 SFT: 多様な指示追従タスク\nFunction-calling データ: ツール使用と API 呼び出しのサンプル\nFlan: タスクフォーマット学習データ\n\nDolci Think SFT との違い:\n\n\n\nTable 3: Think と Instruct の SFT データの比較\n\n\n\n\n\n項目\nDolci Think SFT\nDolci Instruct SFT\n\n\n\n\n思考トレース\nあり\nなし\n\n\n応答スタイル\n段階的推論\n簡潔・直接的\n\n\nFunction-calling\nなし\nあり\n\n\nサンプル数\n194 万\n54.4 万\n\n\n\n\n\n\n\n\n\n規模: 約 10.5 万ペア\n目的: 簡潔性とユーザビリティの向上\n主要な改善点:\nMulti-turn preferences（複数ターン選好データ）:\n合成会話を生成し、複数ターンにわたる一貫した応答を学習します。\nLength control（応答長制御）:\nChosen と Rejected の長さ差を 100 トークン以下に制限することで、冗長性を抑えています。\n\nモデルが単に「長い応答」を学習するのを防ぐ\nユーザビリティを重視した簡潔な応答を促進\n\n\n\n\n規模: 約 12.8 万プロンプト\n目的: RLVR による核心能力のさらなる向上\nドメイン分布:\n\nInstruction Following: 複雑な指示の正確な追従\nChat: 多様な会話シナリオへの対応\nFunction-calling: ツール使用の精度向上\nKnowledge Recall: 知識の正確な想起",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-rl-zero",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-rl-zero",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci RL-Zero は、Base モデルから SFT/DPO を経由せずに、直接 RL で訓練するための特別なデータセットです。\n\n\n研究的価値:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能\n完全にオープンな RL ベンチマークを提供\n\n既存の課題:\n従来のオープンウェイトモデル（Llama 3、Qwen 2.5 など）は事前学習データを公開していないため、RL 研究が制限されていました。\nDolci RL-Zero により、データリークの影響を排除した明確なベンチマークが可能になります。\n\n\n\nDolci RL-Zero は、4 つの異なるドメインで構成されています。\nMath（数学）:\n\n規模: 3 万プロンプト\nタスク: GSM8K、MATH などの数学的推論問題\n検証方法: SymPy による数式比較\n\nCode（コーディング）:\n\n規模: 3 万プロンプト\nタスク: HumanEvalPlus、LiveCodeBench などのプログラミング課題\n検証方法: テストケースの実行と検証\n\nIF（Instruction Following）:\n\n規模: 3 万プロンプト\nタスク: IFEval、IFBench などの精密な指示追従\n検証方法: 制約チェック関数による検証\n\nGeneral Mix（一般混合）:\n\n規模: 3 万プロンプト\nタスク: 上記 3 つのドメインと Chat の混合\n検証方法: ドメインに応じた検証方法\n\n\n\n\nDolci RL-Zero の全データは、評価ベンチマークとの重複を排除するため、厳格な Decontamination 処理が施されています。\n手法: decon パッケージによる 2 フェーズ処理\n\n検出フェーズ: 8-gram マッチングで重複を検出（閾値 50%）\nクラスタ拡張フェーズ: 類似サンプルのクラスタ全体を除去\n\nこれにより、RL 訓練データとベンチマークデータの完全な分離を保証しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-pipeline",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-pipeline",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci のデータキュレーションは、以下のパイプラインで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│              Dolci Data Curation Pipeline                    │\n├──────────────────────────────────────────────────────────────┤\n│  Step 1: Source Selection                                    │\n│    └─&gt; Public datasets (OpenThoughts, WildChat, etc.)        │\n│        └─&gt; Model generation (QwQ-32B, DeepSeek R1)           │\n├──────────────────────────────────────────────────────────────┤\n│  Step 2: Heuristic Filtering                                 │\n│    └─&gt; Remove incomplete traces                              │\n│        └─&gt; Remove domain-specific errors                     │\n│            └─&gt; Remove excessive repetition                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 3: Topic Filtering                                     │\n│    └─&gt; OpenAI taxonomy classification                        │\n│        └─&gt; Remove off-topic samples                          │\n├──────────────────────────────────────────────────────────────┤\n│  Step 4: Difficulty Filtering                                │\n│    └─&gt; Select challenging prompts for RL                     │\n│        └─&gt; Balance difficulty distribution                   │\n├──────────────────────────────────────────────────────────────┤\n│  Step 5: Data Mixing                                         │\n│    └─&gt; Balance domain distribution                           │\n│        └─&gt; Optimize mix for target tasks                     │\n├──────────────────────────────────────────────────────────────┤\n│  Step 6: Decontamination                                     │\n│    └─&gt; 8-gram matching (50% threshold)                       │\n│        └─&gt; Cluster expansion                                 │\n│            └─&gt; Final Dolci datasets                          │\n└──────────────────────────────────────────────────────────────┘\n\n\nStep 1: Source Selection（ソース選択）:\n公開データセットと強力なモデルによる生成データを収集します。\nStep 2: Heuristic Filtering（ヒューリスティックフィルタリング）:\n明らかな低品質データを除去します。\n\n不完全な思考トレース\nドメイン固有エラー（数式の誤り、コード構文エラー）\n過度の繰り返し\n\nStep 3: Topic Filtering（トピックフィルタリング）:\nOpenAI taxonomy を使用して、トピック外のサンプルを除去します。\nStep 4: Difficulty Filtering（難易度フィルタリング）:\nRL 用に挑戦的なプロンプトを選択し、難易度分布をバランスします。\nStep 5: Data Mixing（データミキシング）:\nドメイン分布をバランスし、ターゲットタスクに最適化されたミックスを作成します。\nStep 6: Decontamination（評価データ汚染除去）:\n評価ベンチマークとの重複を完全に排除します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-features",
    "href": "ja/olmo-3/08-dolci-dataset.html#sec-dolci-features",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci データスイートは、以下の特徴を持ちます。\n\n\nすべてのデータソース、キュレーションパイプライン、処理コードを公開しています。\n公開内容:\n\n元のデータソースへの参照\nキュレーションスクリプト\nフィルタリング基準\nデータミキシング比率\n\n\n\n\n強力なモデルによる生成と厳格なフィルタリングにより、高品質を実現しています。\n品質保証の仕組み:\n\nモデル生成: QwQ-32B、DeepSeek R1 などの最先端モデルを使用\n複数段階フィルタリング: ヒューリスティック、トピック、難易度\nDecontamination: 評価データとの完全な分離\n\n\n\n\nMath、Code、Chat、Instruction Following、Safety など、幅広いドメインをカバーしています。\nドメイン分布:\n\n\n\nTable 4: Dolci のドメインカバレッジ\n\n\n\n\n\nドメイン\nThink SFT\nInstruct SFT\nRL-Zero\n\n\n\n\nMath\n85 万\n含む\n3 万\n\n\nCode\n55 万\n含む\n3 万\n\n\nChat\n45 万\n大部分\n含む\n\n\nIF\n45 万\n大部分\n3 万\n\n\nSafety\n9 万\n含む\n-\n\n\n\n\n\n\n\n\n\nSFT、DPO、RL の各ステージに最適化されたデータを提供しています。\n訓練パイプライン:\nBase Model\n    ↓\n  SFT (Dolci Think/Instruct SFT)\n    ↓\n  DPO (Dolci Think/Instruct DPO)\n    ↓\n  RL (Dolci Think/Instruct RL)\n    ↓\nFinal Model\n各ステージでデータの種類が異なります。\n\nSFT: 高品質な入力-出力ペア\nDPO: 品質差のある選好ペア\nRL: 検証可能な報酬を持つプロンプト\n\n\n\n\n\n\n\nNoteDelta Learning の詳細\n\n\n\n\n\nDelta Learning は、Dolci DPO データセットの作成に使用される重要な手法です。\n核心的洞察:\nDPO で重要なのは、選択（chosen）と棄却（rejected）の「品質差（デルタ）」です。絶対的な品質よりも、相対的な品質差が学習に重要です。\n実験結果:\n\n\n\n設定\nMATH スコア\n変化\n\n\n\n\nBase モデル\n45.2\n-\n\n\nQwen3-32B で SFT\n43.8\n-1.4\n\n\nQwen3-32B (chosen) vs Qwen3-0.6B (rejected) で DPO\n52.3\n+7.1\n\n\n\n同じ Qwen3-32B の出力でも、SFT では性能が低下するのに対し、弱いモデルとペアにして DPO すると大幅に改善します。\n応用:\nこの知見は、Dolci Think DPO と Dolci Instruct DPO の両方で活用されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/08-dolci-dataset.html#まとめ",
    "href": "ja/olmo-3/08-dolci-dataset.html#まとめ",
    "title": "Dolci: Post-training データスイート",
    "section": "",
    "text": "Dolci は、Olmo 3 の Post-training に使用される包括的なデータスイートです。Think、Instruct、RL-Zero という 3 つの異なるモデルバリエーションをサポートし、SFT、DPO、RL の各訓練ステージに最適化されたデータを提供しています。\n主な貢献:\n\n完全オープン: すべてのデータソースとパイプラインを公開\n高品質: 強力なモデルによる生成と厳格なフィルタリング\n多様性: Math, Code, Chat, IF, Safety をカバー\n段階的訓練: SFT、DPO、RL に最適化\n研究価値: RL-Zero により完全オープンな RL ベンチマークを提供\n\nDolci は、完全にオープンな Post-training データセットとして、研究者が再現性の高い研究を行い、任意のステージから介入・カスタマイズできるようにしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Dolci データスイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "+------------------------------------------------------------------+\n|                      olmOCR science PDFs                         |\n+------------------------------------------------------------------+\n\n\nolmOCR science PDFs は、学術 PDF 文書から構築された新しいデータソースです。\n\npeS2o の代替: Semantic Scholar の既存データセット peS2o を置き換えるために開発\nAI2Bot クローラー: robots.txt に準拠した独自のウェブクローラーを使用\n規模: 初期収集で 238M（2億3800万）の PDF 文書を取得\n\n\n\n\nPDF からプレーンテキストへの変換には、以下の 2 段階のアプローチを採用しています。\n\n第 1 段階: olmOCR（AI2 の OCR モデル）による PDF → プレーンテキスト変換\n第 2 段階: Poppler の pdftotext をフォールバックとして使用\n言語検出: Lingua による言語識別（英語文書のみを抽出）\n\n\n\n\nolmOCR science PDFs は、複数段階のフィルタリングプロセスを経て構築されています。\nInitial Collection:     238M PDF documents\n         |\n         v\nLanguage Detection &    160M documents\nSpam Filtering\n         |\n         v\nFuzzy Deduplication:    156M documents (2.3% reduction)\n         |\n         v\nPII Filtering:          148M documents (4.9% reduction)\n         |\n         v\nHeuristic Filtering:    108M documents (final)\n各段階での削減率は以下の通りです。\n\n言語検出・スパムフィルタ: 238M → 160M（約 33% 削減）\nFuzzy deduplication: 160M → 156M（2.3% 削減）\nPII フィルタリング: 156M → 148M（4.9% 削減）\nヒューリスティックフィルタ: 148M → 108M（約 27% 削減）\n\n\n\n\n個人情報（PII: Personally Identifiable Information）を含む文書を除外するため、文書タイプごとの判定を実施しました。\n\n使用モデル: Gemma 3 12B および Gemma 3 4B\n判定基準: 公開意図のない文書かどうかを分類\n対象例: 個人の医療記録、学生の成績表、履歴書など\n削減効果: 4.9% の文書を除外\n\nこのフィルタリングにより、プライバシーを尊重したデータセットを構築しています。\n\n\n\nolmOCR science PDFs は、長文脈研究のための最大のオープンコレクションです。\n文書長別の統計は以下の通りです。\n\n8K+ トークン: 22.3M 文書（640B トークン）\n32K+ トークン: 4.5M 文書（380B トークン）\n\nこれらの長文文書は、長文脈モデリングの研究に特に有用です。\n\n\n\n\n\n\nNote長文脈データの重要性\n\n\n\n8K トークン以上の文書が 22.3M、32K トークン以上の文書が 4.5M も含まれており、長文脈を扱う言語モデルのトレーニングに最適なデータセットとなっています。\n\n\n\n\n\n最終的な文書コレクションは、WebOrganizer を用いて 24 の学術トピック に分類されています。\n\n分類手法: WebOrganizer（AI2 のドメイン分類器）\nトピック数: 24 カテゴリ\n用途: データ分析、トピック別のサンプリング、ドメイン適応研究\n\n\n\n\n\n\n\nTippeS2o との違い\n\n\n\nolmOCR science PDFs は、peS2o と比較して以下の利点があります。\n\nより新しいクローリングデータ\nより厳格な PII フィルタリング\n長文文書が豊富\nrobots.txt に準拠した倫理的なクローリング",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#概要",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#概要",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、学術 PDF 文書から構築された新しいデータソースです。\n\npeS2o の代替: Semantic Scholar の既存データセット peS2o を置き換えるために開発\nAI2Bot クローラー: robots.txt に準拠した独自のウェブクローラーを使用\n規模: 初期収集で 238M（2億3800万）の PDF 文書を取得",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#olmocr-テキスト抽出プロセス",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#olmocr-テキスト抽出プロセス",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "PDF からプレーンテキストへの変換には、以下の 2 段階のアプローチを採用しています。\n\n第 1 段階: olmOCR（AI2 の OCR モデル）による PDF → プレーンテキスト変換\n第 2 段階: Poppler の pdftotext をフォールバックとして使用\n言語検出: Lingua による言語識別（英語文書のみを抽出）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#データ処理パイプライン",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#データ処理パイプライン",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、複数段階のフィルタリングプロセスを経て構築されています。\nInitial Collection:     238M PDF documents\n         |\n         v\nLanguage Detection &    160M documents\nSpam Filtering\n         |\n         v\nFuzzy Deduplication:    156M documents (2.3% reduction)\n         |\n         v\nPII Filtering:          148M documents (4.9% reduction)\n         |\n         v\nHeuristic Filtering:    108M documents (final)\n各段階での削減率は以下の通りです。\n\n言語検出・スパムフィルタ: 238M → 160M（約 33% 削減）\nFuzzy deduplication: 160M → 156M（2.3% 削減）\nPII フィルタリング: 156M → 148M（4.9% 削減）\nヒューリスティックフィルタ: 148M → 108M（約 27% 削減）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#pii-フィルタリング",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#pii-フィルタリング",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "個人情報（PII: Personally Identifiable Information）を含む文書を除外するため、文書タイプごとの判定を実施しました。\n\n使用モデル: Gemma 3 12B および Gemma 3 4B\n判定基準: 公開意図のない文書かどうかを分類\n対象例: 個人の医療記録、学生の成績表、履歴書など\n削減効果: 4.9% の文書を除外\n\nこのフィルタリングにより、プライバシーを尊重したデータセットを構築しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#データ規模と特徴",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#データ規模と特徴",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "olmOCR science PDFs は、長文脈研究のための最大のオープンコレクションです。\n文書長別の統計は以下の通りです。\n\n8K+ トークン: 22.3M 文書（640B トークン）\n32K+ トークン: 4.5M 文書（380B トークン）\n\nこれらの長文文書は、長文脈モデリングの研究に特に有用です。\n\n\n\n\n\n\nNote長文脈データの重要性\n\n\n\n8K トークン以上の文書が 22.3M、32K トークン以上の文書が 4.5M も含まれており、長文脈を扱う言語モデルのトレーニングに最適なデータセットとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/06-olmocr-science-pdfs.html#weborganizer-による分類",
    "href": "ja/olmo-3/06-olmocr-science-pdfs.html#weborganizer-による分類",
    "title": "olmOCR science PDFs",
    "section": "",
    "text": "最終的な文書コレクションは、WebOrganizer を用いて 24 の学術トピック に分類されています。\n\n分類手法: WebOrganizer（AI2 のドメイン分類器）\nトピック数: 24 カテゴリ\n用途: データ分析、トピック別のサンプリング、ドメイン適応研究\n\n\n\n\n\n\n\nTippeS2o との違い\n\n\n\nolmOCR science PDFs は、peS2o と比較して以下の利点があります。\n\nより新しいクローリングデータ\nより厳格な PII フィルタリング\n長文文書が豊富\nrobots.txt に準拠した倫理的なクローリング",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "olmOCR science PDFs"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html",
    "href": "ja/olmo-3/04-long-context-extension.html",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "OLMo 3 では、ベースモデル（8K トークンのコンテキスト長）から 65K トークンへと長文脈拡張を実施しました。この拡張により、長文書の理解や複雑なタスクに対応できるモデルとなっています。\n\n\n長文脈拡張では、以下の規模でトレーニングを行いました。\n\n7B モデル: 50B トークンで学習\n32B モデル: 100B トークンで学習\nコンテキスト長: 8K トークン → 65K トークンへ拡張\n\nこの拡張は、特定のデータミックス（Dolma 3 Longmino Mix）と複数の技術的手法を組み合わせて実現されています。\n\n\n\n長文脈拡張に使用されたデータセットは、以下の3つの主要コンポーネントから構成されています（Table 11 参照）。\n\n\nPDF から抽出された長文書データで、様々な長さのバケットに分類されています。\n\n\n\nLength Bucket\nDocuments\nTokens\n\n\n\n\n8K-16K\n1,090,349\n13.1B\n\n\n16K-32K\n508,354\n11.0B\n\n\n32K-64K\n142,983\n6.1B\n\n\n64K-128K\n54,992\n4.5B\n\n\n128K-256K\n20,893\n3.2B\n\n\n256K-512K\n8,130\n2.4B\n\n\n512K-1M\n3,394\n1.7B\n\n\n1M+\n1,172\n1.8B\n\n\n\n\n\n\n長文脈能力を強化するための合成的に生成されたデータです。\n\nCWE (Common Word Extraction): 7.4B トークン\nREX (Rewriting Expressions): 1.5B トークン\n\n\n\n\nMidtraining phase で使用されたデータの 66% を含めることで、一般的な能力の維持を図っています。\n\nMidtraining data mix: 34.9B トークン（66% の割合）\n\n\n\n\n\n長文脈拡張の実現には、Figure 13 に示される 5つの技術コンポーネントが使用されています。\n\n\nRoPE (Rotary Position Embedding) を拡張するために YaRN (Yet another RoPE extensioN) を採用しました。\n\n\n\n\n\n\nNoteYaRN の適用範囲\n\n\n\nYaRN は Full attention layers のみに適用されています。Sliding window attention layers では、元の RoPE 設定を維持しています。\n\n\n\n\n\n効率的なトレーニングのために、複数の文書を1つのシーケンスにパッキングします。\n\n手法: Best-fit packing アルゴリズム\n目的: GPU メモリの効率的な利用とトレーニングスループットの向上\n\n\n\n\nパッキングされた文書間で情報が漏れないように、文書内マスキングを適用します。\n\n\n\n\n\n\nImportantマスキングの重要性\n\n\n\nDocument packing を使用する場合、異なる文書間でアテンションが計算されないようにマスキングすることが重要です。これにより、各文書が独立して処理されます。\n\n\n\n\n\n複数のチェックポイントを平均化することで、モデルの安定性と性能を向上させます。\n\n手法: 異なるトレーニングステップで保存されたチェックポイントの重みを平均化\n効果: より汎用的で安定したモデルの獲得\n\n\n\n\n長い拡張により多くのトークンを割り当てることで、より良い性能を達成します。\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\n\n\n\n\n長文脈能力を効果的に向上させるため、2つの合成データ生成手法を使用しています。\n\n\n文書内で共通して出現する単語を抽出するタスクを生成します。これにより、モデルは長文書全体を参照する能力を獲得します。\n\n\n\nREX では、12 種類の vignettes（小場面）を用いて、様々な長文脈タスクをシミュレートします。\n\n\n\n\n\n\nTipREX の vignettes\n\n\n\nREX は多様なタスク形式を含み、要約、情報抽出、質問応答など、実際のユースケースに近い合成データを生成します。これにより、モデルは様々な長文脈タスクに適応できるようになります。\n\n\n\n\n\n\n長文脈拡張モデルの性能は、RULER（開発スイート）と HELMET（Held-out 評価）で評価されています（Table 12 参照）。\n\n\nRULER は、様々なコンテキスト長での性能を測定する開発用評価スイートです。\n\n\n\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\n\n\n\n\nOLMo 3 7B\n92.7\n91.7\n88.1\n82.5\n70.3\n-\n85.1\n\n\nOLMo 3 32B\n95.8\n94.9\n92.8\n89.4\n82.1\n-\n91.0\n\n\n\n\n\n\nHELMET は、実際のユースケースに近い held-out 評価セットです。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\nOLMo 3 の長文脈モデルは、同規模の他のオープンモデルと比較して競争力のある性能を示しています。特に、32B モデルは多くのベンチマークで高いスコアを達成しています。\n\n\n\n\n\n長文脈拡張の評価から、以下の知見が得られました。\n\nToken budget の効果: より多くのトークンで学習することで、長文脈能力が大幅に向上\n合成データの重要性: CWE と REX の合成データが、実際のタスクでの性能向上に寄与\nModel souping の効果: 複数チェックポイントの平均化により、安定した性能を獲得\n\nこれらの技術を組み合わせることで、OLMo 3 は 65K トークンの長文脈を効果的に扱えるモデルとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#拡張の概要",
    "href": "ja/olmo-3/04-long-context-extension.html#拡張の概要",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張では、以下の規模でトレーニングを行いました。\n\n7B モデル: 50B トークンで学習\n32B モデル: 100B トークンで学習\nコンテキスト長: 8K トークン → 65K トークンへ拡張\n\nこの拡張は、特定のデータミックス（Dolma 3 Longmino Mix）と複数の技術的手法を組み合わせて実現されています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#dolma-3-longmino-mix-の構成",
    "href": "ja/olmo-3/04-long-context-extension.html#dolma-3-longmino-mix-の構成",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張に使用されたデータセットは、以下の3つの主要コンポーネントから構成されています（Table 11 参照）。\n\n\nPDF から抽出された長文書データで、様々な長さのバケットに分類されています。\n\n\n\nLength Bucket\nDocuments\nTokens\n\n\n\n\n8K-16K\n1,090,349\n13.1B\n\n\n16K-32K\n508,354\n11.0B\n\n\n32K-64K\n142,983\n6.1B\n\n\n64K-128K\n54,992\n4.5B\n\n\n128K-256K\n20,893\n3.2B\n\n\n256K-512K\n8,130\n2.4B\n\n\n512K-1M\n3,394\n1.7B\n\n\n1M+\n1,172\n1.8B\n\n\n\n\n\n\n長文脈能力を強化するための合成的に生成されたデータです。\n\nCWE (Common Word Extraction): 7.4B トークン\nREX (Rewriting Expressions): 1.5B トークン\n\n\n\n\nMidtraining phase で使用されたデータの 66% を含めることで、一般的な能力の維持を図っています。\n\nMidtraining data mix: 34.9B トークン（66% の割合）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#主要技術コンポーネント",
    "href": "ja/olmo-3/04-long-context-extension.html#主要技術コンポーネント",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張の実現には、Figure 13 に示される 5つの技術コンポーネントが使用されています。\n\n\nRoPE (Rotary Position Embedding) を拡張するために YaRN (Yet another RoPE extensioN) を採用しました。\n\n\n\n\n\n\nNoteYaRN の適用範囲\n\n\n\nYaRN は Full attention layers のみに適用されています。Sliding window attention layers では、元の RoPE 設定を維持しています。\n\n\n\n\n\n効率的なトレーニングのために、複数の文書を1つのシーケンスにパッキングします。\n\n手法: Best-fit packing アルゴリズム\n目的: GPU メモリの効率的な利用とトレーニングスループットの向上\n\n\n\n\nパッキングされた文書間で情報が漏れないように、文書内マスキングを適用します。\n\n\n\n\n\n\nImportantマスキングの重要性\n\n\n\nDocument packing を使用する場合、異なる文書間でアテンションが計算されないようにマスキングすることが重要です。これにより、各文書が独立して処理されます。\n\n\n\n\n\n複数のチェックポイントを平均化することで、モデルの安定性と性能を向上させます。\n\n手法: 異なるトレーニングステップで保存されたチェックポイントの重みを平均化\n効果: より汎用的で安定したモデルの獲得\n\n\n\n\n長い拡張により多くのトークンを割り当てることで、より良い性能を達成します。\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#合成データ生成パイプライン",
    "href": "ja/olmo-3/04-long-context-extension.html#合成データ生成パイプライン",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈能力を効果的に向上させるため、2つの合成データ生成手法を使用しています。\n\n\n文書内で共通して出現する単語を抽出するタスクを生成します。これにより、モデルは長文書全体を参照する能力を獲得します。\n\n\n\nREX では、12 種類の vignettes（小場面）を用いて、様々な長文脈タスクをシミュレートします。\n\n\n\n\n\n\nTipREX の vignettes\n\n\n\nREX は多様なタスク形式を含み、要約、情報抽出、質問応答など、実際のユースケースに近い合成データを生成します。これにより、モデルは様々な長文脈タスクに適応できるようになります。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/04-long-context-extension.html#評価結果",
    "href": "ja/olmo-3/04-long-context-extension.html#評価結果",
    "title": "Long-context Extension: 長文脈拡張",
    "section": "",
    "text": "長文脈拡張モデルの性能は、RULER（開発スイート）と HELMET（Held-out 評価）で評価されています（Table 12 参照）。\n\n\nRULER は、様々なコンテキスト長での性能を測定する開発用評価スイートです。\n\n\n\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\n\n\n\n\nOLMo 3 7B\n92.7\n91.7\n88.1\n82.5\n70.3\n-\n85.1\n\n\nOLMo 3 32B\n95.8\n94.9\n92.8\n89.4\n82.1\n-\n91.0\n\n\n\n\n\n\nHELMET は、実際のユースケースに近い held-out 評価セットです。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\nOLMo 3 の長文脈モデルは、同規模の他のオープンモデルと比較して競争力のある性能を示しています。特に、32B モデルは多くのベンチマークで高いスコアを達成しています。\n\n\n\n\n\n長文脈拡張の評価から、以下の知見が得られました。\n\nToken budget の効果: より多くのトークンで学習することで、長文脈能力が大幅に向上\n合成データの重要性: CWE と REX の合成データが、実際のタスクでの性能向上に寄与\nModel souping の効果: 複数チェックポイントの平均化により、安定した性能を獲得\n\nこれらの技術を組み合わせることで、OLMo 3 は 65K トークンの長文脈を効果的に扱えるモデルとなっています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Long-context Extension"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html",
    "href": "ja/olmo-3/02-olmobaseeval.html",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、Base モデル（事前学習済みモデル）の性能を効率的に評価するために設計されたベンチマークスイートである。従来の評価手法が抱える課題を解決し、小規模モデルでも信頼性の高い評価を可能にする。\n\n\n言語モデルの開発プロセスでは、学習途中のモデルを頻繁に評価する必要がある。しかし、既存の評価ベンチマークの多くは完全に学習されたモデルや、指示チューニングされた大規模モデルを対象としており、小規模な Base モデルの評価には適していない。\nOlmoBaseEval は、以下の目的で開発された。\n\n小規模モデルでも意味のある評価結果を提供\n学習途中のモデルの進捗を正確に追跡\n計算コストを抑えつつ高い評価精度を実現\nモデルの基礎能力を多角的に測定\n\n\n\n\n小規模な Base モデルの評価には、主に3つの課題が存在する。\n\n\n\n\n\n\nNote課題1: ランダムチャンス性能\n\n\n\n小規模モデルは多くのタスクでランダム選択と変わらない性能しか示さない。例えば、4択問題でモデルが学習していない場合、正答率は25%前後に留まり、実質的な能力測定ができない。\n\n\n\n\n\n\n\n\nNote課題2: スコア差の小ささ\n\n\n\n学習が進んでもスコアの変化が微小であり、改善の有無を判断しにくい。統計的に有意な差を検出するには、非常に多くのサンプルが必要となる。\n\n\n\n\n\n\n\n\nNote課題3: 評価の不安定性\n\n\n\nタスクによってはノイズが大きく、同じモデルでも評価のたびに結果が変動する。これにより、真の性能向上とノイズによる変動を区別できない。\n\n\n\n\n\nOlmoBaseEval は、上記の課題に対して3つのアプローチで対処する。\n\n\n類似した能力を評価する複数のタスクを集約することで、評価の信頼性を向上させる。\n個別のタスクではノイズが大きくても、同じ能力を測定する複数のタスクの結果を統合することで、より安定した評価指標が得られる。これにより、モデルの特定の能力（例: 推論能力、言語理解能力）を正確に測定できる。\nタスククラスタリングのメリット。\n\n単一タスクのノイズを平均化\n能力の多面的な評価\n評価の再現性向上\n\n\n\n\n小規模モデルに適した評価指標を導入する。\n従来の正答率ベースの評価では、小規模モデルの能力を捉えきれない。プロキシメトリックは、正答/誤答の二値ではなく、モデルの出力分布や確信度などを考慮した連続的な指標を用いる。\n代表的なプロキシメトリック。\n\nMasked Perplexity: 特定のトークンに対するモデルの予測確率を測定\nProbability-based metrics: 正解選択肢への確率割り当てを評価\nCalibration metrics: モデルの確信度と実際の正答率の整合性を測定\n\nこれらの指標は、モデルが完全に学習していない段階でも、学習の進捗を捉えることができる。\n\n\n\n評価タスクのシグナルノイズ比を分析し、信頼性の高いタスクのみを選定する。\nすべてのタスクが等しく有用というわけではない。OlmoBaseEval では、各タスクのシグナルノイズ比を測定し、小規模モデルでも安定した結果を示すタスクを優先的に採用する。\nシグナルノイズ比の分析手法。\n\n複数のモデルサイズでの評価結果の一貫性を確認\n同一モデルの複数回評価での分散を測定\nスケーリング則との整合性を検証\n\n\n\n\n\nOlmoBaseEval は、以下の4つの新規ベンチマークを含む。\n\n\n基本的な言語理解と推論能力を測定する6つのタスク。\n\nReading Comprehension: 短文の理解と情報抽出\nFact Recall: 基本的な知識の記憶\nSimple Logic: 基礎的な論理推論\nPattern Recognition: パターンの認識と予測\nBasic Math: 算数レベルの数値計算\nCommon Sense: 常識的な推論\n\nこれらのタスクは、小規模モデルでもランダムチャンスを超える性能を示すよう設計されている。\n\n\n\n生成タスクを多肢選択形式に変換した5つのタスク。\n従来、生成タスクは Base モデルの評価に適さないとされてきた。Gen2MC は、生成の品質を多肢選択形式で評価することで、この問題を解決する。\n\nSummarization: 要約の適切性を選択肢から判定\nTranslation: 翻訳の正確性を評価\nParaphrasing: 言い換えの妥当性を測定\nQuestion Generation: 質問生成の質を評価\nTitle Generation: タイトル生成の適切性を判定\n\nこの形式により、生成タスクでも確率ベースの評価が可能になる。\n\n\n\n多言語プログラミングベンチマーク（17言語対応）。\nMBPP（Mostly Basic Programming Problems）を17の自然言語に翻訳したベンチマーク。モデルの多言語理解能力とコーディング能力を同時に評価する。\n対応言語の例。\n\nヨーロッパ言語: 英語、スペイン語、フランス語、ドイツ語\nアジア言語: 日本語、中国語、韓国語、ヒンディー語\nその他: アラビア語、ロシア語、ポルトガル語\n\n各言語で同一の問題セットを評価することで、言語間の性能差を分析できる。\n\n\n\nマスクされたトークンの予測精度を測定する評価手法。\n特定の重要なトークン（名詞、動詞など）をマスクし、モデルがそれらをどの程度正確に予測できるかを評価する。この手法は、小規模モデルでも連続的なスコアを提供し、学習の進捗を細かく追跡できる。\nMasked Perplexity の特徴。\n\n連続的なスコアリング（二値判定ではない）\n文脈理解能力の直接的な測定\n計算コストが低い\n学習初期段階から有意な差を検出\n\n\n\n\n\nOlmoBaseEval を用いた評価は、以下の流れで実施される。\n\nベースライン測定: 学習開始前のランダム初期化モデルを評価\n定期評価: 学習ステップごとに自動評価を実行\nクラスタ分析: タスククラスタごとの性能変化を追跡\nスケーリング予測: 小規模モデルの結果から大規模モデルの性能を推定\n\n\n\n\n\n\n\nTip評価の頻度\n\n\n\n小規模モデル（1B パラメータ未満）では、数百ステップごとの評価が推奨される。大規模モデルでは、計算コストを考慮して評価頻度を調整する。\n\n\n\n\n\nOlmoBaseEval は、スケーリング則の分析と組み合わせることで、さらなる洞察を提供する。\n小規模モデルでの評価結果をもとに、より大規模なモデルの性能を予測できる。これにより、大規模モデルを実際に学習する前に、学習戦略の有効性を検証できる。\n予測精度の向上には、以下の要素が重要である。\n\n複数のモデルサイズでの評価データ\nタスククラスタごとのスケーリング曲線\n学習データ量との相関分析\n\n\n\n\nOlmoBaseEval は、従来の評価スイートと比較して、大幅に計算コストを削減する。\n+----------------------------------+\n| Efficiency Comparison            |\n+----------------------------------+\n| Traditional: 100 GPU hours       |\n| OlmoBaseEval: 10 GPU hours       |\n| Reduction: 90%                   |\n+----------------------------------+\n効率化の要因。\n\nタスク数の最適化（重複の排除）\nプロキシメトリックによる評価時間短縮\nバッチ処理の最適化\n\n\n\n\nOlmoBaseEval は、Base モデルの評価における以下の革新をもたらす。\n\n小規模モデルでの信頼性: ランダムチャンス性能を超える評価を実現\n効率性: 計算コストを大幅に削減\n多角的評価: タスククラスタリングによる能力の包括的測定\n予測可能性: スケーリング分析との統合\n\nこの評価スイートにより、モデル開発の初期段階から、データ効率的かつ計算効率的な学習戦略の検証が可能になる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#背景と目的",
    "href": "ja/olmo-3/02-olmobaseeval.html#背景と目的",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "言語モデルの開発プロセスでは、学習途中のモデルを頻繁に評価する必要がある。しかし、既存の評価ベンチマークの多くは完全に学習されたモデルや、指示チューニングされた大規模モデルを対象としており、小規模な Base モデルの評価には適していない。\nOlmoBaseEval は、以下の目的で開発された。\n\n小規模モデルでも意味のある評価結果を提供\n学習途中のモデルの進捗を正確に追跡\n計算コストを抑えつつ高い評価精度を実現\nモデルの基礎能力を多角的に測定",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#従来手法の課題",
    "href": "ja/olmo-3/02-olmobaseeval.html#従来手法の課題",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "小規模な Base モデルの評価には、主に3つの課題が存在する。\n\n\n\n\n\n\nNote課題1: ランダムチャンス性能\n\n\n\n小規模モデルは多くのタスクでランダム選択と変わらない性能しか示さない。例えば、4択問題でモデルが学習していない場合、正答率は25%前後に留まり、実質的な能力測定ができない。\n\n\n\n\n\n\n\n\nNote課題2: スコア差の小ささ\n\n\n\n学習が進んでもスコアの変化が微小であり、改善の有無を判断しにくい。統計的に有意な差を検出するには、非常に多くのサンプルが必要となる。\n\n\n\n\n\n\n\n\nNote課題3: 評価の不安定性\n\n\n\nタスクによってはノイズが大きく、同じモデルでも評価のたびに結果が変動する。これにより、真の性能向上とノイズによる変動を区別できない。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#解決策の3つの柱",
    "href": "ja/olmo-3/02-olmobaseeval.html#解決策の3つの柱",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、上記の課題に対して3つのアプローチで対処する。\n\n\n類似した能力を評価する複数のタスクを集約することで、評価の信頼性を向上させる。\n個別のタスクではノイズが大きくても、同じ能力を測定する複数のタスクの結果を統合することで、より安定した評価指標が得られる。これにより、モデルの特定の能力（例: 推論能力、言語理解能力）を正確に測定できる。\nタスククラスタリングのメリット。\n\n単一タスクのノイズを平均化\n能力の多面的な評価\n評価の再現性向上\n\n\n\n\n小規模モデルに適した評価指標を導入する。\n従来の正答率ベースの評価では、小規模モデルの能力を捉えきれない。プロキシメトリックは、正答/誤答の二値ではなく、モデルの出力分布や確信度などを考慮した連続的な指標を用いる。\n代表的なプロキシメトリック。\n\nMasked Perplexity: 特定のトークンに対するモデルの予測確率を測定\nProbability-based metrics: 正解選択肢への確率割り当てを評価\nCalibration metrics: モデルの確信度と実際の正答率の整合性を測定\n\nこれらの指標は、モデルが完全に学習していない段階でも、学習の進捗を捉えることができる。\n\n\n\n評価タスクのシグナルノイズ比を分析し、信頼性の高いタスクのみを選定する。\nすべてのタスクが等しく有用というわけではない。OlmoBaseEval では、各タスクのシグナルノイズ比を測定し、小規模モデルでも安定した結果を示すタスクを優先的に採用する。\nシグナルノイズ比の分析手法。\n\n複数のモデルサイズでの評価結果の一貫性を確認\n同一モデルの複数回評価での分散を測定\nスケーリング則との整合性を検証",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#新しいベンチマーク",
    "href": "ja/olmo-3/02-olmobaseeval.html#新しいベンチマーク",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、以下の4つの新規ベンチマークを含む。\n\n\n基本的な言語理解と推論能力を測定する6つのタスク。\n\nReading Comprehension: 短文の理解と情報抽出\nFact Recall: 基本的な知識の記憶\nSimple Logic: 基礎的な論理推論\nPattern Recognition: パターンの認識と予測\nBasic Math: 算数レベルの数値計算\nCommon Sense: 常識的な推論\n\nこれらのタスクは、小規模モデルでもランダムチャンスを超える性能を示すよう設計されている。\n\n\n\n生成タスクを多肢選択形式に変換した5つのタスク。\n従来、生成タスクは Base モデルの評価に適さないとされてきた。Gen2MC は、生成の品質を多肢選択形式で評価することで、この問題を解決する。\n\nSummarization: 要約の適切性を選択肢から判定\nTranslation: 翻訳の正確性を評価\nParaphrasing: 言い換えの妥当性を測定\nQuestion Generation: 質問生成の質を評価\nTitle Generation: タイトル生成の適切性を判定\n\nこの形式により、生成タスクでも確率ベースの評価が可能になる。\n\n\n\n多言語プログラミングベンチマーク（17言語対応）。\nMBPP（Mostly Basic Programming Problems）を17の自然言語に翻訳したベンチマーク。モデルの多言語理解能力とコーディング能力を同時に評価する。\n対応言語の例。\n\nヨーロッパ言語: 英語、スペイン語、フランス語、ドイツ語\nアジア言語: 日本語、中国語、韓国語、ヒンディー語\nその他: アラビア語、ロシア語、ポルトガル語\n\n各言語で同一の問題セットを評価することで、言語間の性能差を分析できる。\n\n\n\nマスクされたトークンの予測精度を測定する評価手法。\n特定の重要なトークン（名詞、動詞など）をマスクし、モデルがそれらをどの程度正確に予測できるかを評価する。この手法は、小規模モデルでも連続的なスコアを提供し、学習の進捗を細かく追跡できる。\nMasked Perplexity の特徴。\n\n連続的なスコアリング（二値判定ではない）\n文脈理解能力の直接的な測定\n計算コストが低い\n学習初期段階から有意な差を検出",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#評価の実践",
    "href": "ja/olmo-3/02-olmobaseeval.html#評価の実践",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval を用いた評価は、以下の流れで実施される。\n\nベースライン測定: 学習開始前のランダム初期化モデルを評価\n定期評価: 学習ステップごとに自動評価を実行\nクラスタ分析: タスククラスタごとの性能変化を追跡\nスケーリング予測: 小規模モデルの結果から大規模モデルの性能を推定\n\n\n\n\n\n\n\nTip評価の頻度\n\n\n\n小規模モデル（1B パラメータ未満）では、数百ステップごとの評価が推奨される。大規模モデルでは、計算コストを考慮して評価頻度を調整する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#スケーリング分析との統合",
    "href": "ja/olmo-3/02-olmobaseeval.html#スケーリング分析との統合",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、スケーリング則の分析と組み合わせることで、さらなる洞察を提供する。\n小規模モデルでの評価結果をもとに、より大規模なモデルの性能を予測できる。これにより、大規模モデルを実際に学習する前に、学習戦略の有効性を検証できる。\n予測精度の向上には、以下の要素が重要である。\n\n複数のモデルサイズでの評価データ\nタスククラスタごとのスケーリング曲線\n学習データ量との相関分析",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#計算効率",
    "href": "ja/olmo-3/02-olmobaseeval.html#計算効率",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、従来の評価スイートと比較して、大幅に計算コストを削減する。\n+----------------------------------+\n| Efficiency Comparison            |\n+----------------------------------+\n| Traditional: 100 GPU hours       |\n| OlmoBaseEval: 10 GPU hours       |\n| Reduction: 90%                   |\n+----------------------------------+\n効率化の要因。\n\nタスク数の最適化（重複の排除）\nプロキシメトリックによる評価時間短縮\nバッチ処理の最適化",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/02-olmobaseeval.html#まとめ",
    "href": "ja/olmo-3/02-olmobaseeval.html#まとめ",
    "title": "OlmoBaseEval: Base モデル評価スイート",
    "section": "",
    "text": "OlmoBaseEval は、Base モデルの評価における以下の革新をもたらす。\n\n小規模モデルでの信頼性: ランダムチャンス性能を超える評価を実現\n効率性: 計算コストを大幅に削減\n多角的評価: タスククラスタリングによる能力の包括的測定\n予測可能性: スケーリング分析との統合\n\nこの評価スイートにより、モデル開発の初期段階から、データ効率的かつ計算効率的な学習戦略の検証が可能になる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "OlmoBaseEval: 評価スイート"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html",
    "href": "ja/olmo-3/00-overview.html",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n\n完全オープン: 学習データ、コード、中間チェックポイントをすべて公開\n多様な能力: 長文脈推論、関数呼び出し、コーディング、指示追従、一般的なチャット、知識リコール\nフラッグシップモデル: Olmo 3.1 Think 32B は、これまでに公開された最強の完全オープン思考型モデル\n\nモデルバリエーション:\n\nOlmo 3 Base: 基盤モデル（7B, 32B）\nOlmo 3 Think: 段階的推論を行う思考型モデル\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練したモデル\n\n論文: arXiv:2512.13961\n\n\n\nOlmo 3 の開発は、Base Model Training（基盤モデル訓練）と Post-training（後訓練）の 2 つの主要ステージに分かれています。\n┌───────────────────────────────────────────────────────────────┐\n│                    Base Model Training                        │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 1: Pretraining (5.9T tokens)                           │\n│    → Dolma 3 Mix (Web, PDFs, Code, etc.)                     │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 2: Midtraining (100B tokens)                           │\n│    → Dolma 3 Dolmino Mix (Math, Code, QA, etc.)              │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 3: Long-context Extension (50-100B tokens)             │\n│    → Dolma 3 Longmino Mix (Long PDFs + Midtrain data)        │\n└───────────────────────────────────────────────────────────────┘\n                            ↓\n                     Olmo 3 Base\n                            ↓\n┌───────────────────────────────────────────────────────────────┐\n│                      Post-training                            │\n├───────────────────────────────────────────────────────────────┤\n│  Path 1: Olmo 3 Think                                         │\n│    SFT → DPO (Delta Learning) → RLVR (OlmoRL)               │\n├───────────────────────────────────────────────────────────────┤\n│  Path 2: Olmo 3 Instruct                                      │\n│    SFT → DPO → RLVR                                         │\n├───────────────────────────────────────────────────────────────┤\n│  Path 3: Olmo 3 RL-Zero                                       │\n│    Base → RLVR (from scratch)                                │\n└───────────────────────────────────────────────────────────────┘\n\n\n\n\n\nOlmo 3 Base は、Dolma 3 Mix と呼ばれる 5.9 兆トークンの多様なデータで事前学習されます。\n\n詳細: Dolma 3 データセット\n\n主な革新点:\n\n高速でスケーラブルなグローバル重複排除: 兆トークンスケールでの新しいツール\n\n詳細: Deduplication（重複排除）\n\nolmOCR science PDFs: 学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n\n詳細: olmOCR science PDFs\n\nデータミキシングの新手法: Token-constrained mixing と Quality-aware upsampling\n\n詳細: Data Mixing 手法\n\n\nデータソース:\n\nWeb ページ\n学術 PDF（olmOCR science PDFs）\nコードリポジトリ\n数学データ\nその他の多様なソース\n\n\n\n\n100 億トークンの Dolma 3 Dolmino Mix で中間訓練を実施します。このステージの目的は、コード、数学、一般知識 QA などの重要な能力を強化することです。\n\n詳細: Midtraining（中間訓練）\n\n革新的な手法:\n\n2 部構成のフレームワーク:\n\n個別データソースへの軽量な分散フィードバックループ\n候補ミックスを評価する集中統合テスト\n\nPost-training の下準備: 指示データと思考トレースを意図的に含めることで、後訓練の基盤を構築\n\n評価スイート: OlmoBaseEval\n\n詳細: OlmoBaseEval（評価スイート）\n\n\n\n\nOlmo 3 は、最大 65K トークンのコンテキストをサポートする長文脈能力を持ちます。7B モデルは 50B トークン、32B モデルは 100B トークンの Dolma 3 Longmino Mix で拡張訓練されます。\n\n詳細: Long-context Extension（長文脈拡張）\n\n主要技術:\n\nRoPE 拡張: YaRN を使用した位置エンコーディングの拡張\nDocument packing: Best-fit packing で効率的な長文書の配置\nIntra-document masking: 同一文書内のトークンのみに attention\nModel souping: 複数チェックポイントの平均化\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n\n\n\nOlmo 3 Base は、32B パラメータスケールで 最強の完全オープンモデルです：\n\n完全オープンモデル: Stanford Marin 32B、Apertus 70B を上回る\n数学とコード: 他の完全オープン 32B モデルに対して 2 桁の改善\n長文脈性能: Qwen 2.5 32B、Mistral Small 3.1 24B、Gemma 3 27B に匹敵\n\n\n\n\n\nBase モデルから 3 つのバリエーションを開発します。\n\n\nOlmo 3 Think は、最終回答を生成する前に段階的推論を行い、中間的な思考トレースを生成するように訓練されています。\n訓練パイプライン:\n\nSFT (Supervised Finetuning): Dolci Think SFT で思考トレースを学習\nDPO (Direct Preference Optimization): Delta Learning による選好調整\n\n詳細: Delta Learning\n\nRLVR (Reinforcement Learning with Verifiable Rewards): OlmoRL による強化学習\n\n詳細: OlmoRL / GRPO\n\n\n\n詳細: Dolci データセット\n\n成果:\n\nOlmo 3.1 Think 32B: 最強の完全オープン思考型モデル\nQwen 2.5 32B、Gemma 2/3 27B、DeepSeek R1 32B を上回る\nQwen 3 32B に迫る性能（訓練トークン数は 6 分の 1）\n\n主要ベンチマーク結果 (Olmo 3.1 Think 32B):\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4\n\n\n\n\n\n\nOlmo 3 Instruct は、内部的な思考トレースを生成せずに、効率的で有用な応答を生成するように訓練されています。\n特徴:\n\n簡潔で直接的な応答\n関数呼び出し（Function calling）に最適化\n低レイテンシ（思考トレースなし）\n\n訓練パイプライン:\n\nSFT: Dolci Instruct SFT（関数呼び出しデータを含む）\nDPO: 多ターン選好データと応答長の最適化\nRLVR: 核心能力のさらなる改善\n\n成果:\n\nQwen 2.5、Gemma 3、IBM Granite 3.3、Llama 3 と同等スケールのモデルを上回る\nQwen 3 とのパフォーマンスギャップを縮小\n\n\n\n\nOlmo 3 RL-Zero は、Base モデルから直接 RL で訓練したモデルです。\n目的:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能にする\n完全にオープンな RL ベンチマークを提供\n\nドメイン:\n\nMath（数学）\nCode（コーディング）\nPrecise IF（精密な指示追従）\nGeneral Mix（一般混合）\n\n重要性:\n\n既存のオープンウェイトモデルは事前学習データを公開していないため、RL 研究が制限されていた\nOlmo 3 RL-Zero により、データリークの影響を排除した明確なベンチマークが可能に\n\n\n\n\n\nOlmo 3 Think 32B の訓練には、1024 台の H100 GPU を使用して約 56 日かかりました。\n内訳:\n\nPretraining: 約 47 日（Midtraining と Long-context Extension を含む）\nPost-training: 約 9 日（SFT、DPO、RL）\n\n推定コスト: $2/H100 時間で計算すると約 $2.75M\n\n\n\nOlmo 3 は、すべての中間チェックポイントと最終モデルを公開しています。\n公開内容:\n\nモデル:\n\nすべてのステージの中間チェックポイント\n最終モデル（Base, Think, Instruct, RL-Zero）\n\nデータ:\n\nデータミックス: 実際に訓練に使用したトークン\nソースデータプール: 各ステージの完全なソースデータ\n\nPretraining: 9T トークンのクリーンデータ\nMidtraining: 2T トークンの特化データ\nLong-context: 640B トークンの長文書データ\n\n\nサンプルミックス: より少ない計算リソースでの実験用\n\nPretraining: 150B トークン\nMidtraining: 10B トークン\n\nコード:\n\n訓練コード: OLMo-core（事前学習）、Open Instruct（後訓練）\nデータコード: datamap-rs、duplodocus（重複排除）、dolma3\n評価コード: OLMES、decon（評価データ汚染除去）\n\n\n\n\n\n\n完全オープンなモデルフロー: すべてのステージ、データ、コードを公開\n最強の完全オープンモデル: Base と Think の両方で最高性能\n新しいデータセット: Dolma 3（事前学習）と Dolci（後訓練）\n新しい手法:\n\nOlmoBaseEval（効率的な Base モデル評価）\nOlmoRL（効率的な強化学習フレームワーク）\nDelta Learning（高品質な選好データ作成）\n長文脈拡張の技術（RoPE、Document packing、Intra-document masking）\n\n研究可能性: 思考チェーンを元の訓練データまでトレース可能\n\n\n\n\nOlmo 3 は、完全オープンな AI 研究と開発を推進するための包括的なリリースです。モデルの最終的な重みだけでなく、開発プロセス全体を透明化し、研究者がモデル開発のあらゆる段階で介入・カスタマイズできるようにしています。\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。\nフラッグシップモデル: Olmo 3.1 Think 32B は、推論ベンチマークスイートで Qwen 3 32B に迫りながら、6 分の 1 の訓練トークン数で達成し、すべての訓練データと思考チェーンをトレース可能にしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#概要",
    "href": "ja/olmo-3/00-overview.html#概要",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n\n完全オープン: 学習データ、コード、中間チェックポイントをすべて公開\n多様な能力: 長文脈推論、関数呼び出し、コーディング、指示追従、一般的なチャット、知識リコール\nフラッグシップモデル: Olmo 3.1 Think 32B は、これまでに公開された最強の完全オープン思考型モデル\n\nモデルバリエーション:\n\nOlmo 3 Base: 基盤モデル（7B, 32B）\nOlmo 3 Think: 段階的推論を行う思考型モデル\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練したモデル\n\n論文: arXiv:2512.13961",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#モデルフロー-model-flow",
    "href": "ja/olmo-3/00-overview.html#モデルフロー-model-flow",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 の開発は、Base Model Training（基盤モデル訓練）と Post-training（後訓練）の 2 つの主要ステージに分かれています。\n┌───────────────────────────────────────────────────────────────┐\n│                    Base Model Training                        │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 1: Pretraining (5.9T tokens)                           │\n│    → Dolma 3 Mix (Web, PDFs, Code, etc.)                     │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 2: Midtraining (100B tokens)                           │\n│    → Dolma 3 Dolmino Mix (Math, Code, QA, etc.)              │\n├───────────────────────────────────────────────────────────────┤\n│  Stage 3: Long-context Extension (50-100B tokens)             │\n│    → Dolma 3 Longmino Mix (Long PDFs + Midtrain data)        │\n└───────────────────────────────────────────────────────────────┘\n                            ↓\n                     Olmo 3 Base\n                            ↓\n┌───────────────────────────────────────────────────────────────┐\n│                      Post-training                            │\n├───────────────────────────────────────────────────────────────┤\n│  Path 1: Olmo 3 Think                                         │\n│    SFT → DPO (Delta Learning) → RLVR (OlmoRL)               │\n├───────────────────────────────────────────────────────────────┤\n│  Path 2: Olmo 3 Instruct                                      │\n│    SFT → DPO → RLVR                                         │\n├───────────────────────────────────────────────────────────────┤\n│  Path 3: Olmo 3 RL-Zero                                       │\n│    Base → RLVR (from scratch)                                │\n└───────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#base-model-training",
    "href": "ja/olmo-3/00-overview.html#base-model-training",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 Base は、Dolma 3 Mix と呼ばれる 5.9 兆トークンの多様なデータで事前学習されます。\n\n詳細: Dolma 3 データセット\n\n主な革新点:\n\n高速でスケーラブルなグローバル重複排除: 兆トークンスケールでの新しいツール\n\n詳細: Deduplication（重複排除）\n\nolmOCR science PDFs: 学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n\n詳細: olmOCR science PDFs\n\nデータミキシングの新手法: Token-constrained mixing と Quality-aware upsampling\n\n詳細: Data Mixing 手法\n\n\nデータソース:\n\nWeb ページ\n学術 PDF（olmOCR science PDFs）\nコードリポジトリ\n数学データ\nその他の多様なソース\n\n\n\n\n100 億トークンの Dolma 3 Dolmino Mix で中間訓練を実施します。このステージの目的は、コード、数学、一般知識 QA などの重要な能力を強化することです。\n\n詳細: Midtraining（中間訓練）\n\n革新的な手法:\n\n2 部構成のフレームワーク:\n\n個別データソースへの軽量な分散フィードバックループ\n候補ミックスを評価する集中統合テスト\n\nPost-training の下準備: 指示データと思考トレースを意図的に含めることで、後訓練の基盤を構築\n\n評価スイート: OlmoBaseEval\n\n詳細: OlmoBaseEval（評価スイート）\n\n\n\n\nOlmo 3 は、最大 65K トークンのコンテキストをサポートする長文脈能力を持ちます。7B モデルは 50B トークン、32B モデルは 100B トークンの Dolma 3 Longmino Mix で拡張訓練されます。\n\n詳細: Long-context Extension（長文脈拡張）\n\n主要技術:\n\nRoPE 拡張: YaRN を使用した位置エンコーディングの拡張\nDocument packing: Best-fit packing で効率的な長文書の配置\nIntra-document masking: 同一文書内のトークンのみに attention\nModel souping: 複数チェックポイントの平均化\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n\n\n\nOlmo 3 Base は、32B パラメータスケールで 最強の完全オープンモデルです：\n\n完全オープンモデル: Stanford Marin 32B、Apertus 70B を上回る\n数学とコード: 他の完全オープン 32B モデルに対して 2 桁の改善\n長文脈性能: Qwen 2.5 32B、Mistral Small 3.1 24B、Gemma 3 27B に匹敵",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#post-training",
    "href": "ja/olmo-3/00-overview.html#post-training",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Base モデルから 3 つのバリエーションを開発します。\n\n\nOlmo 3 Think は、最終回答を生成する前に段階的推論を行い、中間的な思考トレースを生成するように訓練されています。\n訓練パイプライン:\n\nSFT (Supervised Finetuning): Dolci Think SFT で思考トレースを学習\nDPO (Direct Preference Optimization): Delta Learning による選好調整\n\n詳細: Delta Learning\n\nRLVR (Reinforcement Learning with Verifiable Rewards): OlmoRL による強化学習\n\n詳細: OlmoRL / GRPO\n\n\n\n詳細: Dolci データセット\n\n成果:\n\nOlmo 3.1 Think 32B: 最強の完全オープン思考型モデル\nQwen 2.5 32B、Gemma 2/3 27B、DeepSeek R1 32B を上回る\nQwen 3 32B に迫る性能（訓練トークン数は 6 分の 1）\n\n主要ベンチマーク結果 (Olmo 3.1 Think 32B):\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4\n\n\n\n\n\n\nOlmo 3 Instruct は、内部的な思考トレースを生成せずに、効率的で有用な応答を生成するように訓練されています。\n特徴:\n\n簡潔で直接的な応答\n関数呼び出し（Function calling）に最適化\n低レイテンシ（思考トレースなし）\n\n訓練パイプライン:\n\nSFT: Dolci Instruct SFT（関数呼び出しデータを含む）\nDPO: 多ターン選好データと応答長の最適化\nRLVR: 核心能力のさらなる改善\n\n成果:\n\nQwen 2.5、Gemma 3、IBM Granite 3.3、Llama 3 と同等スケールのモデルを上回る\nQwen 3 とのパフォーマンスギャップを縮小\n\n\n\n\nOlmo 3 RL-Zero は、Base モデルから直接 RL で訓練したモデルです。\n目的:\n\n事前学習データが RL パフォーマンスに与える影響を研究可能にする\n完全にオープンな RL ベンチマークを提供\n\nドメイン:\n\nMath（数学）\nCode（コーディング）\nPrecise IF（精密な指示追従）\nGeneral Mix（一般混合）\n\n重要性:\n\n既存のオープンウェイトモデルは事前学習データを公開していないため、RL 研究が制限されていた\nOlmo 3 RL-Zero により、データリークの影響を排除した明確なベンチマークが可能に",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#訓練コストとタイムライン",
    "href": "ja/olmo-3/00-overview.html#訓練コストとタイムライン",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 Think 32B の訓練には、1024 台の H100 GPU を使用して約 56 日かかりました。\n内訳:\n\nPretraining: 約 47 日（Midtraining と Long-context Extension を含む）\nPost-training: 約 9 日（SFT、DPO、RL）\n\n推定コスト: $2/H100 時間で計算すると約 $2.75M",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#オープンアーティファクト",
    "href": "ja/olmo-3/00-overview.html#オープンアーティファクト",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、すべての中間チェックポイントと最終モデルを公開しています。\n公開内容:\n\nモデル:\n\nすべてのステージの中間チェックポイント\n最終モデル（Base, Think, Instruct, RL-Zero）\n\nデータ:\n\nデータミックス: 実際に訓練に使用したトークン\nソースデータプール: 各ステージの完全なソースデータ\n\nPretraining: 9T トークンのクリーンデータ\nMidtraining: 2T トークンの特化データ\nLong-context: 640B トークンの長文書データ\n\n\nサンプルミックス: より少ない計算リソースでの実験用\n\nPretraining: 150B トークン\nMidtraining: 10B トークン\n\nコード:\n\n訓練コード: OLMo-core（事前学習）、Open Instruct（後訓練）\nデータコード: datamap-rs、duplodocus（重複排除）、dolma3\n評価コード: OLMES、decon（評価データ汚染除去）",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#主な貢献",
    "href": "ja/olmo-3/00-overview.html#主な貢献",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "完全オープンなモデルフロー: すべてのステージ、データ、コードを公開\n最強の完全オープンモデル: Base と Think の両方で最高性能\n新しいデータセット: Dolma 3（事前学習）と Dolci（後訓練）\n新しい手法:\n\nOlmoBaseEval（効率的な Base モデル評価）\nOlmoRL（効率的な強化学習フレームワーク）\nDelta Learning（高品質な選好データ作成）\n長文脈拡張の技術（RoPE、Document packing、Intra-document masking）\n\n研究可能性: 思考チェーンを元の訓練データまでトレース可能",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/olmo-3/00-overview.html#まとめ",
    "href": "ja/olmo-3/00-overview.html#まとめ",
    "title": "Olmo 3 Technical Report まとめ",
    "section": "",
    "text": "Olmo 3 は、完全オープンな AI 研究と開発を推進するための包括的なリリースです。モデルの最終的な重みだけでなく、開発プロセス全体を透明化し、研究者がモデル開発のあらゆる段階で介入・カスタマイズできるようにしています。\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。\nフラッグシップモデル: Olmo 3.1 Think 32B は、推論ベンチマークスイートで Qwen 3 32B に迫りながら、6 分の 1 の訓練トークン数で達成し、すべての訓練データと思考チェーンをトレース可能にしています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html",
    "href": "ja/molmo2/07-packing-message-trees.html",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing は、複数の短い訓練例を1つの長いシーケンスにマージして、バッチ作成時の無駄な padding を避ける技術です。訓練例のトークン数は様々で、数百トークン（純粋なテキストや小さい画像）から16,000トークン以上（字幕付き動画や長コンテキスト訓練中の長い動画）まで幅広く分布しています。\n\n\nVLM における Packing は、以下の理由により非自明な課題となります:\n\n二重のパッキング要求: ViT のクロップと LLM のトークンの両方を効率的にパックする必要がある\nモデル多様性: 画像・動画をトークンに変換する異なるアプローチを持つモデルをサポートする必要がある\n\n\n\n\nMolmo2 では、小規模なメモリ内の例のプールから最大効率のパックシーケンスを構築する オンザフライパッキングアルゴリズム を開発しました。このアルゴリズムは標準的な PyTorch データローダーに統合可能です。\n\n\n\n\n\n\nNote効率改善\n\n\n\nSFT 時には、平均 3.8 個の例 を 16,348 トークンのシーケンス に詰め込むことができ、15倍の訓練効率 を達成しています。\n\n\n\n\n\n\nMessage Trees は、複数のアノテーションを持つ動画や画像をエンコードする手法です。1つの視覚入力に対して複数の異なるアノテーション（質問応答ペア、キャプション、ポインティングなど）を効率的に処理できます。\n\n\nMessage Trees では、以下のような木構造でデータを表現します:\nVisual Input (Root)\n├─ Annotation 1 (Branch 1)\n├─ Annotation 2 (Branch 2)\n├─ Annotation 3 (Branch 3)\n└─ Annotation 4 (Branch 4)\n具体的には:\n\n視覚入力が最初のメッセージとしてエンコードされる\n各アノテーションが異なるブランチとなる\n木構造が単一のシーケンスとして線形化される\nカスタムアテンションマスクを使用してブランチ間のクロスアテンションを防止\n\n\n\n\n\n\n\nTipデータ統計\n\n\n\n訓練データ内の例は、平均して 4つのアノテーション を持っています。\n\n\n\n\n\nMessage Trees では、ブランチ間の独立性を保つために カスタムアテンションマスク を使用します。これにより、異なるアノテーション（ブランチ）が互いにアテンションすることを防ぎます。\n\n\n\n\n\n\ngraph TB\n    V[視覚入力&lt;br/&gt;Vision Input]\n\n    V --&gt; A1[アノテーション 1&lt;br/&gt;Branch 1]\n    V --&gt; A2[アノテーション 2&lt;br/&gt;Branch 2]\n    V --&gt; A3[アノテーション 3&lt;br/&gt;Branch 3]\n    V --&gt; A4[アノテーション 4&lt;br/&gt;Branch 4]\n\n    style V fill:#e1f5ff\n    style A1 fill:#fff4e1\n    style A2 fill:#fff4e1\n    style A3 fill:#fff4e1\n    style A4 fill:#fff4e1\n\n    classDef attention stroke:#2196F3,stroke-width:2px\n    class V attention\n\n\n\n\nFigure 1: Message Trees のアテンション構造\n\n\n\n\n\n各ブランチは視覚入力にはアテンションできますが、他のブランチには クロスアテンションできません。詳細なアテンションマスクのパターンは、元論文の Figure 3 を参照してください。\n\n\n\n\nPacking と Message Trees を組み合わせることで、Molmo2 は以下を実現しています:\n\n高密度の訓練データ利用: 1つの視覚入力に対する複数のアノテーションを効率的に活用\n最小限のパディング: 異なる長さの例を効率的に詰め込み、GPU メモリを有効活用\n高速な訓練: 15倍の効率化により、大規模データでの訓練を加速\n\nこの2つの技術は、Molmo2 の効率的な訓練における重要な基盤となっています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#packing-とは",
    "href": "ja/molmo2/07-packing-message-trees.html#packing-とは",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing は、複数の短い訓練例を1つの長いシーケンスにマージして、バッチ作成時の無駄な padding を避ける技術です。訓練例のトークン数は様々で、数百トークン（純粋なテキストや小さい画像）から16,000トークン以上（字幕付き動画や長コンテキスト訓練中の長い動画）まで幅広く分布しています。\n\n\nVLM における Packing は、以下の理由により非自明な課題となります:\n\n二重のパッキング要求: ViT のクロップと LLM のトークンの両方を効率的にパックする必要がある\nモデル多様性: 画像・動画をトークンに変換する異なるアプローチを持つモデルをサポートする必要がある\n\n\n\n\nMolmo2 では、小規模なメモリ内の例のプールから最大効率のパックシーケンスを構築する オンザフライパッキングアルゴリズム を開発しました。このアルゴリズムは標準的な PyTorch データローダーに統合可能です。\n\n\n\n\n\n\nNote効率改善\n\n\n\nSFT 時には、平均 3.8 個の例 を 16,348 トークンのシーケンス に詰め込むことができ、15倍の訓練効率 を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#message-trees-とは",
    "href": "ja/molmo2/07-packing-message-trees.html#message-trees-とは",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Message Trees は、複数のアノテーションを持つ動画や画像をエンコードする手法です。1つの視覚入力に対して複数の異なるアノテーション（質問応答ペア、キャプション、ポインティングなど）を効率的に処理できます。\n\n\nMessage Trees では、以下のような木構造でデータを表現します:\nVisual Input (Root)\n├─ Annotation 1 (Branch 1)\n├─ Annotation 2 (Branch 2)\n├─ Annotation 3 (Branch 3)\n└─ Annotation 4 (Branch 4)\n具体的には:\n\n視覚入力が最初のメッセージとしてエンコードされる\n各アノテーションが異なるブランチとなる\n木構造が単一のシーケンスとして線形化される\nカスタムアテンションマスクを使用してブランチ間のクロスアテンションを防止\n\n\n\n\n\n\n\nTipデータ統計\n\n\n\n訓練データ内の例は、平均して 4つのアノテーション を持っています。\n\n\n\n\n\nMessage Trees では、ブランチ間の独立性を保つために カスタムアテンションマスク を使用します。これにより、異なるアノテーション（ブランチ）が互いにアテンションすることを防ぎます。\n\n\n\n\n\n\ngraph TB\n    V[視覚入力&lt;br/&gt;Vision Input]\n\n    V --&gt; A1[アノテーション 1&lt;br/&gt;Branch 1]\n    V --&gt; A2[アノテーション 2&lt;br/&gt;Branch 2]\n    V --&gt; A3[アノテーション 3&lt;br/&gt;Branch 3]\n    V --&gt; A4[アノテーション 4&lt;br/&gt;Branch 4]\n\n    style V fill:#e1f5ff\n    style A1 fill:#fff4e1\n    style A2 fill:#fff4e1\n    style A3 fill:#fff4e1\n    style A4 fill:#fff4e1\n\n    classDef attention stroke:#2196F3,stroke-width:2px\n    class V attention\n\n\n\n\nFigure 1: Message Trees のアテンション構造\n\n\n\n\n\n各ブランチは視覚入力にはアテンションできますが、他のブランチには クロスアテンションできません。詳細なアテンションマスクのパターンは、元論文の Figure 3 を参照してください。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/07-packing-message-trees.html#packing-と-message-trees-の相乗効果",
    "href": "ja/molmo2/07-packing-message-trees.html#packing-と-message-trees-の相乗効果",
    "title": "Packing & Message Trees",
    "section": "",
    "text": "Packing と Message Trees を組み合わせることで、Molmo2 は以下を実現しています:\n\n高密度の訓練データ利用: 1つの視覚入力に対する複数のアノテーションを効率的に活用\n最小限のパディング: 異なる長さの例を効率的に詰め込み、GPU メモリを有効活用\n高速な訓練: 15倍の効率化により、大規模データでの訓練を加速\n\nこの2つの技術は、Molmo2 の効率的な訓練における重要な基盤となっています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Packing & Message Trees"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html",
    "href": "ja/molmo2/05-long-context-training.html",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training は、Molmo2 の Stage 3 として実施される最終的な訓練段階です。Stage 2 の Supervised Fine-Tuning (SFT) と同じデータミックスを使用しながら、コンテキスト長を大幅に拡張 することで、長尺ビデオや大量の画像を扱う能力を向上させます。\nこの段階は、オーバーヘッドが大きいため 短期間のみ実施 されますが、長尺ビデオ理解タスクでの性能向上に重要な役割を果たします。\n\n\n\nLong-Context Training の主な目的は以下の通りです。\n\n長尺ビデオの理解: 10分以上の長い動画を処理する能力の向上\n大量フレームの処理: より多くのフレームを同時に扱うことで、時間的な文脈をより正確に把握\n複雑なマルチモーダル入力: 字幕付き動画や、複数の画像セットなど、トークン数が多い入力への対応\n\n\n\n\nStage 2 (SFT) と Stage 3 (Long-Context Training) の主な違いは以下の表の通りです。\n\n\n\n\n\n\n\n\n\nパラメータ\nStage 2 (SFT)\nStage 3 (Long-Context)\n変化率\n\n\n\n\nシーケンス長\n16,384\n36,864\n+225%\n\n\n最大フレーム数 (F)\n128\n384\n+300%\n\n\n訓練ステップ数\n30,000\n2,000\n-93%\n\n\nバッチサイズ\n128\n128\n変更なし\n\n\nデータミックス\nSFT データミックス\nSFT データミックス（同一）\n変更なし\n\n\n並列化手法\n標準的な Data Parallelism\nContext Parallelism (CP)\n追加\n\n\n\n\n\n\n\n\n\nNoteなぜ短期間のみ実施するか\n\n\n\nLong-Context Training は 2,000 ステップ のみ実施されます（Stage 2 の30,000ステップと比較して6.7%）。これは、Context Parallelism による 計算オーバーヘッドが大きい ためです。\n具体的には:\n\n各例が 8 GPU のグループ で処理される\nVision encoder と attentional pooling の分散処理が必要\n通常の訓練よりも通信コストが高い\n\n短期間の訓練でも、長尺ビデオベンチマークでの性能が 有意に向上 することが確認されています（Table 11）。\n\n\n\n\n\nStage 3 では、最大シーケンス長を 16,384 → 36,864 トークンに拡張します。これにより、以下のような入力が扱えるようになります。\n例:\n\n長尺ビデオ: 384 フレーム（2 fps で約3分12秒）+ 字幕\n複雑な QA: 長いキャプション（平均924語）を含む動画に対する複数ターンの会話\nマルチ画像: 大量の高解像度画像（複数のクロップ含む）\n\nシーケンス長の拡張により、packing アルゴリズムは最大 36,864 トークン と 384 画像クロップ を単一のパックシーケンスに詰め込むことができます（Stage 2 では16,384トークン、128クロップが上限）。\n\n\n\nStage 3 では、動画の最大フレーム数を F = 128 → F = 384 に拡張します。\nサンプリング方法:\n\nサンプリングレート: 2 fps（変更なし）\n最大長: 384 / 2 = 192秒（約3分12秒）\nフレーム選択: 動画長が F/S を超える場合、F フレームを均等にサンプリング\n最終フレーム: 常に含まれる（動画プレイヤーが最終フレームを表示するため）\n\n\n\n\nStage\n最大フレーム数 (F)\n最大動画長（2 fps）\n\n\n\n\nStage 2 (SFT)\n128\n64秒（約1分）\n\n\nStage 3 (Long-Context)\n384\n192秒（約3分）\n\n\n\n\n\n\n\n\n\nTip推論時のフレーム数拡張\n\n\n\n訓練時は F = 384 ですが、推論時には テストタイムスケーリング を使用して、さらに多くのフレームを処理することが可能です。\nアブレーション（Table 13）によると、Molmo2-8B（SFT後、Long-Context訓練前）は、推論時に 224 フレーム で最良の性能を示しました。Long-Context訓練後は、さらに多くのフレームを処理できる可能性があります。\n\n\n\n\n\nLong-Context Training では、LLM に対して Context Parallelism (CP) を使用します。これは、長いシーケンスを複数の GPU に分散して処理する並列化手法です。\n\n\nMolmo2 は、CP の実装として Ulysses attention を採用しています。\n選択理由:\n\nAll-gather 操作の柔軟性: Molmo2 の packing と message tree システムで使用される カスタムアテンションマスク に対応可能\n通信効率: シーケンス次元を分割し、必要な情報のみを all-gather\n\n動作概要:\n\n各 GPU が シーケンスの一部 を処理\nAttention 計算時に all-gather で他 GPU の情報を集約\nカスタムアテンションマスク（packing や message tree 用）を適用\n\n\n\n\n\n\n\nNoteContext Parallelism の設定\n\n\n\n\nCP グループサイズ: 8 GPU\n各例の処理: 8 GPU のグループで1つの例を処理\n適用範囲: LLM のみ（Vision encoder は別途分散）\n\n参考文献: [56] Ulysses attention\n\n\n\n\n\nContext Parallelism は LLM に適用されますが、Vision encoder と attentional pooling も CP グループ全体に分散されます。\n分散方法:\n\nフレーム分割: 384 フレームを 8 GPU に分割（各 GPU が48フレームを処理）\nVision encoder: 各 GPU が担当フレームの Vision Transformer 処理を実行\nAttentional pooling: Vision token を LLM 用にプーリング（3x3 pooling for video）\n統合: プーリングされた視覚トークンを LLM の入力として結合\n\n効果:\n\nメモリフットプリント削減: Vision encoder のアクティベーションを複数 GPU に分散\n計算効率: フレーム処理を並列化\n\n\n\n\n\n\n\nImportantVision Encoder 分散の重要性\n\n\n\n論文では、Vision encoder と attentional pooling の分散処理が 非常に効果的 にモデルのメモリフットプリントを削減したと述べられています。\n384 フレームの処理は膨大なメモリを必要とするため（各フレームが複数のパッチに分割され、ViT で処理される）、この分散処理なしでは訓練が困難でした。\n\n\n\n\n\n\n\n\nLong-Context Training では、Stage 2 (SFT) と 同じハイパーパラメータ を使用します（Table 12）。\n\nOptimizer: AdamW\nLearning rate: Cosine decay（10% まで減衰）\nWarmup: ViT と LLM で長めのウォームアップ\nWeight decay: なし\n\n\n\n\n\n\n\nモデル\nGPU 数\n訓練時間\nGPU 時間\n\n\n\n\nMolmo2-4B\n128\n25.3 時間\n3,200 GPU-hr\n\n\nMolmo2-O-7B\n128\n25.7 時間\n3,300 GPU-hr\n\n\nMolmo2-8B\n128\n26.0 時間\n3,300 GPU-hr\n\n\n\n注: GPU は Nvidia H100 を使用\nStage 2 (SFT) と比較すると、訓練時間は約40%（4B: 7.5k → 3.2k GPU-hr）です。これは、ステップ数が少ない（2k vs 30k）ためです。\n\n\n\n\nLong-Context Training の効果を検証したアブレーション（Table 11）の結果は以下の通りです。\n\n\n\n設定\n短尺ビデオ QA\n長尺ビデオ QA\nMolmo2 Video Cap.\n画像 QA\n\n\n\n\nLong-Context SFT あり\n69.4\n67.4\n39.9\n80.6\n\n\nLong-Context SFT なし\n69.6\n64.4\n42.3\n80.5\n\n\n\n観察:\n\n長尺ビデオ QA: +3.0 ポイント向上（67.4 vs 64.4）\n短尺ビデオ QA: ほぼ変化なし（-0.2 ポイント）\nビデオキャプション: -2.4 ポイント低下（42.3 → 39.9）\n画像 QA: ほぼ変化なし（+0.1 ポイント）\n\n\n\n\n\n\n\nWarningトレードオフ: キャプション性能の低下\n\n\n\nLong-Context Training は長尺ビデオ理解を向上させますが、ビデオキャプション の性能がわずかに低下します。\nこれは、長いコンテキストに適応する過程で、短い出力（キャプション）の生成能力が若干犠牲になる可能性を示唆しています。実用上は、タスクに応じて Long-Context Training の有無を選択することが重要です。\n\n\n\n\n\nMolmo2 は Long-Context Training により長尺ビデオ理解が向上しましたが、最良のオープンウェイトモデル（Eagle2.5-8B など）には及びません。\n主な原因:\n\nオープンデータの不足: 10分以上の長尺ビデオのアノテーションデータが極めて少ない\n計算制限: 超長尺コンテキストの訓練は計算コストが高く、大規模実施が困難\n訓練期間の制約: わずか2,000ステップの訓練では、長尺ビデオに特化した能力の獲得が限定的\n\n\n\n\n\n\n\nNote今後の展望\n\n\n\n論文では、オープンソースの長尺ビデオデータ の不足が主要なボトルネックであると述べられています。\nコミュニティが長尺ビデオのアノテーションデータを構築し、より長期間の Long-Context Training を実施できれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。\n\n\n\n\n\nLong-Context Training（Stage 3）は、Molmo2 の訓練パイプラインの最終段階として、以下を実現します。\n\nコンテキスト長: 16,384 → 36,864（+125%）\nフレーム数: F = 128 → F = 384（+200%）\n並列化: Context Parallelism（Ulysses attention）を使用し、8 GPU で処理\nVision encoder 分散: フレーム処理を CP グループ全体に分散してメモリ削減\n短期間訓練: わずか2,000ステップ（オーバーヘッドのため）\n\nこの短期間の訓練でも、長尺ビデオ QA で +3.0 ポイント の性能向上を達成しており、Long-Context Training の有効性が示されています。ただし、ビデオキャプションの性能がわずかに低下するトレードオフも確認されています。\n今後、オープンソースの長尺ビデオデータが充実し、より長期間の Long-Context Training が可能になれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#概要",
    "href": "ja/molmo2/05-long-context-training.html#概要",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training は、Molmo2 の Stage 3 として実施される最終的な訓練段階です。Stage 2 の Supervised Fine-Tuning (SFT) と同じデータミックスを使用しながら、コンテキスト長を大幅に拡張 することで、長尺ビデオや大量の画像を扱う能力を向上させます。\nこの段階は、オーバーヘッドが大きいため 短期間のみ実施 されますが、長尺ビデオ理解タスクでの性能向上に重要な役割を果たします。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#目的",
    "href": "ja/molmo2/05-long-context-training.html#目的",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training の主な目的は以下の通りです。\n\n長尺ビデオの理解: 10分以上の長い動画を処理する能力の向上\n大量フレームの処理: より多くのフレームを同時に扱うことで、時間的な文脈をより正確に把握\n複雑なマルチモーダル入力: 字幕付き動画や、複数の画像セットなど、トークン数が多い入力への対応",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#stage-2-との比較",
    "href": "ja/molmo2/05-long-context-training.html#stage-2-との比較",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 2 (SFT) と Stage 3 (Long-Context Training) の主な違いは以下の表の通りです。\n\n\n\n\n\n\n\n\n\nパラメータ\nStage 2 (SFT)\nStage 3 (Long-Context)\n変化率\n\n\n\n\nシーケンス長\n16,384\n36,864\n+225%\n\n\n最大フレーム数 (F)\n128\n384\n+300%\n\n\n訓練ステップ数\n30,000\n2,000\n-93%\n\n\nバッチサイズ\n128\n128\n変更なし\n\n\nデータミックス\nSFT データミックス\nSFT データミックス（同一）\n変更なし\n\n\n並列化手法\n標準的な Data Parallelism\nContext Parallelism (CP)\n追加\n\n\n\n\n\n\n\n\n\nNoteなぜ短期間のみ実施するか\n\n\n\nLong-Context Training は 2,000 ステップ のみ実施されます（Stage 2 の30,000ステップと比較して6.7%）。これは、Context Parallelism による 計算オーバーヘッドが大きい ためです。\n具体的には:\n\n各例が 8 GPU のグループ で処理される\nVision encoder と attentional pooling の分散処理が必要\n通常の訓練よりも通信コストが高い\n\n短期間の訓練でも、長尺ビデオベンチマークでの性能が 有意に向上 することが確認されています（Table 11）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#コンテキスト長の拡張",
    "href": "ja/molmo2/05-long-context-training.html#コンテキスト長の拡張",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 3 では、最大シーケンス長を 16,384 → 36,864 トークンに拡張します。これにより、以下のような入力が扱えるようになります。\n例:\n\n長尺ビデオ: 384 フレーム（2 fps で約3分12秒）+ 字幕\n複雑な QA: 長いキャプション（平均924語）を含む動画に対する複数ターンの会話\nマルチ画像: 大量の高解像度画像（複数のクロップ含む）\n\nシーケンス長の拡張により、packing アルゴリズムは最大 36,864 トークン と 384 画像クロップ を単一のパックシーケンスに詰め込むことができます（Stage 2 では16,384トークン、128クロップが上限）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#フレーム数の拡張",
    "href": "ja/molmo2/05-long-context-training.html#フレーム数の拡張",
    "title": "Long-Context Training",
    "section": "",
    "text": "Stage 3 では、動画の最大フレーム数を F = 128 → F = 384 に拡張します。\nサンプリング方法:\n\nサンプリングレート: 2 fps（変更なし）\n最大長: 384 / 2 = 192秒（約3分12秒）\nフレーム選択: 動画長が F/S を超える場合、F フレームを均等にサンプリング\n最終フレーム: 常に含まれる（動画プレイヤーが最終フレームを表示するため）\n\n\n\n\nStage\n最大フレーム数 (F)\n最大動画長（2 fps）\n\n\n\n\nStage 2 (SFT)\n128\n64秒（約1分）\n\n\nStage 3 (Long-Context)\n384\n192秒（約3分）\n\n\n\n\n\n\n\n\n\nTip推論時のフレーム数拡張\n\n\n\n訓練時は F = 384 ですが、推論時には テストタイムスケーリング を使用して、さらに多くのフレームを処理することが可能です。\nアブレーション（Table 13）によると、Molmo2-8B（SFT後、Long-Context訓練前）は、推論時に 224 フレーム で最良の性能を示しました。Long-Context訓練後は、さらに多くのフレームを処理できる可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#context-parallelism-cp",
    "href": "ja/molmo2/05-long-context-training.html#context-parallelism-cp",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training では、LLM に対して Context Parallelism (CP) を使用します。これは、長いシーケンスを複数の GPU に分散して処理する並列化手法です。\n\n\nMolmo2 は、CP の実装として Ulysses attention を採用しています。\n選択理由:\n\nAll-gather 操作の柔軟性: Molmo2 の packing と message tree システムで使用される カスタムアテンションマスク に対応可能\n通信効率: シーケンス次元を分割し、必要な情報のみを all-gather\n\n動作概要:\n\n各 GPU が シーケンスの一部 を処理\nAttention 計算時に all-gather で他 GPU の情報を集約\nカスタムアテンションマスク（packing や message tree 用）を適用\n\n\n\n\n\n\n\nNoteContext Parallelism の設定\n\n\n\n\nCP グループサイズ: 8 GPU\n各例の処理: 8 GPU のグループで1つの例を処理\n適用範囲: LLM のみ（Vision encoder は別途分散）\n\n参考文献: [56] Ulysses attention\n\n\n\n\n\nContext Parallelism は LLM に適用されますが、Vision encoder と attentional pooling も CP グループ全体に分散されます。\n分散方法:\n\nフレーム分割: 384 フレームを 8 GPU に分割（各 GPU が48フレームを処理）\nVision encoder: 各 GPU が担当フレームの Vision Transformer 処理を実行\nAttentional pooling: Vision token を LLM 用にプーリング（3x3 pooling for video）\n統合: プーリングされた視覚トークンを LLM の入力として結合\n\n効果:\n\nメモリフットプリント削減: Vision encoder のアクティベーションを複数 GPU に分散\n計算効率: フレーム処理を並列化\n\n\n\n\n\n\n\nImportantVision Encoder 分散の重要性\n\n\n\n論文では、Vision encoder と attentional pooling の分散処理が 非常に効果的 にモデルのメモリフットプリントを削減したと述べられています。\n384 フレームの処理は膨大なメモリを必要とするため（各フレームが複数のパッチに分割され、ViT で処理される）、この分散処理なしでは訓練が困難でした。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#訓練の詳細",
    "href": "ja/molmo2/05-long-context-training.html#訓練の詳細",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training では、Stage 2 (SFT) と 同じハイパーパラメータ を使用します（Table 12）。\n\nOptimizer: AdamW\nLearning rate: Cosine decay（10% まで減衰）\nWarmup: ViT と LLM で長めのウォームアップ\nWeight decay: なし\n\n\n\n\n\n\n\nモデル\nGPU 数\n訓練時間\nGPU 時間\n\n\n\n\nMolmo2-4B\n128\n25.3 時間\n3,200 GPU-hr\n\n\nMolmo2-O-7B\n128\n25.7 時間\n3,300 GPU-hr\n\n\nMolmo2-8B\n128\n26.0 時間\n3,300 GPU-hr\n\n\n\n注: GPU は Nvidia H100 を使用\nStage 2 (SFT) と比較すると、訓練時間は約40%（4B: 7.5k → 3.2k GPU-hr）です。これは、ステップ数が少ない（2k vs 30k）ためです。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#アブレーション結果",
    "href": "ja/molmo2/05-long-context-training.html#アブレーション結果",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training の効果を検証したアブレーション（Table 11）の結果は以下の通りです。\n\n\n\n設定\n短尺ビデオ QA\n長尺ビデオ QA\nMolmo2 Video Cap.\n画像 QA\n\n\n\n\nLong-Context SFT あり\n69.4\n67.4\n39.9\n80.6\n\n\nLong-Context SFT なし\n69.6\n64.4\n42.3\n80.5\n\n\n\n観察:\n\n長尺ビデオ QA: +3.0 ポイント向上（67.4 vs 64.4）\n短尺ビデオ QA: ほぼ変化なし（-0.2 ポイント）\nビデオキャプション: -2.4 ポイント低下（42.3 → 39.9）\n画像 QA: ほぼ変化なし（+0.1 ポイント）\n\n\n\n\n\n\n\nWarningトレードオフ: キャプション性能の低下\n\n\n\nLong-Context Training は長尺ビデオ理解を向上させますが、ビデオキャプション の性能がわずかに低下します。\nこれは、長いコンテキストに適応する過程で、短い出力（キャプション）の生成能力が若干犠牲になる可能性を示唆しています。実用上は、タスクに応じて Long-Context Training の有無を選択することが重要です。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#長尺ビデオでの課題",
    "href": "ja/molmo2/05-long-context-training.html#長尺ビデオでの課題",
    "title": "Long-Context Training",
    "section": "",
    "text": "Molmo2 は Long-Context Training により長尺ビデオ理解が向上しましたが、最良のオープンウェイトモデル（Eagle2.5-8B など）には及びません。\n主な原因:\n\nオープンデータの不足: 10分以上の長尺ビデオのアノテーションデータが極めて少ない\n計算制限: 超長尺コンテキストの訓練は計算コストが高く、大規模実施が困難\n訓練期間の制約: わずか2,000ステップの訓練では、長尺ビデオに特化した能力の獲得が限定的\n\n\n\n\n\n\n\nNote今後の展望\n\n\n\n論文では、オープンソースの長尺ビデオデータ の不足が主要なボトルネックであると述べられています。\nコミュニティが長尺ビデオのアノテーションデータを構築し、より長期間の Long-Context Training を実施できれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/05-long-context-training.html#まとめ",
    "href": "ja/molmo2/05-long-context-training.html#まとめ",
    "title": "Long-Context Training",
    "section": "",
    "text": "Long-Context Training（Stage 3）は、Molmo2 の訓練パイプラインの最終段階として、以下を実現します。\n\nコンテキスト長: 16,384 → 36,864（+125%）\nフレーム数: F = 128 → F = 384（+200%）\n並列化: Context Parallelism（Ulysses attention）を使用し、8 GPU で処理\nVision encoder 分散: フレーム処理を CP グループ全体に分散してメモリ削減\n短期間訓練: わずか2,000ステップ（オーバーヘッドのため）\n\nこの短期間の訓練でも、長尺ビデオ QA で +3.0 ポイント の性能向上を達成しており、Long-Context Training の有効性が示されています。ただし、ビデオキャプションの性能がわずかに低下するトレードオフも確認されています。\n今後、オープンソースの長尺ビデオデータが充実し、より長期間の Long-Context Training が可能になれば、Molmo2 の長尺ビデオ理解能力はさらに向上する可能性があります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Long-Context Training"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html",
    "href": "ja/molmo2/03-multi-image-understanding.html",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Multi-Image Understanding は、複数の画像を同時に処理し、それらの関連性や違いを理解する能力です。従来の単一画像処理では各画像を独立して扱うのに対し、Multi-Image Understanding では複数の画像間の関係性を捉えることができます。\n\n\n単一画像処理:\n\n1枚の画像に対して質問応答やキャプション生成を実行\n画像間の比較や関連性の理解は不可能\n文書の複数ページやビフォー・アフター比較などには対応困難\n\nMulti-Image Understanding:\n\n2〜5枚の意味的に関連する画像セットを処理\n画像間の共通点・相違点を理解\n複数画像にまたがる質問応答やグラウンディングが可能\n\n\n\n\nMolmo2-MultiImageQA は、意味的に関連する画像セットに対する質問応答データセットです。\nデータセット規模:\n\n45,000 画像セット（96,000 ユニーク画像から構成）\n72,000 QA ペア\n1セットあたり 2〜5 枚の画像（平均 2.73 枚）\n\n収集方法: 人手によるアノテーションで構築されており、以下のプロセスで作成されました。\n\nPixMoCap で訓練されたモデルで各画像のキャプションを生成\nキャプションの文レベルの類似度に基づいて画像をグルーピング\nアノテーターが各セットに対して質問を作成\nClaude Sonnet 4.5 との反復ループで回答を改善\n\nこのアプローチにより、実世界でのマルチイメージクエリをサポートする高品質なデータセットが構築されました。\n\n\n\nMolmo2-MultiImagePoint は、複数画像にわたるポインティングとカウンティングのデータセットです。\nデータセット規模:\n\n470,000 以上のポインティング・カウンティング例\n1セットあたり 2〜5 枚の画像（平均 3.24 枚）\n\n収集方法: 合成的に構築されており、以下のパイプラインで作成されました。\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Step 1: Soft Clustering of Images                          │\n│  - Use images from PixMo-Points                             │\n│  - Combine single-token & sentence-level embedding          │\n│  - Generate semantically related sets (2-5 images)          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 2: Label Normalization                                │\n│  - Lowercase, punctuation/whitespace normalization          │\n│  - Synonym consolidation                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 3: Canonical Label Generation                         │\n│  - Use LLM to merge normalized labels                       │\n│  - Create single canonical description                      │\n│  - Defines shared entity/concept across all images          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 4: Training-time Sampling                             │\n│  - Sample from original annotations (not just canonical)    │\n│  - Preserve lexical diversity & improve robustness          │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\n\n\nNoteCanonical Label の役割\n\n\n\nCanonical Label は、画像セット内の複数の人手アノテーションを統合した標準的な記述です。例えば、「waterfall」「滝」「瀑布」などの異なる表現を「waterfall」という単一の canonical label に統合します。\nただし、トレーニング時には常に canonical label を使用するのではなく、元のアノテーションからも確率的にサンプリングすることで、多様な表現に対応できるモデルを構築しています。\n\n\n\n\n\n\nMolmo2-SynMultiImageQA は、テキストリッチな画像に特化した合成マルチイメージデータセットです。\nデータセット規模:\n\n188,000 例の合成マルチイメージ QA\n\n収集方法: CoSyn [172] を拡張して構築されました。CoSyn は、チャート、表、文書などのテキストリッチな画像に対する質問応答を合成的に生成するフレームワークです。\n対象画像タイプ:\n\nチャート（charts）\n表（tables）\n文書（documents）\n\nこれらのテキストリッチな画像は、文書理解や複数文書間の比較など、実用的なタスクに直結する重要なデータです。\n\n\n\n\n\n\nTip実用例: Multi-Image Understanding の活用\n\n\n\n文書理解:\n\n契約書の複数ページにわたる条項の比較\nレポートの異なるセクション間の整合性チェック\n複数の請求書の内容比較\n\n複数画像の比較:\n\n製品の異なる角度からの写真を比較して特徴を理解\nビフォー・アフター写真の変化検出\n複数のチャートやグラフを横断した傾向分析\n\nグラウンディング:\n\n「すべての画像で滝を指し示して」のような複数画像にわたるポインティング\n「赤い車が何枚の画像に写っているか？」のようなカウンティング\nセット全体での共通オブジェクトの検出\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nデータセット\n規模\n画像セットサイズ\n収集方法\n用途\n\n\n\n\nMolmo2-MultiImageQA\n45k セット72k QA\n2-5枚(平均2.73)\n人手\n一般的な QA\n\n\nMolmo2-MultiImagePoint\n470k 例\n2-5枚(平均3.24)\n合成\nポインティング・カウンティング\n\n\nMolmo2-SynMultiImageQA\n188k 例\n-\n合成(CoSyn拡張)\nテキストリッチ画像の QA\n\n\n\n\n\n\nMulti-Image Understanding は、単一画像処理では不可能だった以下のタスクを実現します。\n情報の統合: 複数の情報源（画像）から情報を統合し、包括的な理解を提供します。\n比較・対照: 画像間の共通点や相違点を明確に識別できます。\n文書処理: 複数ページの文書や、複数の関連文書を横断した理解が可能になります。\n現実世界への適用: 実際のアプリケーションでは、複数の画像を扱うシナリオが頻繁に発生します（例: ECサイトの商品画像、医療画像の時系列比較、監視カメラの複数アングルなど）。\nMolmo2 は、これらの3つのデータセットを活用することで、オープンソースモデルの中で最高水準の Multi-Image Understanding 能力を実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#単一画像処理との違い",
    "href": "ja/molmo2/03-multi-image-understanding.html#単一画像処理との違い",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "単一画像処理:\n\n1枚の画像に対して質問応答やキャプション生成を実行\n画像間の比較や関連性の理解は不可能\n文書の複数ページやビフォー・アフター比較などには対応困難\n\nMulti-Image Understanding:\n\n2〜5枚の意味的に関連する画像セットを処理\n画像間の共通点・相違点を理解\n複数画像にまたがる質問応答やグラウンディングが可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimageqa-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimageqa-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-MultiImageQA は、意味的に関連する画像セットに対する質問応答データセットです。\nデータセット規模:\n\n45,000 画像セット（96,000 ユニーク画像から構成）\n72,000 QA ペア\n1セットあたり 2〜5 枚の画像（平均 2.73 枚）\n\n収集方法: 人手によるアノテーションで構築されており、以下のプロセスで作成されました。\n\nPixMoCap で訓練されたモデルで各画像のキャプションを生成\nキャプションの文レベルの類似度に基づいて画像をグルーピング\nアノテーターが各セットに対して質問を作成\nClaude Sonnet 4.5 との反復ループで回答を改善\n\nこのアプローチにより、実世界でのマルチイメージクエリをサポートする高品質なデータセットが構築されました。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimagepoint-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-multiimagepoint-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-MultiImagePoint は、複数画像にわたるポインティングとカウンティングのデータセットです。\nデータセット規模:\n\n470,000 以上のポインティング・カウンティング例\n1セットあたり 2〜5 枚の画像（平均 3.24 枚）\n\n収集方法: 合成的に構築されており、以下のパイプラインで作成されました。\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Step 1: Soft Clustering of Images                          │\n│  - Use images from PixMo-Points                             │\n│  - Combine single-token & sentence-level embedding          │\n│  - Generate semantically related sets (2-5 images)          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 2: Label Normalization                                │\n│  - Lowercase, punctuation/whitespace normalization          │\n│  - Synonym consolidation                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 3: Canonical Label Generation                         │\n│  - Use LLM to merge normalized labels                       │\n│  - Create single canonical description                      │\n│  - Defines shared entity/concept across all images          │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Step 4: Training-time Sampling                             │\n│  - Sample from original annotations (not just canonical)    │\n│  - Preserve lexical diversity & improve robustness          │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\n\n\nNoteCanonical Label の役割\n\n\n\nCanonical Label は、画像セット内の複数の人手アノテーションを統合した標準的な記述です。例えば、「waterfall」「滝」「瀑布」などの異なる表現を「waterfall」という単一の canonical label に統合します。\nただし、トレーニング時には常に canonical label を使用するのではなく、元のアノテーションからも確率的にサンプリングすることで、多様な表現に対応できるモデルを構築しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#molmo2-synmultiimageqa-データセット",
    "href": "ja/molmo2/03-multi-image-understanding.html#molmo2-synmultiimageqa-データセット",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Molmo2-SynMultiImageQA は、テキストリッチな画像に特化した合成マルチイメージデータセットです。\nデータセット規模:\n\n188,000 例の合成マルチイメージ QA\n\n収集方法: CoSyn [172] を拡張して構築されました。CoSyn は、チャート、表、文書などのテキストリッチな画像に対する質問応答を合成的に生成するフレームワークです。\n対象画像タイプ:\n\nチャート（charts）\n表（tables）\n文書（documents）\n\nこれらのテキストリッチな画像は、文書理解や複数文書間の比較など、実用的なタスクに直結する重要なデータです。\n\n\n\n\n\n\nTip実用例: Multi-Image Understanding の活用\n\n\n\n文書理解:\n\n契約書の複数ページにわたる条項の比較\nレポートの異なるセクション間の整合性チェック\n複数の請求書の内容比較\n\n複数画像の比較:\n\n製品の異なる角度からの写真を比較して特徴を理解\nビフォー・アフター写真の変化検出\n複数のチャートやグラフを横断した傾向分析\n\nグラウンディング:\n\n「すべての画像で滝を指し示して」のような複数画像にわたるポインティング\n「赤い車が何枚の画像に写っているか？」のようなカウンティング\nセット全体での共通オブジェクトの検出",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#データセット統計",
    "href": "ja/molmo2/03-multi-image-understanding.html#データセット統計",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "データセット\n規模\n画像セットサイズ\n収集方法\n用途\n\n\n\n\nMolmo2-MultiImageQA\n45k セット72k QA\n2-5枚(平均2.73)\n人手\n一般的な QA\n\n\nMolmo2-MultiImagePoint\n470k 例\n2-5枚(平均3.24)\n合成\nポインティング・カウンティング\n\n\nMolmo2-SynMultiImageQA\n188k 例\n-\n合成(CoSyn拡張)\nテキストリッチ画像の QA",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/03-multi-image-understanding.html#multi-image-understanding-の重要性",
    "href": "ja/molmo2/03-multi-image-understanding.html#multi-image-understanding-の重要性",
    "title": "Multi-Image Understanding",
    "section": "",
    "text": "Multi-Image Understanding は、単一画像処理では不可能だった以下のタスクを実現します。\n情報の統合: 複数の情報源（画像）から情報を統合し、包括的な理解を提供します。\n比較・対照: 画像間の共通点や相違点を明確に識別できます。\n文書処理: 複数ページの文書や、複数の関連文書を横断した理解が可能になります。\n現実世界への適用: 実際のアプリケーションでは、複数の画像を扱うシナリオが頻繁に発生します（例: ECサイトの商品画像、医療画像の時系列比較、監視カメラの複数アングルなど）。\nMolmo2 は、これらの3つのデータセットを活用することで、オープンソースモデルの中で最高水準の Multi-Image Understanding 能力を実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Multi-Image Understanding"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html",
    "href": "ja/molmo2/01-dense-video-captioning.html",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の事前学習に使用される超詳細（dense）なビデオキャプションデータセットです。このデータセットは、従来のビデオキャプションデータセットと比較して桁違いに詳細な記述を含み、平均 924 語/動画 という驚異的な記述量を実現しています。\n従来の VLM が生成する短く表面的なキャプションとは異なり、Molmo2-Cap は動的イベントと細かな視覚的詳細の両方を捉えることで、モデルがビデオの時空間的理解を深める基盤を提供します。\nデータセット規模:\n\n104k のビデオレベルキャプション\n431k のクリップレベルキャプション\n平均 924 語/動画 の超詳細記述\n\n\n\n\n\n\nビデオキャプションは画像キャプションよりも本質的に困難です。なぜなら、アノテーターは以下の両方を記述する必要があるからです:\n\n動的イベント: 時間とともに変化する出来事、動作、状態遷移\n細かな視覚的詳細: オブジェクトの外観、空間配置、属性の変化\n\n既存のビデオキャプションデータセットの多くは、表面的な記述に留まっており、ビデオグラウンディング（いつ・どこで何が起きたか）を学習するには不十分でした。Molmo2-Cap は、この gap を埋めるために設計されました。\n\n\n\nより詳細なキャプションは、モデルに以下の能力を与えます:\n\n時空間的な理解: 「いつ」「どこで」「何が」起きたかを正確に把握\n細粒度の視覚認識: 小さな物体、微細な動作、属性の変化を捉える\n文脈理解: イベント間の因果関係や時間的依存性を学習\n\n\n\n\n\n\n\n\n\n\n\nNote他のビデオキャプションデータセットとの比較\n\n\n\n\n\nMolmo2-Cap は既存のビデオキャプションデータセットを大きく上回る記述量を実現しています:\n\n\n\nデータセット\n平均語数/動画\n特徴\n\n\n\n\nMolmo2-Cap\n924 語\n人手による音声記述 + Molmo 統合\n\n\nLLaVA-Video-178K\n547 語\nGPT ベースの合成キャプション\n\n\nShareGPT4-Video\n280 語\nGPT ベースの合成キャプション\n\n\nRDCap\n100 語\n既存データセット\n\n\nRCap\n89 語\n既存データセット\n\n\nVideo Localized Narratives\n75 語\n人手アノテーション\n\n\n\nMolmo2-Cap は、LLaVA-Video の 1.7 倍、ShareGPT4-Video の 3.3 倍、Video Localized Narratives の 12 倍 以上の記述量を持ちます。\n重要な差異:\n\nMolmo2-Cap は プロプライエタリモデル（GPT など）に依存しない 完全にオープンなパイプラインで構築されている\n人手による音声記述 をベースにしており、合成データよりも自然で詳細\nフレームレベルのキャプション統合 により、低レベルの視覚的詳細も漏れなく記述\n\n\n\n\n\n\n\nMolmo2-Cap のデータ収集は、革新的な 2 段階パイプライン を採用しています。\n\n\n\n初期プール構築: 10M 以上のビデオクリップを複数の大規模ソース（YT-Temporal、YouTube など）から収集\n情報量フィルタリング:\n\n音声トラックを除去し、1 fps で均一サンプリング\nH.264 でエンコードし、正規化された情報量スコアを算出: bits / (duration × W × H)\n平均 - 1σ 未満のビデオを除外（視覚的・時間的多様性が低いビデオを排除）\n\n多様性ベースサンプリング:\n\nSAM 2 でフレームをセグメント化し、視覚的複雑さを推定\nMolmo でフレームをキャプション化し、MetaCLIP パイプラインでキーワード抽出\nエントロピー最大化を目指した貪欲サンプリング（キーワード分布とセグメント数分布）\n最終的に約 100k のビデオ を選定（サンプリング率 1%）\n\n\n\n\n\n\n\nビデオを可変長のクリップ（10〜30 秒）に分割します。情報密度が高いクリップほど短い期間に設定 することで、アノテーターの負担を均等化しながら詳細な記述を促します。\n\n平均 4〜5 クリップ/動画 に分割\n\n\n\n\n\n\n\n\n\n\nTipなぜ音声記述を使うのか？\n\n\n\n音声記述（Spoken Captions）は、タイピングによる記述と比較して以下の利点があります:\n\n記述速度が速い: タイピングよりも自然に詳細を語れる\n自然な言語表現: 書き言葉よりも口語の方が流暢で豊かな記述が得られる\n認知負荷の軽減: タイピングに気を取られず、ビデオに集中できる\n\nこの手法は PixMo-Cap（画像キャプションデータセット）でも採用されており、高品質なキャプション生成に有効であることが実証されています。\n\n\nアノテーションプロセス:\n\nクリップ記述:\n\nアノテーターは短いクリップごとに音声で内容を説明（音声はミュート）\n画面上で起きていることを詳細に語る\nリアルタイム文字起こし（Whisper-1）が自動実行\nアノテーターは転写結果を編集し、誤認識を修正\n\nビデオ全体の要約:\n\nすべてのクリップ記述が完了した後、ビデオ全体の包括的な説明を記述\n\n質問ベースのプロンプト:\n\nアノテーターに「動的な視覚的詳細」を記述するよう促すため、事前定義された質問セットを提示\n例: 「オブジェクトや出来事は時間とともにどう変化したか？」\n\n\n\n\n\n\nWhisper の文字起こしは不完全な文や口語表現を含むため、テキスト専用 LLM で以下を実行:\n\n文の構造を整理し、一貫性を確保\n冗長性を削除し、読みやすさを向上\n元の意味を保持しながら流暢な文章に変換\n\n\n\n\n人手記述だけでは見落としがちな 低レベルの視覚的詳細 を補完するため:\n\nMolmo でフレームレベルのキャプションを生成:\n\n個々のフレームを Molmo（早期バージョン）でキャプション化\n色、テクスチャ、細かなオブジェクト属性などを記述\n\nLLM でマージ:\n\nクリップレベルのキャプションとフレームレベルのキャプションを統合\n重複を削除し、一貫性のある長文キャプションを生成\n\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 1: Video Selection                                   │\n│  10M+ clips -&gt; Filter -&gt; Diversity Sampling -&gt; 100k         │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 2: Human Annotation                                  │\n│  Split -&gt; Voice -&gt; Whisper -&gt; Edit                          │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 3: LLM Refinement                                    │\n│  Organize -&gt; Convert to coherent text                       │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 4: Molmo Integration                                 │\n│  Molmo frame captions -&gt; LLM merge -&gt; Final caption         │\n└─────────────────────────────────────────────────────────────┘\n\n\n\n\nビデオソース:\n\nYT-Temporal\nYouTube キーワード検索\n複数の大規模ビデオデータセット\n\nライセンス: Creative Commons（一部）\nフィルタリング:\n\n視覚的・時間的多様性が低いビデオを除外\n繰り返しパターンを含む低品質キャプションをヒューリスティックルールで除去\n\n\n\n\nMolmo2-Cap は、Molmo2 の 事前学習（Pre-training） フェーズで使用されます:\n\n長さ条件付きキャプション生成: モデルは指定された長さのキャプションを生成するよう学習\n重み付けサンプリング: ビデオキャプションデータには固定重み 0.1 を設定（他のタスクとバランス）\n\n\n\n\nMolmo2-Cap は以下の点で重要な貢献をしています:\n\nオープンソースの基盤: プロプライエタリモデル（GPT など）に依存しない完全にオープンなパイプライン\nビデオグラウンディングの基礎: 超詳細な記述により、時空間的なポインティングとトラッキングを学習\nデータ品質の新基準: 平均 924 語/動画という記述量は、今後のビデオキャプションデータセットのベンチマークとなる\n再現可能な手法: 音声記述 + LLM 整形 + Molmo 統合という明確なパイプラインは、他のプロジェクトでも再利用可能\n\n\n\n\nMolmo2 のビデオキャプション能力を評価するため、Molmo2-CapTest という評価セットが構築されています:\n\n693 の Creative Commons ライセンスビデオ\nMolmo2-Cap と同様のプロトコルで収集されたが、手動選定された高品質アノテーター が担当\n各ビデオに複数のリファレンスキャプションを用意\nF1 スコア でキャプション品質を評価\n\n\n\n\nMolmo2-Cap は、以下の革新的な設計により、史上最も詳細なビデオキャプションデータセットとなっています:\n\n音声記述 + Whisper: 自然で流暢な記述を効率的に収集\nLLM 整形: 口語表現を読みやすい文章に変換\nMolmo 統合: 低レベルの視覚的詳細を補完\n多様性ベースサンプリング: 視覚的・意味的に多様なビデオセットを構築\n\nこのデータセットは、Molmo2 がビデオグラウンディング（pointing & tracking）を実現する上で不可欠な基盤となっており、完全にオープンなビデオ VLM の可能性を示しています。\n\n関連セクション:\n\nVideo Grounding: Pointing & Tracking - ビデオポインティング・トラッキングデータセット\nMulti-Image Understanding - マルチイメージ理解データセット",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#概要",
    "href": "ja/molmo2/01-dense-video-captioning.html#概要",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の事前学習に使用される超詳細（dense）なビデオキャプションデータセットです。このデータセットは、従来のビデオキャプションデータセットと比較して桁違いに詳細な記述を含み、平均 924 語/動画 という驚異的な記述量を実現しています。\n従来の VLM が生成する短く表面的なキャプションとは異なり、Molmo2-Cap は動的イベントと細かな視覚的詳細の両方を捉えることで、モデルがビデオの時空間的理解を深める基盤を提供します。\nデータセット規模:\n\n104k のビデオレベルキャプション\n431k のクリップレベルキャプション\n平均 924 語/動画 の超詳細記述",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#なぜ-dense-captioning-が重要か",
    "href": "ja/molmo2/01-dense-video-captioning.html#なぜ-dense-captioning-が重要か",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "ビデオキャプションは画像キャプションよりも本質的に困難です。なぜなら、アノテーターは以下の両方を記述する必要があるからです:\n\n動的イベント: 時間とともに変化する出来事、動作、状態遷移\n細かな視覚的詳細: オブジェクトの外観、空間配置、属性の変化\n\n既存のビデオキャプションデータセットの多くは、表面的な記述に留まっており、ビデオグラウンディング（いつ・どこで何が起きたか）を学習するには不十分でした。Molmo2-Cap は、この gap を埋めるために設計されました。\n\n\n\nより詳細なキャプションは、モデルに以下の能力を与えます:\n\n時空間的な理解: 「いつ」「どこで」「何が」起きたかを正確に把握\n細粒度の視覚認識: 小さな物体、微細な動作、属性の変化を捉える\n文脈理解: イベント間の因果関係や時間的依存性を学習",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#既存データセットとの比較",
    "href": "ja/molmo2/01-dense-video-captioning.html#既存データセットとの比較",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Note他のビデオキャプションデータセットとの比較\n\n\n\n\n\nMolmo2-Cap は既存のビデオキャプションデータセットを大きく上回る記述量を実現しています:\n\n\n\nデータセット\n平均語数/動画\n特徴\n\n\n\n\nMolmo2-Cap\n924 語\n人手による音声記述 + Molmo 統合\n\n\nLLaVA-Video-178K\n547 語\nGPT ベースの合成キャプション\n\n\nShareGPT4-Video\n280 語\nGPT ベースの合成キャプション\n\n\nRDCap\n100 語\n既存データセット\n\n\nRCap\n89 語\n既存データセット\n\n\nVideo Localized Narratives\n75 語\n人手アノテーション\n\n\n\nMolmo2-Cap は、LLaVA-Video の 1.7 倍、ShareGPT4-Video の 3.3 倍、Video Localized Narratives の 12 倍 以上の記述量を持ちます。\n重要な差異:\n\nMolmo2-Cap は プロプライエタリモデル（GPT など）に依存しない 完全にオープンなパイプラインで構築されている\n人手による音声記述 をベースにしており、合成データよりも自然で詳細\nフレームレベルのキャプション統合 により、低レベルの視覚的詳細も漏れなく記述",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#データ収集パイプライン",
    "href": "ja/molmo2/01-dense-video-captioning.html#データ収集パイプライン",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap のデータ収集は、革新的な 2 段階パイプライン を採用しています。\n\n\n\n初期プール構築: 10M 以上のビデオクリップを複数の大規模ソース（YT-Temporal、YouTube など）から収集\n情報量フィルタリング:\n\n音声トラックを除去し、1 fps で均一サンプリング\nH.264 でエンコードし、正規化された情報量スコアを算出: bits / (duration × W × H)\n平均 - 1σ 未満のビデオを除外（視覚的・時間的多様性が低いビデオを排除）\n\n多様性ベースサンプリング:\n\nSAM 2 でフレームをセグメント化し、視覚的複雑さを推定\nMolmo でフレームをキャプション化し、MetaCLIP パイプラインでキーワード抽出\nエントロピー最大化を目指した貪欲サンプリング（キーワード分布とセグメント数分布）\n最終的に約 100k のビデオ を選定（サンプリング率 1%）\n\n\n\n\n\n\n\nビデオを可変長のクリップ（10〜30 秒）に分割します。情報密度が高いクリップほど短い期間に設定 することで、アノテーターの負担を均等化しながら詳細な記述を促します。\n\n平均 4〜5 クリップ/動画 に分割\n\n\n\n\n\n\n\n\n\n\nTipなぜ音声記述を使うのか？\n\n\n\n音声記述（Spoken Captions）は、タイピングによる記述と比較して以下の利点があります:\n\n記述速度が速い: タイピングよりも自然に詳細を語れる\n自然な言語表現: 書き言葉よりも口語の方が流暢で豊かな記述が得られる\n認知負荷の軽減: タイピングに気を取られず、ビデオに集中できる\n\nこの手法は PixMo-Cap（画像キャプションデータセット）でも採用されており、高品質なキャプション生成に有効であることが実証されています。\n\n\nアノテーションプロセス:\n\nクリップ記述:\n\nアノテーターは短いクリップごとに音声で内容を説明（音声はミュート）\n画面上で起きていることを詳細に語る\nリアルタイム文字起こし（Whisper-1）が自動実行\nアノテーターは転写結果を編集し、誤認識を修正\n\nビデオ全体の要約:\n\nすべてのクリップ記述が完了した後、ビデオ全体の包括的な説明を記述\n\n質問ベースのプロンプト:\n\nアノテーターに「動的な視覚的詳細」を記述するよう促すため、事前定義された質問セットを提示\n例: 「オブジェクトや出来事は時間とともにどう変化したか？」\n\n\n\n\n\n\nWhisper の文字起こしは不完全な文や口語表現を含むため、テキスト専用 LLM で以下を実行:\n\n文の構造を整理し、一貫性を確保\n冗長性を削除し、読みやすさを向上\n元の意味を保持しながら流暢な文章に変換\n\n\n\n\n人手記述だけでは見落としがちな 低レベルの視覚的詳細 を補完するため:\n\nMolmo でフレームレベルのキャプションを生成:\n\n個々のフレームを Molmo（早期バージョン）でキャプション化\n色、テクスチャ、細かなオブジェクト属性などを記述\n\nLLM でマージ:\n\nクリップレベルのキャプションとフレームレベルのキャプションを統合\n重複を削除し、一貫性のある長文キャプションを生成\n\n\n\n\n\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 1: Video Selection                                   │\n│  10M+ clips -&gt; Filter -&gt; Diversity Sampling -&gt; 100k         │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 2: Human Annotation                                  │\n│  Split -&gt; Voice -&gt; Whisper -&gt; Edit                          │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 3: LLM Refinement                                    │\n│  Organize -&gt; Convert to coherent text                       │\n└─────────────────────────────────────────────────────────────┘\n                              ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Stage 4: Molmo Integration                                 │\n│  Molmo frame captions -&gt; LLM merge -&gt; Final caption         │\n└─────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#データセットの統計",
    "href": "ja/molmo2/01-dense-video-captioning.html#データセットの統計",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "ビデオソース:\n\nYT-Temporal\nYouTube キーワード検索\n複数の大規模ビデオデータセット\n\nライセンス: Creative Commons（一部）\nフィルタリング:\n\n視覚的・時間的多様性が低いビデオを除外\n繰り返しパターンを含む低品質キャプションをヒューリスティックルールで除去",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#学習での使用",
    "href": "ja/molmo2/01-dense-video-captioning.html#学習での使用",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、Molmo2 の 事前学習（Pre-training） フェーズで使用されます:\n\n長さ条件付きキャプション生成: モデルは指定された長さのキャプションを生成するよう学習\n重み付けサンプリング: ビデオキャプションデータには固定重み 0.1 を設定（他のタスクとバランス）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#影響と貢献",
    "href": "ja/molmo2/01-dense-video-captioning.html#影響と貢献",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は以下の点で重要な貢献をしています:\n\nオープンソースの基盤: プロプライエタリモデル（GPT など）に依存しない完全にオープンなパイプライン\nビデオグラウンディングの基礎: 超詳細な記述により、時空間的なポインティングとトラッキングを学習\nデータ品質の新基準: 平均 924 語/動画という記述量は、今後のビデオキャプションデータセットのベンチマークとなる\n再現可能な手法: 音声記述 + LLM 整形 + Molmo 統合という明確なパイプラインは、他のプロジェクトでも再利用可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#評価-molmo2-captest",
    "href": "ja/molmo2/01-dense-video-captioning.html#評価-molmo2-captest",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2 のビデオキャプション能力を評価するため、Molmo2-CapTest という評価セットが構築されています:\n\n693 の Creative Commons ライセンスビデオ\nMolmo2-Cap と同様のプロトコルで収集されたが、手動選定された高品質アノテーター が担当\n各ビデオに複数のリファレンスキャプションを用意\nF1 スコア でキャプション品質を評価",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "ja/molmo2/01-dense-video-captioning.html#まとめ",
    "href": "ja/molmo2/01-dense-video-captioning.html#まとめ",
    "title": "Dense Video Captioning",
    "section": "",
    "text": "Molmo2-Cap は、以下の革新的な設計により、史上最も詳細なビデオキャプションデータセットとなっています:\n\n音声記述 + Whisper: 自然で流暢な記述を効率的に収集\nLLM 整形: 口語表現を読みやすい文章に変換\nMolmo 統合: 低レベルの視覚的詳細を補完\n多様性ベースサンプリング: 視覚的・意味的に多様なビデオセットを構築\n\nこのデータセットは、Molmo2 がビデオグラウンディング（pointing & tracking）を実現する上で不可欠な基盤となっており、完全にオープンなビデオ VLM の可能性を示しています。\n\n関連セクション:\n\nVideo Grounding: Pointing & Tracking - ビデオポインティング・トラッキングデータセット\nMulti-Image Understanding - マルチイメージ理解データセット",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Dense Video Captioning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Books",
    "section": "",
    "text": "Machine learning research notes.\n\n日本語 (Japanese)English\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMolmo2\n\n\n\nVLM\n\nMultimodal\n\n\n\n完全オープンな Vision-Language Model で、ビデオグラウンディングを実現\n\n\n\n\n\nFeb 3, 2026\n\n\nNaoto Iwase\n\n\n\n\n\n\n\n\n\n\n\n\nOlmo 3\n\n\n\nLLM\n\nReasoning\n\n\n\n完全オープンな言語・思考型モデル（7B/32B）\n\n\n\n\n\nFeb 2, 2026\n\n\nNaoto Iwase\n\n\n\n\n\n\n\n\n\n\n\n\nThe Principles of Deep Learning Theory\n\n\n\nDeep Learning\n\nStatistical Physics\n\n\n\n物理学の有効理論アプローチによる深層学習の理論的理解\n\n\n\n\n\nFeb 1, 2026\n\n\nNaoto Iwase\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNote\n\n\n\nEnglish translations are in progress. Please check the Japanese tab for now."
  },
  {
    "objectID": "ja/molmo2/00-overview.html",
    "href": "ja/molmo2/00-overview.html",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備えたことです。\n従来の VLM は画像や動画の内容を理解して説明することはできましたが、「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示す（grounding）能力が不足していました。Molmo2 は、ビデオ内の時空間的なポインティングとトラッキングを実現し、オープンソースモデルの中で最高水準の性能を達成しています。\n論文: arXiv:2601.10611\n主な貢献:\n\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）\nビデオグラウンディング（pointing & tracking）の実現\n超詳細なビデオキャプション（平均924語/動画）\n完全オープン（モデル、データ、コード）\n\nモデルサイズ:\n\nMolmo2-4B（Qwen3 LLM ベース）\nMolmo2-8B（Qwen3 LLM ベース）\nMolmo2-O-7B（OLMo LLM ベース、完全オープン）\n\n\n\n\n現在、最も強力な Video-Language Model (VLM) はプロプライエタリであり、ウェイト、データ、トレーニングレシピが公開されていません。また、オープンウェイトモデルの多くは、プロプライエタリモデルから合成データを生成する「蒸留」に依存しており、完全に独立したオープンな基盤が不足していました。\nさらに、既存の VLM には グラウンディング（grounding） 能力が欠けています。グラウンディングとは、モデルが「ロボットが赤いブロックを何回掴んだか？」という質問に対して、各掴みイベントの時空間座標を出力したり、「カップがいつテーブルから落ちたか？」に対してカップの軌跡（track）を返したりする能力です。\n画像グラウンディングは既に標準的な機能ですが、ビデオグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nMolmo2 は、この gap を埋めるために開発されました。\n\n\n\nMolmo2 の核心は、9つの新規データセット です。すべてプロプライエタリモデルからの蒸留を一切使用せず、人手アノテーションと LLM ベースの合成パイプラインで構築されています。\n\n\n\n\n\n\nImportantプロプライエタリモデル非依存の重要性\n\n\n\n多くのオープンモデル（LLaVA-Video, PLM, ShareGPT4Video など）は、GPT-4V や Gemini などのプロプライエタリモデルから合成データを生成する「蒸留」アプローチを採用しています。\nこの手法には以下の問題があります：\n\n透明性の欠如: プロプライエタリモデルの能力に依存するため、データの品質が不透明\nバイアスの継承: プロプライエタリモデルのバイアスや誤りがそのまま継承される\n改善の限界: 元モデルを超える性能を達成することが困難\n\nMolmo2 は、人手アノテーションと自前モデル（Molmo, Claude Sonnet 4.5）のみを使用することで、完全に独立したデータセット構築を実現しています。これにより、オープンソースコミュニティが SOTA を超える基盤を得られます。\n\n\n\n\n\n内容: 104k のビデオレベルキャプション + 431k のクリップレベルキャプション\n特徴: 平均 924 語/動画 という超詳細なキャプション\n\n既存データセットとの比較: Video Localized Narratives (75語), ShareGPT4-Video (280語), LLaVA-Video (547語)\n\nパイプライン:\n\nアノテーターが短いクリップを音声で説明（タイピングより詳細に記述可能）\nWhisper-1 で文字起こし\nLLM で文章を整形\nMolmo でフレームレベルのキャプションを生成し、統合\n\n\n\n詳細: Dense Video Captioning\n\n\n\n\n\n内容: 140k のビデオ QA ペア\n特徴: 人手による詳細な質問と回答\nパイプライン:\n\nビデオを31カテゴリにクラスタリングして多様性を確保\nアノテーターが詳細な質問を作成\nClaude Sonnet 4.5 が初期回答を生成\nアノテーターが反復的に回答を改善\n\n\n\n\n\n\nCapQA: 1M QA ペア（200k 動画、5 QA/動画）\n\nビデオをシーンに分割し、各シーンをキャプション化\nLLM がキャプションから QA を生成\n\nSubtitleQA: 300k QA ペア（100k 動画、3 QA/動画）\n\nWhisper-1 で字幕を抽出\n視覚情報と字幕の両方を使う推論問題を生成\n\n\n\n\n\n\n内容: 650k のビデオポインティングクエリ（280k 動画、平均6ポイント/動画）\nカテゴリ: 8種類\n\nObjects, Animals, Actions/Events\nReferring expressions, Indirect references\nSpatial references, Comparative references\nVisual artifacts/anomalies（生成動画用）\n\nパイプライン:\n\nLLM がキャプションからクエリを生成\nアノテーターがフレーム（2 fps）と正確な位置をクリック\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\n内容: 3.6k ビデオクリップ、15k の複雑な自然言語クエリ（平均2.28オブジェクト/クエリ）\n特徴: 既存のトラッキングアノテーションに対して、複雑なテキストクエリを作成\nパイプライン:\n\nセグメンテーションまたはバウンディングボックスのトラックを表示\nアノテーターがオブジェクトのサブセットに適用される非自明なクエリを作成\n別ラウンドで検証\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\nVideoPoint: 6つのデータセットから49k のポインティング・カウンティング QA に変換\nVideoTrack: 7つの Ref-VOS データセット + 11のバウンディングボックストラッキングデータセットを変換（SAM-2 でセグメンテーションマスク生成）\n\n\n\n\n\n内容: 45k 画像セット（96k ユニーク画像）、72k QA ペア\n特徴: 意味的に関連する画像セット（2-5枚、平均2.73枚）に対する QA\nパイプライン:\n\nキャプションの類似度で画像をグルーピング\nアノテーターが質問を作成\nLLM との反復ループで回答を改善\n\n\n\n詳細: Multi-Image Understanding\n\n\n\n\n\nMultiImagePoint: 470k のポインティング・カウンティング例（PixMo-Points からクラスタリングで生成）\nSynMultiImageQA: 188k の合成マルチイメージ例（CoSyn を拡張、チャート・表・文書など）\n\n\n\n\n\nMolmo2 は、標準的な VLM アーキテクチャを採用しています。\n┌─────────────────────────────────────────────────────────────┐\n│  Video Input (max 128 frames @ 2fps, or 384 for long ctx)   │\n│  or Image Input (1 crop + up to K=8 overlapping crops)      │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision Transformer (ViT)                                   │\n│  - Extracts patch-level features                            │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision-Language Connector                                  │\n│  - Uses features from 3rd-to-last & 9th-from-last ViT layer │\n│  - Attention pooling: 2x2 for images, 3x3 for video frames  │\n│  - Shared MLP projection                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  LLM (Qwen3 or OLMo)                                        │\n│  - Visual tokens + text timestamps (video) or image indices │\n│  - Bi-directional attention between vision tokens           │\n│  - Output: text + &lt;points&gt; (for grounding)                  │\n└─────────────────────────────────────────────────────────────┘\n主要な設計選択:\n\nCropping: 画像は最大24クロップ（推論時）、動画は2 fpsサンプリング\nBi-directional attention: 画像トークン同士が相互に attend 可能（性能向上）\nPointing フォーマット: 正規化された (x, y, timestamp/image_index, object_id) をプレーンテキストで出力\n\n\n詳細: Vision-Language Connector\n\n\n\n\n\n\nMolmo2 は 3段階 でトレーニングされます。\n\n\n\nデータ: PixMo-Cap（キャプション）、PixMo-Points（ポインティング）、Tulu（NLP）\nミキシング比率: 60% キャプション、30% ポインティング、10% NLP\nステップ数: 32k ステップ、バッチサイズ128（約4エポック）\n学習率: ViT、Connector、LLM で個別に設定\n\n\n\n\n\nデータ: PixMo + Molmo2 データセット + オープンソースビデオ/画像データセット\nカテゴリ別サンプリング: 手動で調整したサンプリングレート（Table 1参照）\nステップ数: 30k ステップ、バッチサイズ128、最大シーケンス長16,384\n\n\n\n\nカテゴリ\nサンプリング率\nデータセット数\n例数\n\n\n\n\nCaptions/Long QA\n13.6%\n6\n1.2M\n\n\nImage QA\n22.7%\n32\n2.4M\n\n\nVideo QA\n18.2%\n32\n2.4M\n\n\nImage Pointing\n9.1%\n4\n1.1M\n\n\nVideo Pointing\n13.6%\n7\n0.37M\n\n\nVideo Tracking\n13.6%\n22\n0.80M\n\n\nNLP\n9.1%\n1\n0.99M\n\n\n\n\n\n\n\nコンテキスト長: 36,864（Stage 2の2.25倍）\nフレーム数: F = 384（Stage 2 の3倍）\nステップ数: 2k ステップ\n並列化: Context Parallelism (CP) を使用、8 GPU で処理\n注意: オーバーヘッドが大きいため短期間のみ実施\n\n\n詳細: Long-Context Training\n\n\n\n\n\n\n\nデータには、単一トークン出力の多肢選択問題から、4000+トークンの長いビデオキャプションまで含まれます。長い出力が損失の大部分を占めてしまうと、短い回答タスクの性能が低下します。\n解決策: タスクごとに重み付けを調整\n\nビデオキャプション: 重み 0.1\nポインティング: 重み 0.2\nその他: \\(\\frac{4}{\\sqrt{n}}\\)（\\(n\\) = 回答トークン数）\n\n\n詳細: Token Weighting\n\n\n\n\n例によってトークン数が大きく異なる（数百～16k+）ため、padding を避けるために packing を使用します。Vision-language モデルでは、ViT 用のクロップと LLM 用のトークンの両方を効率的にパックする必要があります。\nMolmo2 は、オンザフライでパッキングするアルゴリズムを開発し、15倍 のトレーニング効率を達成しています。\n\n詳細: Packing & Message Trees\n\n\n\n\n1つの画像/動画に複数のアノテーションがある場合、メッセージツリー としてエンコードします。視覚入力が最初のメッセージとなり、各アノテーションが異なるブランチになります。ツリーは単一シーケンスに線形化され、カスタムアテンションマスクによってブランチ間のクロスアテンションを防ぎます。\n平均して、データ内の例には4つのアノテーションがあり、packing により16,348トークンのシーケンスに平均3.8例を詰め込むことができています。\n\n詳細: Packing & Message Trees\n\n\n\n\n\n\n\n\nMolmo2 は、標準的なビデオベンチマークと新規のキャプション・カウンティングベンチマークで評価されています。\n主要な結果:\n\n短尺ビデオ理解: オープンウェイトモデル中で SOTA\n\nNextQA: 86.2（Molmo2-8B）\nPerceptionTest: 82.1\nMVBench: 75.9\nMotionBench: 62.2\n\nキャプション: Molmo2-CapTest で F1 Score 43.2（Molmo2-8B）\n\nGPT-5 (50.1)、Gemini 2.5 Pro (42.1) に次ぐ性能\n\nカウンティング: Molmo2-VideoCount で 35.5% accuracy（Molmo2-8B）\n\nQwen3-VL-8B (29.6%) を大きく上回る\n\n長尺ビデオ: 最良のオープンウェイトモデル（Eagle2.5-8B など）には及ばない\n\n原因: オープンソースの長尺（10+分）トレーニングデータ不足\n\n\n\n\n\n\n\n\nNote長尺ビデオでの課題\n\n\n\n\n\nMolmo2 は、以下の長尺ビデオベンチマークで課題を抱えています：\n\nLongVideoBench: 67.5（Eagle2.5-8B: 66.4、PLM-8B: 56.9）\nMLVU: 60.2（Eagle2.5-8B: 60.4、PLM-8B: 52.6）\nLVBench: 52.8（Eagle2.5-8B: 50.9、PLM-8B: 44.5）\n\n原因:\n\nオープンデータ不足: 10分以上の動画に対する高品質なアノテーションが不足\n計算制約: Long-Context Training（Stage 3）は 2,000 ステップのみ実施（オーバーヘッドが大きい）\nトレードオフ: キャプション品質を優先したため、長尺ビデオタスクの性能がやや低下\n\nただし、Molmo2 の長尺ビデオ性能は依然として多くのオープンモデルを上回っており、完全オープンデータのみを使用していることを考慮すれば十分な成果です。\n\n詳細: Long-Context Training\n\n\n\n\nHuman Preference Study:\n\nElo スコア: 1057（Molmo2-8B）\nGemini 3 Pro (1082)、Gemini 2.5 Flash (1084) に次ぐ5位\n完全オープンモデルとしては最高性能\n\n\n\n\nMolmo2 の最大の強みは ビデオグラウンディング です。\nVideo Pointing:\n\nMolmo2-VP ベンチマーク（新規）で F1 Score 38.4\n\nGemini 3 Pro (20.0) を大きく上回る\nプロプライエタリモデルを含めて最高性能\n\n\nVideo Tracking:\n\nBURST（test）で accuracy 56.2\nMolmo2-VC（新規ベンチマーク）で J&F 41.1\n\nGemini 3 Pro を上回る（詳細はベンチマークによる）\n\n\n既存のオープンウェイトモデル（Qwen3-VL など）は、ビデオトラッキング機能を提供していないため、Molmo2 が新たな capability を開拓したと言えます。\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\nMolmo2 は画像タスクでも強力な性能を維持しています。\n\nMMMU: 47.9（Molmo2-8B）\nMathVista: 63.1\nChartQA: 79.5\nAI2D: 84.5\n\nビデオ能力を追加しても、画像タスクの性能を損なっていないことが確認されています。\n\n\n\n論文では、以下の要素の影響を検証しています。\n\nBi-directional attention on vision tokens: 有効（性能向上）\nToken weighting: 有効（長短出力のバランス改善）\nPacking: 15倍の効率向上\nMessage trees: 複数アノテーションの効率的な学習\n\n\n詳細: Token Weighting, Packing & Message Trees\n\n\n\n\n\nMolmo2 は、以下の研究の流れを統合しています。\n\nVision-Language Models: GPT-4V, Gemini, LLaVA, InternVL\nVideo Understanding: VideoChat, LLaVA-Video, PLM, Eagle\nGrounding: KOSMOS-2, Ferret, GPT-4V with set-of-mark\nVideo Grounding: Grounding-DINO, SAM-2, Ref-VOS\nOpen Data: PixMo, CoSyn, ShareGPT4Video\n\nMolmo2 の独自性は、完全オープン かつ ビデオグラウンディング を実現した点にあります。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\n\n\nプロプライエタリモデル:\n\nGPT-4V / GPT-5: 画像グラウンディングは可能だが、ビデオトラッキングは非サポート\nGemini 2.5 Pro / 3 Pro: 限定的なビデオグラウンディング機能を持つが、Molmo2 の pointing/tracking 性能に及ばない\nClaude Sonnet 4.5: ビデオグラウンディング機能なし\n\nオープンウェイトモデル:\n\nQwen3-VL（4B/8B）: ビデオ理解は強力だが、ビデオグラウンディング機能なし。Molmo2 が counting で上回る\nInternVL3.5（4B/8B）: 画像タスクに特化、ビデオグラウンディングなし\nLLaVA-Video-7B: プロプライエタリモデルから蒸留されたデータを使用。グラウンディング機能なし\nPLM（3B/8B）: 詳細なキャプションデータを持つが、Molmo2 より平均語数が少ない（Molmo2: 924語 vs PLM: 不明）\n\n完全オープンモデル:\n\nMolmo2-O-7B: OLMo LLM を使用した唯一の完全オープンモデル（ViT は依然としてプロプライエタリ）\n\nMolmo2 は、ビデオグラウンディング という新しい capability を開拓し、プロプライエタリモデルを含めて最高性能を達成した点で、他モデルと一線を画しています。\n\n\n\n\n\n\nMolmo2 は、完全オープンな VLM として、以下を達成しました。\n\n9つの新規データセット を構築（プロプライエタリモデルへの依存ゼロ）\nビデオグラウンディング（pointing & tracking）を実現\n短尺ビデオ理解 でオープンモデル中 SOTA\nキャプション・カウンティング でプロプライエタリモデルに迫る性能\n完全オープン（モデル、データ、コード）\n\n課題として、長尺ビデオ（10+分）でのパフォーマンスは最良のオープンウェイトモデルに及びませんが、これはオープンソースの長尺データ不足が原因です。\nMolmo2 は、オープンソースコミュニティが SOTA の VLM を構築するための強固な基盤を提供します。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#概要",
    "href": "ja/molmo2/00-overview.html#概要",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備えたことです。\n従来の VLM は画像や動画の内容を理解して説明することはできましたが、「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示す（grounding）能力が不足していました。Molmo2 は、ビデオ内の時空間的なポインティングとトラッキングを実現し、オープンソースモデルの中で最高水準の性能を達成しています。\n論文: arXiv:2601.10611\n主な貢献:\n\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）\nビデオグラウンディング（pointing & tracking）の実現\n超詳細なビデオキャプション（平均924語/動画）\n完全オープン（モデル、データ、コード）\n\nモデルサイズ:\n\nMolmo2-4B（Qwen3 LLM ベース）\nMolmo2-8B（Qwen3 LLM ベース）\nMolmo2-O-7B（OLMo LLM ベース、完全オープン）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#モチベーション-video-grounding-の重要性",
    "href": "ja/molmo2/00-overview.html#モチベーション-video-grounding-の重要性",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "現在、最も強力な Video-Language Model (VLM) はプロプライエタリであり、ウェイト、データ、トレーニングレシピが公開されていません。また、オープンウェイトモデルの多くは、プロプライエタリモデルから合成データを生成する「蒸留」に依存しており、完全に独立したオープンな基盤が不足していました。\nさらに、既存の VLM には グラウンディング（grounding） 能力が欠けています。グラウンディングとは、モデルが「ロボットが赤いブロックを何回掴んだか？」という質問に対して、各掴みイベントの時空間座標を出力したり、「カップがいつテーブルから落ちたか？」に対してカップの軌跡（track）を返したりする能力です。\n画像グラウンディングは既に標準的な機能ですが、ビデオグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nMolmo2 は、この gap を埋めるために開発されました。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#データセット-9つの新規データセット",
    "href": "ja/molmo2/00-overview.html#データセット-9つの新規データセット",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 の核心は、9つの新規データセット です。すべてプロプライエタリモデルからの蒸留を一切使用せず、人手アノテーションと LLM ベースの合成パイプラインで構築されています。\n\n\n\n\n\n\nImportantプロプライエタリモデル非依存の重要性\n\n\n\n多くのオープンモデル（LLaVA-Video, PLM, ShareGPT4Video など）は、GPT-4V や Gemini などのプロプライエタリモデルから合成データを生成する「蒸留」アプローチを採用しています。\nこの手法には以下の問題があります：\n\n透明性の欠如: プロプライエタリモデルの能力に依存するため、データの品質が不透明\nバイアスの継承: プロプライエタリモデルのバイアスや誤りがそのまま継承される\n改善の限界: 元モデルを超える性能を達成することが困難\n\nMolmo2 は、人手アノテーションと自前モデル（Molmo, Claude Sonnet 4.5）のみを使用することで、完全に独立したデータセット構築を実現しています。これにより、オープンソースコミュニティが SOTA を超える基盤を得られます。\n\n\n\n\n\n内容: 104k のビデオレベルキャプション + 431k のクリップレベルキャプション\n特徴: 平均 924 語/動画 という超詳細なキャプション\n\n既存データセットとの比較: Video Localized Narratives (75語), ShareGPT4-Video (280語), LLaVA-Video (547語)\n\nパイプライン:\n\nアノテーターが短いクリップを音声で説明（タイピングより詳細に記述可能）\nWhisper-1 で文字起こし\nLLM で文章を整形\nMolmo でフレームレベルのキャプションを生成し、統合\n\n\n\n詳細: Dense Video Captioning\n\n\n\n\n\n内容: 140k のビデオ QA ペア\n特徴: 人手による詳細な質問と回答\nパイプライン:\n\nビデオを31カテゴリにクラスタリングして多様性を確保\nアノテーターが詳細な質問を作成\nClaude Sonnet 4.5 が初期回答を生成\nアノテーターが反復的に回答を改善\n\n\n\n\n\n\nCapQA: 1M QA ペア（200k 動画、5 QA/動画）\n\nビデオをシーンに分割し、各シーンをキャプション化\nLLM がキャプションから QA を生成\n\nSubtitleQA: 300k QA ペア（100k 動画、3 QA/動画）\n\nWhisper-1 で字幕を抽出\n視覚情報と字幕の両方を使う推論問題を生成\n\n\n\n\n\n\n内容: 650k のビデオポインティングクエリ（280k 動画、平均6ポイント/動画）\nカテゴリ: 8種類\n\nObjects, Animals, Actions/Events\nReferring expressions, Indirect references\nSpatial references, Comparative references\nVisual artifacts/anomalies（生成動画用）\n\nパイプライン:\n\nLLM がキャプションからクエリを生成\nアノテーターがフレーム（2 fps）と正確な位置をクリック\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\n内容: 3.6k ビデオクリップ、15k の複雑な自然言語クエリ（平均2.28オブジェクト/クエリ）\n特徴: 既存のトラッキングアノテーションに対して、複雑なテキストクエリを作成\nパイプライン:\n\nセグメンテーションまたはバウンディングボックスのトラックを表示\nアノテーターがオブジェクトのサブセットに適用される非自明なクエリを作成\n別ラウンドで検証\n\n\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\n\nVideoPoint: 6つのデータセットから49k のポインティング・カウンティング QA に変換\nVideoTrack: 7つの Ref-VOS データセット + 11のバウンディングボックストラッキングデータセットを変換（SAM-2 でセグメンテーションマスク生成）\n\n\n\n\n\n内容: 45k 画像セット（96k ユニーク画像）、72k QA ペア\n特徴: 意味的に関連する画像セット（2-5枚、平均2.73枚）に対する QA\nパイプライン:\n\nキャプションの類似度で画像をグルーピング\nアノテーターが質問を作成\nLLM との反復ループで回答を改善\n\n\n\n詳細: Multi-Image Understanding\n\n\n\n\n\nMultiImagePoint: 470k のポインティング・カウンティング例（PixMo-Points からクラスタリングで生成）\nSynMultiImageQA: 188k の合成マルチイメージ例（CoSyn を拡張、チャート・表・文書など）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#アーキテクチャ",
    "href": "ja/molmo2/00-overview.html#アーキテクチャ",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、標準的な VLM アーキテクチャを採用しています。\n┌─────────────────────────────────────────────────────────────┐\n│  Video Input (max 128 frames @ 2fps, or 384 for long ctx)   │\n│  or Image Input (1 crop + up to K=8 overlapping crops)      │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision Transformer (ViT)                                   │\n│  - Extracts patch-level features                            │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  Vision-Language Connector                                  │\n│  - Uses features from 3rd-to-last & 9th-from-last ViT layer │\n│  - Attention pooling: 2x2 for images, 3x3 for video frames  │\n│  - Shared MLP projection                                    │\n└─────────────────────────────────────────────────────────────┘\n                            ↓\n┌─────────────────────────────────────────────────────────────┐\n│  LLM (Qwen3 or OLMo)                                        │\n│  - Visual tokens + text timestamps (video) or image indices │\n│  - Bi-directional attention between vision tokens           │\n│  - Output: text + &lt;points&gt; (for grounding)                  │\n└─────────────────────────────────────────────────────────────┘\n主要な設計選択:\n\nCropping: 画像は最大24クロップ（推論時）、動画は2 fpsサンプリング\nBi-directional attention: 画像トークン同士が相互に attend 可能（性能向上）\nPointing フォーマット: 正規化された (x, y, timestamp/image_index, object_id) をプレーンテキストで出力\n\n\n詳細: Vision-Language Connector",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#トレーニング",
    "href": "ja/molmo2/00-overview.html#トレーニング",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は 3段階 でトレーニングされます。\n\n\n\nデータ: PixMo-Cap（キャプション）、PixMo-Points（ポインティング）、Tulu（NLP）\nミキシング比率: 60% キャプション、30% ポインティング、10% NLP\nステップ数: 32k ステップ、バッチサイズ128（約4エポック）\n学習率: ViT、Connector、LLM で個別に設定\n\n\n\n\n\nデータ: PixMo + Molmo2 データセット + オープンソースビデオ/画像データセット\nカテゴリ別サンプリング: 手動で調整したサンプリングレート（Table 1参照）\nステップ数: 30k ステップ、バッチサイズ128、最大シーケンス長16,384\n\n\n\n\nカテゴリ\nサンプリング率\nデータセット数\n例数\n\n\n\n\nCaptions/Long QA\n13.6%\n6\n1.2M\n\n\nImage QA\n22.7%\n32\n2.4M\n\n\nVideo QA\n18.2%\n32\n2.4M\n\n\nImage Pointing\n9.1%\n4\n1.1M\n\n\nVideo Pointing\n13.6%\n7\n0.37M\n\n\nVideo Tracking\n13.6%\n22\n0.80M\n\n\nNLP\n9.1%\n1\n0.99M\n\n\n\n\n\n\n\nコンテキスト長: 36,864（Stage 2の2.25倍）\nフレーム数: F = 384（Stage 2 の3倍）\nステップ数: 2k ステップ\n並列化: Context Parallelism (CP) を使用、8 GPU で処理\n注意: オーバーヘッドが大きいため短期間のみ実施\n\n\n詳細: Long-Context Training\n\n\n\n\n\n\n\nデータには、単一トークン出力の多肢選択問題から、4000+トークンの長いビデオキャプションまで含まれます。長い出力が損失の大部分を占めてしまうと、短い回答タスクの性能が低下します。\n解決策: タスクごとに重み付けを調整\n\nビデオキャプション: 重み 0.1\nポインティング: 重み 0.2\nその他: \\(\\frac{4}{\\sqrt{n}}\\)（\\(n\\) = 回答トークン数）\n\n\n詳細: Token Weighting\n\n\n\n\n例によってトークン数が大きく異なる（数百～16k+）ため、padding を避けるために packing を使用します。Vision-language モデルでは、ViT 用のクロップと LLM 用のトークンの両方を効率的にパックする必要があります。\nMolmo2 は、オンザフライでパッキングするアルゴリズムを開発し、15倍 のトレーニング効率を達成しています。\n\n詳細: Packing & Message Trees\n\n\n\n\n1つの画像/動画に複数のアノテーションがある場合、メッセージツリー としてエンコードします。視覚入力が最初のメッセージとなり、各アノテーションが異なるブランチになります。ツリーは単一シーケンスに線形化され、カスタムアテンションマスクによってブランチ間のクロスアテンションを防ぎます。\n平均して、データ内の例には4つのアノテーションがあり、packing により16,348トークンのシーケンスに平均3.8例を詰め込むことができています。\n\n詳細: Packing & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#評価",
    "href": "ja/molmo2/00-overview.html#評価",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、標準的なビデオベンチマークと新規のキャプション・カウンティングベンチマークで評価されています。\n主要な結果:\n\n短尺ビデオ理解: オープンウェイトモデル中で SOTA\n\nNextQA: 86.2（Molmo2-8B）\nPerceptionTest: 82.1\nMVBench: 75.9\nMotionBench: 62.2\n\nキャプション: Molmo2-CapTest で F1 Score 43.2（Molmo2-8B）\n\nGPT-5 (50.1)、Gemini 2.5 Pro (42.1) に次ぐ性能\n\nカウンティング: Molmo2-VideoCount で 35.5% accuracy（Molmo2-8B）\n\nQwen3-VL-8B (29.6%) を大きく上回る\n\n長尺ビデオ: 最良のオープンウェイトモデル（Eagle2.5-8B など）には及ばない\n\n原因: オープンソースの長尺（10+分）トレーニングデータ不足\n\n\n\n\n\n\n\n\nNote長尺ビデオでの課題\n\n\n\n\n\nMolmo2 は、以下の長尺ビデオベンチマークで課題を抱えています：\n\nLongVideoBench: 67.5（Eagle2.5-8B: 66.4、PLM-8B: 56.9）\nMLVU: 60.2（Eagle2.5-8B: 60.4、PLM-8B: 52.6）\nLVBench: 52.8（Eagle2.5-8B: 50.9、PLM-8B: 44.5）\n\n原因:\n\nオープンデータ不足: 10分以上の動画に対する高品質なアノテーションが不足\n計算制約: Long-Context Training（Stage 3）は 2,000 ステップのみ実施（オーバーヘッドが大きい）\nトレードオフ: キャプション品質を優先したため、長尺ビデオタスクの性能がやや低下\n\nただし、Molmo2 の長尺ビデオ性能は依然として多くのオープンモデルを上回っており、完全オープンデータのみを使用していることを考慮すれば十分な成果です。\n\n詳細: Long-Context Training\n\n\n\n\nHuman Preference Study:\n\nElo スコア: 1057（Molmo2-8B）\nGemini 3 Pro (1082)、Gemini 2.5 Flash (1084) に次ぐ5位\n完全オープンモデルとしては最高性能\n\n\n\n\nMolmo2 の最大の強みは ビデオグラウンディング です。\nVideo Pointing:\n\nMolmo2-VP ベンチマーク（新規）で F1 Score 38.4\n\nGemini 3 Pro (20.0) を大きく上回る\nプロプライエタリモデルを含めて最高性能\n\n\nVideo Tracking:\n\nBURST（test）で accuracy 56.2\nMolmo2-VC（新規ベンチマーク）で J&F 41.1\n\nGemini 3 Pro を上回る（詳細はベンチマークによる）\n\n\n既存のオープンウェイトモデル（Qwen3-VL など）は、ビデオトラッキング機能を提供していないため、Molmo2 が新たな capability を開拓したと言えます。\n\n詳細: Video Grounding: Pointing & Tracking\n\n\n\n\nMolmo2 は画像タスクでも強力な性能を維持しています。\n\nMMMU: 47.9（Molmo2-8B）\nMathVista: 63.1\nChartQA: 79.5\nAI2D: 84.5\n\nビデオ能力を追加しても、画像タスクの性能を損なっていないことが確認されています。\n\n\n\n論文では、以下の要素の影響を検証しています。\n\nBi-directional attention on vision tokens: 有効（性能向上）\nToken weighting: 有効（長短出力のバランス改善）\nPacking: 15倍の効率向上\nMessage trees: 複数アノテーションの効率的な学習\n\n\n詳細: Token Weighting, Packing & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#関連研究",
    "href": "ja/molmo2/00-overview.html#関連研究",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、以下の研究の流れを統合しています。\n\nVision-Language Models: GPT-4V, Gemini, LLaVA, InternVL\nVideo Understanding: VideoChat, LLaVA-Video, PLM, Eagle\nGrounding: KOSMOS-2, Ferret, GPT-4V with set-of-mark\nVideo Grounding: Grounding-DINO, SAM-2, Ref-VOS\nOpen Data: PixMo, CoSyn, ShareGPT4Video\n\nMolmo2 の独自性は、完全オープン かつ ビデオグラウンディング を実現した点にあります。\n\n\n\n\n\n\nNote他モデルとの比較\n\n\n\n\n\nプロプライエタリモデル:\n\nGPT-4V / GPT-5: 画像グラウンディングは可能だが、ビデオトラッキングは非サポート\nGemini 2.5 Pro / 3 Pro: 限定的なビデオグラウンディング機能を持つが、Molmo2 の pointing/tracking 性能に及ばない\nClaude Sonnet 4.5: ビデオグラウンディング機能なし\n\nオープンウェイトモデル:\n\nQwen3-VL（4B/8B）: ビデオ理解は強力だが、ビデオグラウンディング機能なし。Molmo2 が counting で上回る\nInternVL3.5（4B/8B）: 画像タスクに特化、ビデオグラウンディングなし\nLLaVA-Video-7B: プロプライエタリモデルから蒸留されたデータを使用。グラウンディング機能なし\nPLM（3B/8B）: 詳細なキャプションデータを持つが、Molmo2 より平均語数が少ない（Molmo2: 924語 vs PLM: 不明）\n\n完全オープンモデル:\n\nMolmo2-O-7B: OLMo LLM を使用した唯一の完全オープンモデル（ViT は依然としてプロプライエタリ）\n\nMolmo2 は、ビデオグラウンディング という新しい capability を開拓し、プロプライエタリモデルを含めて最高性能を達成した点で、他モデルと一線を画しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/00-overview.html#結論",
    "href": "ja/molmo2/00-overview.html#結論",
    "title": "Molmo2 Technical Report まとめ",
    "section": "",
    "text": "Molmo2 は、完全オープンな VLM として、以下を達成しました。\n\n9つの新規データセット を構築（プロプライエタリモデルへの依存ゼロ）\nビデオグラウンディング（pointing & tracking）を実現\n短尺ビデオ理解 でオープンモデル中 SOTA\nキャプション・カウンティング でプロプライエタリモデルに迫る性能\n完全オープン（モデル、データ、コード）\n\n課題として、長尺ビデオ（10+分）でのパフォーマンスは最良のオープンウェイトモデルに及びませんが、これはオープンソースの長尺データ不足が原因です。\nMolmo2 は、オープンソースコミュニティが SOTA の VLM を構築するための強固な基盤を提供します。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "全体像"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html",
    "href": "ja/molmo2/02-video-grounding.html",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Video Grounding（ビデオグラウンディング） は、モデルがビデオ内の特定のオブジェクトやイベントを 時空間的に正確に指し示す（grounding） 能力です。従来の Vision-Language Model (VLM) は、「このビデオには何がありますか？」という質問に回答できても、「赤いブロックが何回掴まれましたか？それぞれどこですか？」という質問に対して正確な時刻と位置を返すことはできませんでした。\nMolmo2 は、この gap を埋めるために、Video Pointing と Video Tracking という2つの grounding 機能を実装しています。\n\n\n画像におけるグラウンディング（pointing）は既に標準的な機能となっており、Molmo2 の前身である Molmo1 や GPT-4V、Gemini などでサポートされています。しかし、ビデオにおけるグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nビデオグラウンディングは、以下のような実用的なユースケースで重要です。\n\nロボティクス: 「ロボットが赤いブロックを何回掴んだか？」といった質問に対して、各掴みイベントの時空間座標を返す\nビデオ検索: 「カップがいつテーブルから落ちたか？」という質問に対してカップの軌跡（track）を返す\n生成動画の品質評価: 生成されたビデオに視覚的な異常（artifacts/anomalies）がある箇所を自動検出する\n\n\n\n\n\nMolmo2 は、2種類のビデオグラウンディング機能を提供します。\n\n\nVideo Pointing は、ビデオ内の特定のフレームにおける特定のオブジェクトやイベントの 位置を点（points）で示す タスクです。複数フレームにわたる場合もありますが、各フレームは独立して扱われます。\n例:\n\n質問: 「滝をポイントして」\n回答: &lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 ...\"&gt;waterfall&lt;/points&gt;\n\n特徴:\n\nオブジェクトのカウンティング（counting）と組み合わせて使用されることが多い\n「何個ありますか？」という質問に加えて、「それぞれどこですか？」という空間的な情報を提供\nフレーム間でオブジェクトが移動しても、各フレームでの位置を個別に記録\n\n\n\n\nVideo Tracking は、ビデオ内の特定のオブジェクトを 時間を通して追跡（track） するタスクです。同一のオブジェクトが複数フレームにわたって移動する場合、そのオブジェクトの軌跡を一貫して記録します。\n例:\n\n質問: 「赤い車を追跡して」\n回答: オブジェクトごとに一意の ID を付与し、各フレームでの位置を記録\n\n特徴:\n\nオブジェクトの 一貫性 が重要（同一オブジェクトには同一 ID）\n複雑な自然言語クエリに対応（「左から2番目の選手」「緑のシャツを着た人」など）\n複数オブジェクトの同時追跡をサポート（平均2.28オブジェクト/クエリ）\n\n\n\n\n\n\n\nNotePointing vs Tracking の違い\n\n\n\n\nPointing: フレームごとに独立した位置情報（カウンティング重視）\nTracking: オブジェクトの時間的な一貫性（軌跡重視）\n\n実用上、Pointing は「いつどこにあるか」を知りたい場合に、Tracking は「どう動いたか」を知りたい場合に適しています。\n\n\n\n\n\n\nMolmo2-VideoPoint は、ビデオ内のオブジェクトやイベントをポイントするための人手アノテーションデータセットです。\n\n\n\n動画数: 280k 動画\nクエリ数: 650k 以上\n平均ポイント数: 6 ポイント/動画\nフレームレート: 2 fps でサンプリング\n\n\n\n\nMolmo2-VideoPoint は、以下の8つの多様なカテゴリをカバーしています。\n\nObjects（オブジェクト）: 一般的な物体（「車」「コップ」など）\nAnimals（動物）: 動物の検出とカウンティング\nActions/Events（行動・イベント）: 時間的なイベント（「ジャンプ」「投げる」など）\nReferring expressions（参照表現）: 複雑な記述（「左から2番目の人」など）\nIndirect references（間接参照）: 間接的な指示（「彼が持っている物」など）\nSpatial references（空間参照）: 空間的な関係（「テーブルの上にあるもの」など）\nComparative references（比較参照）: 比較的な記述（「一番大きい犬」など）\nVisual artifacts/anomalies（視覚的な異常）: 生成動画における異常検出\n\n\n\n\n\n\n\nTip生成動画の異常検出\n\n\n\nカテゴリ8の Visual artifacts/anomalies は、AI生成動画における品質評価のために設計されています。約25種類の text-to-video (T2V) モデルで生成された10k動画を使用し、消失する被写体（Vanishing Subject）、物理的な不整合（Physical Incongruity）、時間的な歪み（Temporal Dysmorphia）などの異常を検出する能力を学習します。\n\n\n\n\n\n\nクエリ生成: LLM が Molmo2-Cap で生成されたビデオキャプションからポインティングクエリを生成\nフレーム選択: アノテーターがオブジェクトが出現するフレームを特定（2 fps でサンプリング）\n位置アノテーション: アノテーターがオブジェクトの正確な位置をクリック\nフォーマット: 時刻（フレームインデックス）、カウント、正規化された (x, y) 座標を記録\n\n\n\n\n\nカウント数: 0-5個のオブジェクトが多数を占める（低カウント重視）\n\n中・高カウント例はトレーニング時にアップサンプリング\n\nフレーム数: アノテーション付きフレーム数は左側に偏った分布（多くの例は少数のフレームのみ）\nカテゴリ: Action/Event、Object、Referring expression が最も多い（これらが学習困難なため）\n\n\n\n\n\nMolmo2-VideoTrack は、複雑な自然言語クエリに対応したオブジェクトトラッキングデータセットです。\n\n\n\nビデオクリップ数: 3.6k（トレーニング用）+ 1.3k（評価用）= 合計約5k\nクエリ数: 15k の複雑な自然言語クエリ（トレーニング用）\n平均オブジェクト数: 2.28 オブジェクト/クエリ（多くは複数オブジェクトを追跡）\n平均クエリ長: 8.21 単語/クエリ\n動画長: 最長2分、多くは10-30秒\n平均アノテーション数: 6.08 オブジェクト/動画\n\n\n\n\nMolmo2-VideoTrack は、既存のセグメンテーションおよびバウンディングボックストラッキングデータセットを基に、人手で複雑なテキストクエリを追加したものです。\nセグメンテーションベース（一般的なオブジェクトトラッキング）:\n\nSAM-V, VIPSeg, MOSE, MOSEv2\n\nバウンディングボックスベース（ドメイン特化型）:\n\nスポーツ: TeamTrack, SoccerNet, SportsMOT\n自動運転: BDD100K\n動物: APTv2, AnimalTrack, BFT\nUAV（ドローン）: UAV-MOTD, SeaDrones\n人物: MOT20, PersonPath, DanceTrack\n\n\n\n\n\n\n\nImportantバウンディングボックスからセグメンテーションへの変換\n\n\n\nバウンディングボックスベースのデータセットでは、中心点がオブジェクト上にない可能性があるため、SAM 2 を使用して各バウンディングボックスをセグメンテーションマスクに変換しました。\n変換プロセス:\n\n最初のバウンディングボックスを SAM 2 にプロンプトとして入力\nセグメンテーションマスクを生成し、ビデオ全体に伝播\nIoU が 0.5 未満のトラックは除外\n生成されたマスクから中心付近の点をサンプリング\n\nこれにより、信頼性の高い点ベースのトラッキングアノテーションを得られます。\n\n\n\n\n\nMolmo2-VideoTrack の収集は、Ref-VOS（Referring Video Object Segmentation）のアプローチに従っています。\n\n既存トラックの表示: アノテーターにセグメンテーションまたはバウンディングボックスのトラックを表示\nクエリ作成: アノテーターがオブジェクトのサブセットに適用される 非自明な テキストクエリを作成\n\n例: 「緑のシャツを着た左から2番目の選手」「テーブルの上の赤いカップ」\n\n検証: 別のアノテーターが検証ラウンドでクエリの品質をチェック\n\n検証後、約70%のクエリが保持される\n\n\n\n\n\nMolmo2-VideoTrack は、多様なドメインをカバーしています。\n\n一般的なオブジェクト: 日常的な物体（セグメンテーションデータセットから）\nスポーツ: サッカー選手、チームメンバー、競技者\n交通: 車、歩行者、自転車\n動物: 野生動物、ペット\nUAV: ドローン映像における追跡\n人物: 歩行者、ダンサー\n\n複数オブジェクトの追跡が主な焦点であり、クエリの多くは複数のオブジェクトを同時に記述します（平均3.31オブジェクト/クエリ）。\n\n\n\n\nMolmo2 は、既存のオープンソースデータセットを Pointing と Tracking の形式に変換した Academic データセット も使用しています。\n\n\n既存のオブジェクトトラッキングアノテーションを 49k のポインティング・カウンティング QA に変換しました。\nソースデータセット（6つ）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-DAVIS17\n\n変換プロセス:\n\nオブジェクトが最初に出現するフレームのタイムスタンプを取得\nオブジェクトのマスク内からランダムに点をサンプリング（ガウス分布、マスク中心付近）\nポインティング QA 形式に変換\n\n\n\n\n既存のビデオオブジェクトセグメンテーション（VOS）およびトラッキングデータセットを変換しました。\nセグメンテーションベース（7つの Ref-VOS データセット）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-Youtube-VOS, Ref-DAVIS17\n\nバウンディングボックスベース（11のトラッキングデータセット）:\n\nTrackingNet, VastTrack, GOT-10k, LaSOT, TNL2K, WebUAV, WebUOT, LVOS V1/V2, UW-COT220, TNLLT, YouTube-VIS, MoCA-Video\n\nSAM 2 を使用してバウンディングボックスをセグメンテーションマスクに変換し、点ベースのトラッキングタスクを生成しました。\n\n\n\n\n\n\nNoteAcademicVideoTrack の規模\n\n\n\nAcademicVideoTrack は、トレーニングデータの大部分を占めており、130k のクエリと 800k の例（トークン数ベース）を提供しています。これに対して、Molmo2-VideoTrack は 8k のクエリですが、より複雑で多様なテキストクエリを含んでいます。\n\n\n\n\n\n\nMolmo2 は、ビデオグラウンディングにおいて プロプライエタリモデルを含めて最高水準 の性能を達成しています。\n\n\n以下の表は、BURST-VideoCount（VC）、Molmo2-VideoCount（Molmo2-VC）、Molmo2-VideoPoint（Molmo2-VP）における性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル\nBURST-VC Acc.\nBURST-VC Close Acc.\nMolmo2-VC Acc.\nMolmo2-VC Close Acc.\nMolmo2-VP F1\nMolmo2-VP Recall\nMolmo2-VP Precision\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\n\n\nGPT-5\n43.1\n73.7\n35.8\n50.3\n4.1\n4.4\n4.2\n\n\nGPT-5 mini\n46.0\n73.0\n29.8\n49.3\n2.2\n2.2\n2.2\n\n\nGemini 3 Pro\n44.0\n71.7\n37.1\n53.1\n20.0\n27.4\n19.8\n\n\nGemini 2.5 Pro\n41.6\n70.0\n35.8\n56.5\n13.0\n14.5\n13.6\n\n\nGemini 2.5 Flash\n38.7\n70.0\n31.9\n48.2\n11.1\n11.2\n12.2\n\n\nClaude Sonnet 4.5\n42.4\n72.6\n27.2\n45.1\n3.5\n3.7\n4.3\n\n\nOpen Weights Only\n\n\n\n\n\n\n\n\n\nQwen3-VL-4B\n38.9\n74.7\n25.3\n44.3\n0.0\n0.0\n0.0\n\n\nQwen3-VL-8B\n42.0\n74.4\n29.6\n47.7\n1.5\n1.5\n1.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\n\n\nMolmo2-4B\n61.5\n76.1\n34.3\n56.1\n39.9\n42.7\n39.4\n\n\nMolmo2-8B\n60.8\n75.0\n35.5\n53.3\n38.4\n39.3\n38.7\n\n\nMolmo2-O-7B\n61.6\n76.0\n33.2\n50.5\n35.8\n35.8\n37.9\n\n\n\n\n\n\n\n\n\nTip主要な結果\n\n\n\n\nBURST-VC: Molmo2 は全モデル中で最高精度（61.5% accuracy）を達成\nMolmo2-VP: Molmo2-4B は F1 Score 39.9 で、Gemini 3 Pro（20.0）の 約2倍 の性能\nQwen3-VL との比較: Qwen3-VL はビデオポインティングをほぼサポートしていない（F1 Score 0.0-1.5）\n\nMolmo2 は、オープンウェイトモデルとしてだけでなく、プロプライエタリモデルを含めても最高水準 のビデオポインティング性能を達成しています。\n\n\n評価指標の説明:\n\nAccuracy: 完全一致\nClose Accuracy: 誤差が Δ = 1 + ⌊0.05 × gt⌋ 以内であれば正解（カウント数が多いほど許容誤差が大きい）\nF1, Recall, Precision: 生成された点が ground-truth マスク内にあるかを評価\n\n\n\n\n以下の表は、主要なビデオトラッキングベンチマークにおける性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\nモデル\nMeViS valid J&F\nMeViS valid-u J&F\nRef-YT-VOS valid J&F\nRef-Davis test J&F\nReasonVOS J&F\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\nGPT-5\n23.4\n26.5\n30.9\n25.2\n24.7\n\n\nGPT-5 mini\n15.7\n15.4\n16.2\n8.4\n14.6\n\n\nGemini 3 Pro\n42.5\n51.1\n55.0\n66.6\n52.6\n\n\nGemini 2.5 Pro\n40.7\n52.8\n45.1\n45.6\n44.0\n\n\nGemini 2.5 Flash\n27.6\n31.8\n36.0\n31.6\n26.5\n\n\nOpen Weights Only\n\n\n\n\n\n\n\nQwen3-VL-4B\n29.7\n30.6\n32.1\n44.4\n26.5\n\n\nQwen3-VL-8B\n35.1\n34.4\n48.3\n41.0\n24.9\n\n\nSpecialized Open Models\n\n\n\n\n\n\n\nVideoLISA\n44.4\n53.2\n63.7\n68.8\n47.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\nMolmo2-4B\n56.2\n62.1\n67.2\n65.4\n56.5\n\n\nMolmo2-8B\n56.1\n60.4\n67.8\n64.5\n55.6\n\n\nMolmo2-O-7B\n54.5\n59.8\n64.8\n62.1\n51.9\n\n\n\n\n\n\n\n\n\nImportant特化型モデルとの比較\n\n\n\nVideoLISA は Ref-VOS に特化したモデルであり、一部のベンチマーク（MeViS valid-u, Ref-YT-VOS, Ref-Davis）で Molmo2 と同等またはそれ以上の性能を示しています。しかし、Molmo2 は 汎用的なビデオ理解モデル として、ビデオ QA、キャプション、カウンティングなど幅広いタスクをサポートしている点が異なります。\n\n\n評価指標の説明:\n\nJ&F: セグメンテーションマスクの品質を測る指標（Jaccard Index と Contour Accuracy の平均）\nF1, HOTA: オブジェクトトラッキングの精度を測る指標\n\n主要な結果:\n\nMeViS: Molmo2-4B は J&F 56.2 で、Gemini 3 Pro（42.5）を 13.7ポイント上回る\nRef-YT-VOS: Molmo2-8B は J&F 67.8 で、オープンモデル中で最高（VideoLISA 63.7 を上回る）\nQwen3-VL との比較: Molmo2 は Qwen3-VL-8B（35.1 J&F）の 約1.6倍 の性能\n\n\n\n\n\nMolmo2 は、ビデオグラウンディングの出力に プレーンテキスト座標 を使用しています。これは、特別なトークンや外部ツールを使わずに、LLM のテキスト生成能力だけでグラウンディングを実現するアプローチです。\n\n\n&lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 t^3 count_3 x_3 y_3\"&gt;\nobject_label\n&lt;/points&gt;\n要素の説明:\n\nt^i: フレームインデックス（またはタイムスタンプ）\ncount_i: そのフレームでのオブジェクトのカウント（何個目か）\nx_i, y_i: 正規化された座標（0.0-1.0）\nobject_label: オブジェクトの名前やラベル\n\n\n\n\nトラッキングでは、オブジェクトごとに一意の ID（count_i）を割り当て、複数フレームにわたって同じ ID を維持します。\n&lt;points coords=\"t^1 1 0.45 0.32 t^2 1 0.48 0.35 t^3 1 0.51 0.38\"&gt;\nred car\n&lt;/points&gt;\n&lt;points coords=\"t^1 2 0.62 0.55 t^2 2 0.65 0.57 t^3 2 0.68 0.59\"&gt;\nblue car\n&lt;/points&gt;\nこの例では、1 が赤い車、2 が青い車を示しており、各フレーム（t^1, t^2, t^3）での位置が記録されています。\n\n\n\n\n\n\nNotePlain-Text Coordinates の利点\n\n\n\n\nシンプル: 特別なトークンや外部ツールが不要\n柔軟性: LLM の生成能力をそのまま活用できる\nスケーラビリティ: 複数オブジェクト、複数フレームに自然に拡張可能\n人間可読性: デバッグや分析が容易\n\n一方で、座標の精度は LLM のテキスト生成精度に依存するため、非常に高精度な座標が必要な場合には専用のヘッドを追加するアプローチ（例: Grounding-DINO）の方が有利な場合もあります。\n\n\n\n\n\n\nMolmo2 は、Video Grounding という新しい capability を完全オープンなモデルとして実現しました。\n主要な成果:\n\n2つのグラウンディング機能:\n\nVideo Pointing: フレームごとの位置情報とカウンティング\nVideo Tracking: オブジェクトの時間的な軌跡追跡\n\n大規模な人手アノテーションデータセット:\n\nMolmo2-VideoPoint: 650k クエリ、8つの多様なカテゴリ\nMolmo2-VideoTrack: 15k クエリ、平均2.28オブジェクト/クエリ\n\nAcademic データセットの活用:\n\n既存のオープンソースデータセットを Pointing/Tracking 形式に変換\n49k の Pointing QA、130k の Tracking クエリ\n\nプロプライエタリモデルを上回る性能:\n\nVideo Pointing で F1 Score 39.9（Gemini 3 Pro の約2倍）\nVideo Tracking で J&F 56.2（Gemini 3 Pro より13.7ポイント高い）\n\nPlain-Text Coordinates フォーマット:\n\nシンプルで拡張性の高い出力形式\nLLM の生成能力を直接活用\n\n\nMolmo2 のビデオグラウンディング機能は、ロボティクス、ビデオ検索、生成動画の品質評価など、幅広い実用的なアプリケーションへの道を開きます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#video-grounding-とは",
    "href": "ja/molmo2/02-video-grounding.html#video-grounding-とは",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Video Grounding（ビデオグラウンディング） は、モデルがビデオ内の特定のオブジェクトやイベントを 時空間的に正確に指し示す（grounding） 能力です。従来の Vision-Language Model (VLM) は、「このビデオには何がありますか？」という質問に回答できても、「赤いブロックが何回掴まれましたか？それぞれどこですか？」という質問に対して正確な時刻と位置を返すことはできませんでした。\nMolmo2 は、この gap を埋めるために、Video Pointing と Video Tracking という2つの grounding 機能を実装しています。\n\n\n画像におけるグラウンディング（pointing）は既に標準的な機能となっており、Molmo2 の前身である Molmo1 や GPT-4V、Gemini などでサポートされています。しかし、ビデオにおけるグラウンディングは一部のプロプライエタリシステムでのみ限定的にサポートされており、オープンソースでは未開拓の領域でした。\nビデオグラウンディングは、以下のような実用的なユースケースで重要です。\n\nロボティクス: 「ロボットが赤いブロックを何回掴んだか？」といった質問に対して、各掴みイベントの時空間座標を返す\nビデオ検索: 「カップがいつテーブルから落ちたか？」という質問に対してカップの軌跡（track）を返す\n生成動画の品質評価: 生成されたビデオに視覚的な異常（artifacts/anomalies）がある箇所を自動検出する",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#video-pointing-vs-video-tracking",
    "href": "ja/molmo2/02-video-grounding.html#video-pointing-vs-video-tracking",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、2種類のビデオグラウンディング機能を提供します。\n\n\nVideo Pointing は、ビデオ内の特定のフレームにおける特定のオブジェクトやイベントの 位置を点（points）で示す タスクです。複数フレームにわたる場合もありますが、各フレームは独立して扱われます。\n例:\n\n質問: 「滝をポイントして」\n回答: &lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 ...\"&gt;waterfall&lt;/points&gt;\n\n特徴:\n\nオブジェクトのカウンティング（counting）と組み合わせて使用されることが多い\n「何個ありますか？」という質問に加えて、「それぞれどこですか？」という空間的な情報を提供\nフレーム間でオブジェクトが移動しても、各フレームでの位置を個別に記録\n\n\n\n\nVideo Tracking は、ビデオ内の特定のオブジェクトを 時間を通して追跡（track） するタスクです。同一のオブジェクトが複数フレームにわたって移動する場合、そのオブジェクトの軌跡を一貫して記録します。\n例:\n\n質問: 「赤い車を追跡して」\n回答: オブジェクトごとに一意の ID を付与し、各フレームでの位置を記録\n\n特徴:\n\nオブジェクトの 一貫性 が重要（同一オブジェクトには同一 ID）\n複雑な自然言語クエリに対応（「左から2番目の選手」「緑のシャツを着た人」など）\n複数オブジェクトの同時追跡をサポート（平均2.28オブジェクト/クエリ）\n\n\n\n\n\n\n\nNotePointing vs Tracking の違い\n\n\n\n\nPointing: フレームごとに独立した位置情報（カウンティング重視）\nTracking: オブジェクトの時間的な一貫性（軌跡重視）\n\n実用上、Pointing は「いつどこにあるか」を知りたい場合に、Tracking は「どう動いたか」を知りたい場合に適しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#molmo2-videopoint-データセット",
    "href": "ja/molmo2/02-video-grounding.html#molmo2-videopoint-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2-VideoPoint は、ビデオ内のオブジェクトやイベントをポイントするための人手アノテーションデータセットです。\n\n\n\n動画数: 280k 動画\nクエリ数: 650k 以上\n平均ポイント数: 6 ポイント/動画\nフレームレート: 2 fps でサンプリング\n\n\n\n\nMolmo2-VideoPoint は、以下の8つの多様なカテゴリをカバーしています。\n\nObjects（オブジェクト）: 一般的な物体（「車」「コップ」など）\nAnimals（動物）: 動物の検出とカウンティング\nActions/Events（行動・イベント）: 時間的なイベント（「ジャンプ」「投げる」など）\nReferring expressions（参照表現）: 複雑な記述（「左から2番目の人」など）\nIndirect references（間接参照）: 間接的な指示（「彼が持っている物」など）\nSpatial references（空間参照）: 空間的な関係（「テーブルの上にあるもの」など）\nComparative references（比較参照）: 比較的な記述（「一番大きい犬」など）\nVisual artifacts/anomalies（視覚的な異常）: 生成動画における異常検出\n\n\n\n\n\n\n\nTip生成動画の異常検出\n\n\n\nカテゴリ8の Visual artifacts/anomalies は、AI生成動画における品質評価のために設計されています。約25種類の text-to-video (T2V) モデルで生成された10k動画を使用し、消失する被写体（Vanishing Subject）、物理的な不整合（Physical Incongruity）、時間的な歪み（Temporal Dysmorphia）などの異常を検出する能力を学習します。\n\n\n\n\n\n\nクエリ生成: LLM が Molmo2-Cap で生成されたビデオキャプションからポインティングクエリを生成\nフレーム選択: アノテーターがオブジェクトが出現するフレームを特定（2 fps でサンプリング）\n位置アノテーション: アノテーターがオブジェクトの正確な位置をクリック\nフォーマット: 時刻（フレームインデックス）、カウント、正規化された (x, y) 座標を記録\n\n\n\n\n\nカウント数: 0-5個のオブジェクトが多数を占める（低カウント重視）\n\n中・高カウント例はトレーニング時にアップサンプリング\n\nフレーム数: アノテーション付きフレーム数は左側に偏った分布（多くの例は少数のフレームのみ）\nカテゴリ: Action/Event、Object、Referring expression が最も多い（これらが学習困難なため）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#molmo2-videotrack-データセット",
    "href": "ja/molmo2/02-video-grounding.html#molmo2-videotrack-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2-VideoTrack は、複雑な自然言語クエリに対応したオブジェクトトラッキングデータセットです。\n\n\n\nビデオクリップ数: 3.6k（トレーニング用）+ 1.3k（評価用）= 合計約5k\nクエリ数: 15k の複雑な自然言語クエリ（トレーニング用）\n平均オブジェクト数: 2.28 オブジェクト/クエリ（多くは複数オブジェクトを追跡）\n平均クエリ長: 8.21 単語/クエリ\n動画長: 最長2分、多くは10-30秒\n平均アノテーション数: 6.08 オブジェクト/動画\n\n\n\n\nMolmo2-VideoTrack は、既存のセグメンテーションおよびバウンディングボックストラッキングデータセットを基に、人手で複雑なテキストクエリを追加したものです。\nセグメンテーションベース（一般的なオブジェクトトラッキング）:\n\nSAM-V, VIPSeg, MOSE, MOSEv2\n\nバウンディングボックスベース（ドメイン特化型）:\n\nスポーツ: TeamTrack, SoccerNet, SportsMOT\n自動運転: BDD100K\n動物: APTv2, AnimalTrack, BFT\nUAV（ドローン）: UAV-MOTD, SeaDrones\n人物: MOT20, PersonPath, DanceTrack\n\n\n\n\n\n\n\nImportantバウンディングボックスからセグメンテーションへの変換\n\n\n\nバウンディングボックスベースのデータセットでは、中心点がオブジェクト上にない可能性があるため、SAM 2 を使用して各バウンディングボックスをセグメンテーションマスクに変換しました。\n変換プロセス:\n\n最初のバウンディングボックスを SAM 2 にプロンプトとして入力\nセグメンテーションマスクを生成し、ビデオ全体に伝播\nIoU が 0.5 未満のトラックは除外\n生成されたマスクから中心付近の点をサンプリング\n\nこれにより、信頼性の高い点ベースのトラッキングアノテーションを得られます。\n\n\n\n\n\nMolmo2-VideoTrack の収集は、Ref-VOS（Referring Video Object Segmentation）のアプローチに従っています。\n\n既存トラックの表示: アノテーターにセグメンテーションまたはバウンディングボックスのトラックを表示\nクエリ作成: アノテーターがオブジェクトのサブセットに適用される 非自明な テキストクエリを作成\n\n例: 「緑のシャツを着た左から2番目の選手」「テーブルの上の赤いカップ」\n\n検証: 別のアノテーターが検証ラウンドでクエリの品質をチェック\n\n検証後、約70%のクエリが保持される\n\n\n\n\n\nMolmo2-VideoTrack は、多様なドメインをカバーしています。\n\n一般的なオブジェクト: 日常的な物体（セグメンテーションデータセットから）\nスポーツ: サッカー選手、チームメンバー、競技者\n交通: 車、歩行者、自転車\n動物: 野生動物、ペット\nUAV: ドローン映像における追跡\n人物: 歩行者、ダンサー\n\n複数オブジェクトの追跡が主な焦点であり、クエリの多くは複数のオブジェクトを同時に記述します（平均3.31オブジェクト/クエリ）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#academic-データセット",
    "href": "ja/molmo2/02-video-grounding.html#academic-データセット",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、既存のオープンソースデータセットを Pointing と Tracking の形式に変換した Academic データセット も使用しています。\n\n\n既存のオブジェクトトラッキングアノテーションを 49k のポインティング・カウンティング QA に変換しました。\nソースデータセット（6つ）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-DAVIS17\n\n変換プロセス:\n\nオブジェクトが最初に出現するフレームのタイムスタンプを取得\nオブジェクトのマスク内からランダムに点をサンプリング（ガウス分布、マスク中心付近）\nポインティング QA 形式に変換\n\n\n\n\n既存のビデオオブジェクトセグメンテーション（VOS）およびトラッキングデータセットを変換しました。\nセグメンテーションベース（7つの Ref-VOS データセット）:\n\nMeViS, ReVOS, LV-VIS, OVIS, BURST, Ref-Youtube-VOS, Ref-DAVIS17\n\nバウンディングボックスベース（11のトラッキングデータセット）:\n\nTrackingNet, VastTrack, GOT-10k, LaSOT, TNL2K, WebUAV, WebUOT, LVOS V1/V2, UW-COT220, TNLLT, YouTube-VIS, MoCA-Video\n\nSAM 2 を使用してバウンディングボックスをセグメンテーションマスクに変換し、点ベースのトラッキングタスクを生成しました。\n\n\n\n\n\n\nNoteAcademicVideoTrack の規模\n\n\n\nAcademicVideoTrack は、トレーニングデータの大部分を占めており、130k のクエリと 800k の例（トークン数ベース）を提供しています。これに対して、Molmo2-VideoTrack は 8k のクエリですが、より複雑で多様なテキストクエリを含んでいます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#評価結果-プロプライエタリモデルを上回る性能",
    "href": "ja/molmo2/02-video-grounding.html#評価結果-プロプライエタリモデルを上回る性能",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、ビデオグラウンディングにおいて プロプライエタリモデルを含めて最高水準 の性能を達成しています。\n\n\n以下の表は、BURST-VideoCount（VC）、Molmo2-VideoCount（Molmo2-VC）、Molmo2-VideoPoint（Molmo2-VP）における性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル\nBURST-VC Acc.\nBURST-VC Close Acc.\nMolmo2-VC Acc.\nMolmo2-VC Close Acc.\nMolmo2-VP F1\nMolmo2-VP Recall\nMolmo2-VP Precision\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\n\n\nGPT-5\n43.1\n73.7\n35.8\n50.3\n4.1\n4.4\n4.2\n\n\nGPT-5 mini\n46.0\n73.0\n29.8\n49.3\n2.2\n2.2\n2.2\n\n\nGemini 3 Pro\n44.0\n71.7\n37.1\n53.1\n20.0\n27.4\n19.8\n\n\nGemini 2.5 Pro\n41.6\n70.0\n35.8\n56.5\n13.0\n14.5\n13.6\n\n\nGemini 2.5 Flash\n38.7\n70.0\n31.9\n48.2\n11.1\n11.2\n12.2\n\n\nClaude Sonnet 4.5\n42.4\n72.6\n27.2\n45.1\n3.5\n3.7\n4.3\n\n\nOpen Weights Only\n\n\n\n\n\n\n\n\n\nQwen3-VL-4B\n38.9\n74.7\n25.3\n44.3\n0.0\n0.0\n0.0\n\n\nQwen3-VL-8B\n42.0\n74.4\n29.6\n47.7\n1.5\n1.5\n1.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\n\n\nMolmo2-4B\n61.5\n76.1\n34.3\n56.1\n39.9\n42.7\n39.4\n\n\nMolmo2-8B\n60.8\n75.0\n35.5\n53.3\n38.4\n39.3\n38.7\n\n\nMolmo2-O-7B\n61.6\n76.0\n33.2\n50.5\n35.8\n35.8\n37.9\n\n\n\n\n\n\n\n\n\nTip主要な結果\n\n\n\n\nBURST-VC: Molmo2 は全モデル中で最高精度（61.5% accuracy）を達成\nMolmo2-VP: Molmo2-4B は F1 Score 39.9 で、Gemini 3 Pro（20.0）の 約2倍 の性能\nQwen3-VL との比較: Qwen3-VL はビデオポインティングをほぼサポートしていない（F1 Score 0.0-1.5）\n\nMolmo2 は、オープンウェイトモデルとしてだけでなく、プロプライエタリモデルを含めても最高水準 のビデオポインティング性能を達成しています。\n\n\n評価指標の説明:\n\nAccuracy: 完全一致\nClose Accuracy: 誤差が Δ = 1 + ⌊0.05 × gt⌋ 以内であれば正解（カウント数が多いほど許容誤差が大きい）\nF1, Recall, Precision: 生成された点が ground-truth マスク内にあるかを評価\n\n\n\n\n以下の表は、主要なビデオトラッキングベンチマークにおける性能を示しています。\n\n\n\n\n\n\n\n\n\n\n\nモデル\nMeViS valid J&F\nMeViS valid-u J&F\nRef-YT-VOS valid J&F\nRef-Davis test J&F\nReasonVOS J&F\n\n\n\n\nAPI Only\n\n\n\n\n\n\n\nGPT-5\n23.4\n26.5\n30.9\n25.2\n24.7\n\n\nGPT-5 mini\n15.7\n15.4\n16.2\n8.4\n14.6\n\n\nGemini 3 Pro\n42.5\n51.1\n55.0\n66.6\n52.6\n\n\nGemini 2.5 Pro\n40.7\n52.8\n45.1\n45.6\n44.0\n\n\nGemini 2.5 Flash\n27.6\n31.8\n36.0\n31.6\n26.5\n\n\nOpen Weights Only\n\n\n\n\n\n\n\nQwen3-VL-4B\n29.7\n30.6\n32.1\n44.4\n26.5\n\n\nQwen3-VL-8B\n35.1\n34.4\n48.3\n41.0\n24.9\n\n\nSpecialized Open Models\n\n\n\n\n\n\n\nVideoLISA\n44.4\n53.2\n63.7\n68.8\n47.5\n\n\nMolmo2 Family\n\n\n\n\n\n\n\nMolmo2-4B\n56.2\n62.1\n67.2\n65.4\n56.5\n\n\nMolmo2-8B\n56.1\n60.4\n67.8\n64.5\n55.6\n\n\nMolmo2-O-7B\n54.5\n59.8\n64.8\n62.1\n51.9\n\n\n\n\n\n\n\n\n\nImportant特化型モデルとの比較\n\n\n\nVideoLISA は Ref-VOS に特化したモデルであり、一部のベンチマーク（MeViS valid-u, Ref-YT-VOS, Ref-Davis）で Molmo2 と同等またはそれ以上の性能を示しています。しかし、Molmo2 は 汎用的なビデオ理解モデル として、ビデオ QA、キャプション、カウンティングなど幅広いタスクをサポートしている点が異なります。\n\n\n評価指標の説明:\n\nJ&F: セグメンテーションマスクの品質を測る指標（Jaccard Index と Contour Accuracy の平均）\nF1, HOTA: オブジェクトトラッキングの精度を測る指標\n\n主要な結果:\n\nMeViS: Molmo2-4B は J&F 56.2 で、Gemini 3 Pro（42.5）を 13.7ポイント上回る\nRef-YT-VOS: Molmo2-8B は J&F 67.8 で、オープンモデル中で最高（VideoLISA 63.7 を上回る）\nQwen3-VL との比較: Molmo2 は Qwen3-VL-8B（35.1 J&F）の 約1.6倍 の性能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#pointing-フォーマット-plain-text-coordinates",
    "href": "ja/molmo2/02-video-grounding.html#pointing-フォーマット-plain-text-coordinates",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、ビデオグラウンディングの出力に プレーンテキスト座標 を使用しています。これは、特別なトークンや外部ツールを使わずに、LLM のテキスト生成能力だけでグラウンディングを実現するアプローチです。\n\n\n&lt;points coords=\"t^1 count_1 x_1 y_1 t^2 count_2 x_2 y_2 t^3 count_3 x_3 y_3\"&gt;\nobject_label\n&lt;/points&gt;\n要素の説明:\n\nt^i: フレームインデックス（またはタイムスタンプ）\ncount_i: そのフレームでのオブジェクトのカウント（何個目か）\nx_i, y_i: 正規化された座標（0.0-1.0）\nobject_label: オブジェクトの名前やラベル\n\n\n\n\nトラッキングでは、オブジェクトごとに一意の ID（count_i）を割り当て、複数フレームにわたって同じ ID を維持します。\n&lt;points coords=\"t^1 1 0.45 0.32 t^2 1 0.48 0.35 t^3 1 0.51 0.38\"&gt;\nred car\n&lt;/points&gt;\n&lt;points coords=\"t^1 2 0.62 0.55 t^2 2 0.65 0.57 t^3 2 0.68 0.59\"&gt;\nblue car\n&lt;/points&gt;\nこの例では、1 が赤い車、2 が青い車を示しており、各フレーム（t^1, t^2, t^3）での位置が記録されています。\n\n\n\n\n\n\nNotePlain-Text Coordinates の利点\n\n\n\n\nシンプル: 特別なトークンや外部ツールが不要\n柔軟性: LLM の生成能力をそのまま活用できる\nスケーラビリティ: 複数オブジェクト、複数フレームに自然に拡張可能\n人間可読性: デバッグや分析が容易\n\n一方で、座標の精度は LLM のテキスト生成精度に依存するため、非常に高精度な座標が必要な場合には専用のヘッドを追加するアプローチ（例: Grounding-DINO）の方が有利な場合もあります。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/02-video-grounding.html#まとめ",
    "href": "ja/molmo2/02-video-grounding.html#まとめ",
    "title": "Video Grounding: Pointing & Tracking",
    "section": "",
    "text": "Molmo2 は、Video Grounding という新しい capability を完全オープンなモデルとして実現しました。\n主要な成果:\n\n2つのグラウンディング機能:\n\nVideo Pointing: フレームごとの位置情報とカウンティング\nVideo Tracking: オブジェクトの時間的な軌跡追跡\n\n大規模な人手アノテーションデータセット:\n\nMolmo2-VideoPoint: 650k クエリ、8つの多様なカテゴリ\nMolmo2-VideoTrack: 15k クエリ、平均2.28オブジェクト/クエリ\n\nAcademic データセットの活用:\n\n既存のオープンソースデータセットを Pointing/Tracking 形式に変換\n49k の Pointing QA、130k の Tracking クエリ\n\nプロプライエタリモデルを上回る性能:\n\nVideo Pointing で F1 Score 39.9（Gemini 3 Pro の約2倍）\nVideo Tracking で J&F 56.2（Gemini 3 Pro より13.7ポイント高い）\n\nPlain-Text Coordinates フォーマット:\n\nシンプルで拡張性の高い出力形式\nLLM の生成能力を直接活用\n\n\nMolmo2 のビデオグラウンディング機能は、ロボティクス、ビデオ検索、生成動画の品質評価など、幅広い実用的なアプリケーションへの道を開きます。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Video Grounding: Pointing & Tracking"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html",
    "href": "ja/molmo2/04-vision-language-connector.html",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector は、Vision Transformer (ViT) が抽出した視覚特徴を、Large Language Model (LLM) が処理できる形式に変換する重要なモジュールです。Molmo2 では、標準的な VLM アーキテクチャ [@Clark2024Molmo] に従い、画像とビデオの両方を統一的に処理できる設計を採用しています。\n\n\n\n\n\n\nflowchart TD\n    A[Visual Input&lt;br/&gt;Image or Video] --&gt; B[ViT Encoding&lt;br/&gt;Patch-level Features]\n    B --&gt; C[Multi-layer Feature&lt;br/&gt;Extraction]\n    C --&gt; D{Input Type}\n    D --&gt;|Image| E[2×2 Attention&lt;br/&gt;Pooling]\n    D --&gt;|Video Frame| F[3×3 Attention&lt;br/&gt;Pooling]\n    E --&gt; G[Shared MLP&lt;br/&gt;Projection]\n    F --&gt; G\n    G --&gt; H[Visual Tokens&lt;br/&gt;for LLM]\n\n    style C fill:#e6f0ff\n    style E fill:#ffe6f0\n    style F fill:#ffe6f0\n    style G fill:#f0ffe6\n\n\n\n\nFigure 1: Vision-Language Connector のデータフロー\n\n\n\n\n\n\n\n\n\n\nMolmo2 の Vision-Language Connector は、ViT の単一層ではなく、複数の層から特徴を抽出 します。\n\nThird-to-last layer（最終層から3番目）: 高レベルのセマンティック特徴\nNinth-from-last layer（最終層から9番目）: 中レベルの特徴\n\nこの設計は、先行研究の Molmo [@Clark2024Molmo] に従っており、異なる抽象度の視覚情報を組み合わせることで、より豊かな表現を実現しています。\nViT Layer Structure:\n┌─────────────────────────────────┐\n│  Layer 0 (Input)                │\n│  Layer 1                        │\n│  ...                            │\n│  Layer N-9  ◄── 9th-from-last  │ ─┐\n│  ...                            │  │\n│  Layer N-3  ◄── 3rd-to-last    │ ─┤ Features used\n│  Layer N-2                      │  │ by Connector\n│  Layer N-1                      │  │\n│  Layer N (Output)               │ ─┘\n└─────────────────────────────────┘\n\n\n\nパッチレベルの特徴を削減するために、Attention Pooling を使用します。パッチの平均をクエリとし、各パッチ窓を単一のベクトルに集約します。\n\n\nInput Patches (4×4 example):\n┌─────┬─────┬─────┬─────┐\n│ P₁  │ P₂  │ P₃  │ P₄  │\n├─────┼─────┼─────┼─────┤\n│ P₅  │ P₆  │ P₇  │ P₈  │\n├─────┼─────┼─────┼─────┤\n│ P₉  │ P₁₀ │ P₁₁ │ P₁₂ │\n├─────┼─────┼─────┼─────┤\n│ P₁₃ │ P₁₄ │ P₁₅ │ P₁₆ │\n└─────┴─────┴─────┴─────┘\n\nAfter 2×2 Attention Pooling:\n┌───────────┬───────────┐\n│ T₁        │ T₂        │\n│ (P₁~P₆)   │ (P₃~P₈)   │\n├───────────┼───────────┤\n│ T₃        │ T₄        │\n│ (P₉~P₁₄)  │ (P₁₁~P₁₆) │\n└───────────┴───────────┘\n\nToken count: 16 → 4 (1/4 reduction)\n\n\n\nビデオではフレーム数が多いため、3×3 の窓 を使用してトークン数をさらに削減します。\nInput Patches (9×9 example):\n┌───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│P₁ │P₂ │P₃ │P₄ │P₅ │P₆ │P₇ │P₈ │P₉ │\n├───┼───┼───┼───┼───┼───┼───┼───┼───┤\n│...│...│...│...│...│...│...│...│...│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┘\n        ↓ 3×3 Attention Pooling\n┌─────────┬─────────┬─────────┐\n│   T₁    │   T₂    │   T₃    │\n│ (9 P's) │ (9 P's) │ (9 P's) │\n├─────────┼─────────┼─────────┤\n│   ...   │   ...   │   ...   │\n└─────────┴─────────┴─────────┘\n\nToken count: 81 → 9 (1/9 reduction)\n\n\n\n\n最後に、プールされた特徴は Shared MLP によって投影されます。この MLP は画像とビデオフレームで パラメータを共有 しており、統一的な視覚表現を学習します。\n\n\n\n\n\n\nflowchart LR\n    A[ViT Layer N-9] --&gt; C[Concat]\n    B[ViT Layer N-3] --&gt; C\n    C --&gt; D{Pooling Window}\n    D --&gt;|Image| E[2×2 Attention]\n    D --&gt;|Video| F[3×3 Attention]\n    E --&gt; G[Shared MLP]\n    F --&gt; G\n    G --&gt; H[Visual Tokens]\n\n    style C fill:#e6f0ff\n    style G fill:#f0ffe6\n\n\n\n\nFigure 2: Vision-Language Connector のアーキテクチャ\n\n\n\n\n\n\n\n\n\n\n\nMolmo2 は、Multi-crop 戦略を採用しています。\n\n1つのダウンスケール済み全体クロップ + 最大 K 個のオーバーラップタイルクロップ\nトレーニング時: K = 8\n推論時: K = 24（高解像度処理）\n\nK 個のクロップでタイル化できない画像は、ダウンスケールされます。\nOriginal Image:\n┌─────────────────────────────────┐\n│                                 │\n│                                 │\n│        High-res Image           │\n│                                 │\n│                                 │\n└─────────────────────────────────┘\n         ↓\nDownscaled Crop + K Overlapping Crops:\n┌───────┐  ┌─────┬─────┬─────┐\n│ Full  │  │ C₁  │ C₂  │ C₃  │\n│ (DS)  │  ├─────┼─────┼─────┤\n└───────┘  │ C₄  │ C₅  │ C₆  │\n           ├─────┼─────┼─────┤\n           │ C₇  │ C₈  │ ... │\n           └─────┴─────┴─────┘\n\n\n\n\n\n\nNoteColumn Tokens\n\n\n\nMulti-crop 画像の場合、Column tokens を LLM への入力に含めます。これにより、画像のアスペクト比情報を LLM に伝達できます。\n単一クロップ画像（常に正方形）には Column tokens を含めません。\n\n\n\n\n\nビデオの場合、計算コストを削減するために以下の戦略を取ります。\n\nサンプリングレート: S = 2 fps（2秒ごとに1フレーム）\n各フレームは単一クロップとして処理（必要に応じてダウンスケール）\n最大フレーム数: F = 128（標準トレーニング）または F = 384（長尺コンテキストトレーニング）\n\nVideo Timeline:\n0s    1s    2s    3s    4s    5s    ...\n│─────│─────│─────│─────│─────│─────│\n      ↓     ↓     ↓     ↓     ↓\n    Frame₁ Frame₂ Frame₃ ... (@ 2 fps)\n\nIf video length &gt; F/S seconds:\n→ Uniformly sample F frames\n→ Always include LAST frame\n\n\n\n\n\n\nTip最終フレームの特別な扱い\n\n\n\nビデオの 最終フレーム は常に含まれます。これは、多くのビデオプレイヤーが再生終了後に最終フレームを表示するため、ユーザーにとって特別な重要性を持つ可能性があるためです。\n\n\n\n\n\n\nMolmo2 では、LLM が視覚トークンを処理する際に、画像トークン同士が相互に attend できる ように設計されています [@Miao2024LongVU; @Wu2024DoubleLLaVA]。\n通常の LLM では、因果的マスク（causal mask）により、各トークンは自分より前のトークンにしか注意を向けられません。しかし、Molmo2 では、視覚トークンに対して bi-directional attention を許可しています。\nStandard Causal Attention:\n  T₁  T₂  T₃  T₄\nT₁ ●   ×   ×   ×\nT₂ ●   ●   ×   ×\nT₃ ●   ●   ●   ×\nT₄ ●   ●   ●   ●\n\nBi-directional Attention on Vision Tokens:\n  V₁  V₂  V₃  T₁  T₂\nV₁ ●   ●   ●   ×   ×\nV₂ ●   ●   ●   ×   ×\nV₃ ●   ●   ●   ×   ×\nT₁ ●   ●   ●   ●   ×\nT₂ ●   ●   ●   ●   ●\n\nV: Vision tokens, T: Text tokens\nこれにより、異なるフレームや異なる画像からの視覚トークンが相互に情報を交換でき、時空間的な関係を学習できます。\n\n\n\n\n\n\nImportantBi-directional Attention の効果\n\n\n\nアブレーション研究により、視覚トークンへの Bi-directional attention が 性能を向上させる ことが確認されています。\n特に、ビデオトラッキングやマルチイメージ理解など、複数のフレーム/画像間の関係を捉える必要があるタスクで有効です。\n\n\n\n\n\nVision-Language Connector によって生成された視覚トークンは、以下の形式で LLM に入力されます。\n\n\n&lt;image_start&gt; [Visual Tokens for Frame₁] &lt;timestamp&gt;0.5s&lt;/timestamp&gt;\n&lt;image_start&gt; [Visual Tokens for Frame₂] &lt;timestamp&gt;1.0s&lt;/timestamp&gt;\n...\n[Subtitle text] &lt;timestamp&gt;0.5s-2.0s&lt;/timestamp&gt;\n\n各フレームの視覚トークンに タイムスタンプ を付加\n字幕が利用可能な場合は、タイムスタンプ付きテキストとして追加\n\n\n\n\n&lt;image_start&gt; [Visual Tokens for Image₁] &lt;image&gt;1&lt;/image&gt;\n&lt;image_start&gt; [Visual Tokens for Image₂] &lt;image&gt;2&lt;/image&gt;\n...\n\n各画像に 画像インデックス を付加\n\n\n\n\n&lt;image_start&gt; [Column Tokens] [Visual Tokens for Full Crop]\n[Visual Tokens for Crop₁] [Visual Tokens for Crop₂] ...\n\nColumn tokens でアスペクト比を伝達\n全体クロップ + 部分クロップのトークンを結合\n\n\n\n\n\nMolmo2 の Vision-Language Connector は、以下の特徴を持ちます。\n\nMulti-layer features: ViT の複数層（3rd-to-last, 9th-from-last）から特徴を抽出\nAdaptive pooling: 画像には 2×2、ビデオフレームには 3×3 の Attention Pooling\nShared parameters: 画像とビデオで統一的な MLP projection\nMulti-crop strategy: 高解像度処理のため、最大24個のクロップを使用\nEfficient video processing: 2 fps サンプリング + 最終フレームの保持\nBi-directional attention: 視覚トークン間の相互作用を許可（性能向上）\nColumn tokens: Multi-crop 画像のアスペクト比情報を伝達\n\nこの設計により、Molmo2 は画像とビデオを統一的に処理しながら、計算効率と表現力のバランスを実現しています。\n\n\n\n\nClark, C., et al. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models. arXiv:2409.17146.\nMiao, X., et al. (2024). LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. arXiv:2410.17434.\nWu, H., et al. (2024). DoubleLLaVA: Efficient Long Video Understanding with Grouped Frame Tokens. arXiv:2410.00907.",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#概要",
    "href": "ja/molmo2/04-vision-language-connector.html#概要",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector は、Vision Transformer (ViT) が抽出した視覚特徴を、Large Language Model (LLM) が処理できる形式に変換する重要なモジュールです。Molmo2 では、標準的な VLM アーキテクチャ [@Clark2024Molmo] に従い、画像とビデオの両方を統一的に処理できる設計を採用しています。\n\n\n\n\n\n\nflowchart TD\n    A[Visual Input&lt;br/&gt;Image or Video] --&gt; B[ViT Encoding&lt;br/&gt;Patch-level Features]\n    B --&gt; C[Multi-layer Feature&lt;br/&gt;Extraction]\n    C --&gt; D{Input Type}\n    D --&gt;|Image| E[2×2 Attention&lt;br/&gt;Pooling]\n    D --&gt;|Video Frame| F[3×3 Attention&lt;br/&gt;Pooling]\n    E --&gt; G[Shared MLP&lt;br/&gt;Projection]\n    F --&gt; G\n    G --&gt; H[Visual Tokens&lt;br/&gt;for LLM]\n\n    style C fill:#e6f0ff\n    style E fill:#ffe6f0\n    style F fill:#ffe6f0\n    style G fill:#f0ffe6\n\n\n\n\nFigure 1: Vision-Language Connector のデータフロー",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#アーキテクチャの詳細",
    "href": "ja/molmo2/04-vision-language-connector.html#アーキテクチャの詳細",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 の Vision-Language Connector は、ViT の単一層ではなく、複数の層から特徴を抽出 します。\n\nThird-to-last layer（最終層から3番目）: 高レベルのセマンティック特徴\nNinth-from-last layer（最終層から9番目）: 中レベルの特徴\n\nこの設計は、先行研究の Molmo [@Clark2024Molmo] に従っており、異なる抽象度の視覚情報を組み合わせることで、より豊かな表現を実現しています。\nViT Layer Structure:\n┌─────────────────────────────────┐\n│  Layer 0 (Input)                │\n│  Layer 1                        │\n│  ...                            │\n│  Layer N-9  ◄── 9th-from-last  │ ─┐\n│  ...                            │  │\n│  Layer N-3  ◄── 3rd-to-last    │ ─┤ Features used\n│  Layer N-2                      │  │ by Connector\n│  Layer N-1                      │  │\n│  Layer N (Output)               │ ─┘\n└─────────────────────────────────┘\n\n\n\nパッチレベルの特徴を削減するために、Attention Pooling を使用します。パッチの平均をクエリとし、各パッチ窓を単一のベクトルに集約します。\n\n\nInput Patches (4×4 example):\n┌─────┬─────┬─────┬─────┐\n│ P₁  │ P₂  │ P₃  │ P₄  │\n├─────┼─────┼─────┼─────┤\n│ P₅  │ P₆  │ P₇  │ P₈  │\n├─────┼─────┼─────┼─────┤\n│ P₉  │ P₁₀ │ P₁₁ │ P₁₂ │\n├─────┼─────┼─────┼─────┤\n│ P₁₃ │ P₁₄ │ P₁₅ │ P₁₆ │\n└─────┴─────┴─────┴─────┘\n\nAfter 2×2 Attention Pooling:\n┌───────────┬───────────┐\n│ T₁        │ T₂        │\n│ (P₁~P₆)   │ (P₃~P₈)   │\n├───────────┼───────────┤\n│ T₃        │ T₄        │\n│ (P₉~P₁₄)  │ (P₁₁~P₁₆) │\n└───────────┴───────────┘\n\nToken count: 16 → 4 (1/4 reduction)\n\n\n\nビデオではフレーム数が多いため、3×3 の窓 を使用してトークン数をさらに削減します。\nInput Patches (9×9 example):\n┌───┬───┬───┬───┬───┬───┬───┬───┬───┐\n│P₁ │P₂ │P₃ │P₄ │P₅ │P₆ │P₇ │P₈ │P₉ │\n├───┼───┼───┼───┼───┼───┼───┼───┼───┤\n│...│...│...│...│...│...│...│...│...│\n└───┴───┴───┴───┴───┴───┴───┴───┴───┘\n        ↓ 3×3 Attention Pooling\n┌─────────┬─────────┬─────────┐\n│   T₁    │   T₂    │   T₃    │\n│ (9 P's) │ (9 P's) │ (9 P's) │\n├─────────┼─────────┼─────────┤\n│   ...   │   ...   │   ...   │\n└─────────┴─────────┴─────────┘\n\nToken count: 81 → 9 (1/9 reduction)\n\n\n\n\n最後に、プールされた特徴は Shared MLP によって投影されます。この MLP は画像とビデオフレームで パラメータを共有 しており、統一的な視覚表現を学習します。\n\n\n\n\n\n\nflowchart LR\n    A[ViT Layer N-9] --&gt; C[Concat]\n    B[ViT Layer N-3] --&gt; C\n    C --&gt; D{Pooling Window}\n    D --&gt;|Image| E[2×2 Attention]\n    D --&gt;|Video| F[3×3 Attention]\n    E --&gt; G[Shared MLP]\n    F --&gt; G\n    G --&gt; H[Visual Tokens]\n\n    style C fill:#e6f0ff\n    style G fill:#f0ffe6\n\n\n\n\nFigure 2: Vision-Language Connector のアーキテクチャ",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#cropping-戦略",
    "href": "ja/molmo2/04-vision-language-connector.html#cropping-戦略",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 は、Multi-crop 戦略を採用しています。\n\n1つのダウンスケール済み全体クロップ + 最大 K 個のオーバーラップタイルクロップ\nトレーニング時: K = 8\n推論時: K = 24（高解像度処理）\n\nK 個のクロップでタイル化できない画像は、ダウンスケールされます。\nOriginal Image:\n┌─────────────────────────────────┐\n│                                 │\n│                                 │\n│        High-res Image           │\n│                                 │\n│                                 │\n└─────────────────────────────────┘\n         ↓\nDownscaled Crop + K Overlapping Crops:\n┌───────┐  ┌─────┬─────┬─────┐\n│ Full  │  │ C₁  │ C₂  │ C₃  │\n│ (DS)  │  ├─────┼─────┼─────┤\n└───────┘  │ C₄  │ C₅  │ C₆  │\n           ├─────┼─────┼─────┤\n           │ C₇  │ C₈  │ ... │\n           └─────┴─────┴─────┘\n\n\n\n\n\n\nNoteColumn Tokens\n\n\n\nMulti-crop 画像の場合、Column tokens を LLM への入力に含めます。これにより、画像のアスペクト比情報を LLM に伝達できます。\n単一クロップ画像（常に正方形）には Column tokens を含めません。\n\n\n\n\n\nビデオの場合、計算コストを削減するために以下の戦略を取ります。\n\nサンプリングレート: S = 2 fps（2秒ごとに1フレーム）\n各フレームは単一クロップとして処理（必要に応じてダウンスケール）\n最大フレーム数: F = 128（標準トレーニング）または F = 384（長尺コンテキストトレーニング）\n\nVideo Timeline:\n0s    1s    2s    3s    4s    5s    ...\n│─────│─────│─────│─────│─────│─────│\n      ↓     ↓     ↓     ↓     ↓\n    Frame₁ Frame₂ Frame₃ ... (@ 2 fps)\n\nIf video length &gt; F/S seconds:\n→ Uniformly sample F frames\n→ Always include LAST frame\n\n\n\n\n\n\nTip最終フレームの特別な扱い\n\n\n\nビデオの 最終フレーム は常に含まれます。これは、多くのビデオプレイヤーが再生終了後に最終フレームを表示するため、ユーザーにとって特別な重要性を持つ可能性があるためです。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#bi-directional-attention",
    "href": "ja/molmo2/04-vision-language-connector.html#bi-directional-attention",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 では、LLM が視覚トークンを処理する際に、画像トークン同士が相互に attend できる ように設計されています [@Miao2024LongVU; @Wu2024DoubleLLaVA]。\n通常の LLM では、因果的マスク（causal mask）により、各トークンは自分より前のトークンにしか注意を向けられません。しかし、Molmo2 では、視覚トークンに対して bi-directional attention を許可しています。\nStandard Causal Attention:\n  T₁  T₂  T₃  T₄\nT₁ ●   ×   ×   ×\nT₂ ●   ●   ×   ×\nT₃ ●   ●   ●   ×\nT₄ ●   ●   ●   ●\n\nBi-directional Attention on Vision Tokens:\n  V₁  V₂  V₃  T₁  T₂\nV₁ ●   ●   ●   ×   ×\nV₂ ●   ●   ●   ×   ×\nV₃ ●   ●   ●   ×   ×\nT₁ ●   ●   ●   ●   ×\nT₂ ●   ●   ●   ●   ●\n\nV: Vision tokens, T: Text tokens\nこれにより、異なるフレームや異なる画像からの視覚トークンが相互に情報を交換でき、時空間的な関係を学習できます。\n\n\n\n\n\n\nImportantBi-directional Attention の効果\n\n\n\nアブレーション研究により、視覚トークンへの Bi-directional attention が 性能を向上させる ことが確認されています。\n特に、ビデオトラッキングやマルチイメージ理解など、複数のフレーム/画像間の関係を捉える必要があるタスクで有効です。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#llm-への入力フォーマット",
    "href": "ja/molmo2/04-vision-language-connector.html#llm-への入力フォーマット",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Vision-Language Connector によって生成された視覚トークンは、以下の形式で LLM に入力されます。\n\n\n&lt;image_start&gt; [Visual Tokens for Frame₁] &lt;timestamp&gt;0.5s&lt;/timestamp&gt;\n&lt;image_start&gt; [Visual Tokens for Frame₂] &lt;timestamp&gt;1.0s&lt;/timestamp&gt;\n...\n[Subtitle text] &lt;timestamp&gt;0.5s-2.0s&lt;/timestamp&gt;\n\n各フレームの視覚トークンに タイムスタンプ を付加\n字幕が利用可能な場合は、タイムスタンプ付きテキストとして追加\n\n\n\n\n&lt;image_start&gt; [Visual Tokens for Image₁] &lt;image&gt;1&lt;/image&gt;\n&lt;image_start&gt; [Visual Tokens for Image₂] &lt;image&gt;2&lt;/image&gt;\n...\n\n各画像に 画像インデックス を付加\n\n\n\n\n&lt;image_start&gt; [Column Tokens] [Visual Tokens for Full Crop]\n[Visual Tokens for Crop₁] [Visual Tokens for Crop₂] ...\n\nColumn tokens でアスペクト比を伝達\n全体クロップ + 部分クロップのトークンを結合",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#まとめ",
    "href": "ja/molmo2/04-vision-language-connector.html#まとめ",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Molmo2 の Vision-Language Connector は、以下の特徴を持ちます。\n\nMulti-layer features: ViT の複数層（3rd-to-last, 9th-from-last）から特徴を抽出\nAdaptive pooling: 画像には 2×2、ビデオフレームには 3×3 の Attention Pooling\nShared parameters: 画像とビデオで統一的な MLP projection\nMulti-crop strategy: 高解像度処理のため、最大24個のクロップを使用\nEfficient video processing: 2 fps サンプリング + 最終フレームの保持\nBi-directional attention: 視覚トークン間の相互作用を許可（性能向上）\nColumn tokens: Multi-crop 画像のアスペクト比情報を伝達\n\nこの設計により、Molmo2 は画像とビデオを統一的に処理しながら、計算効率と表現力のバランスを実現しています。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/04-vision-language-connector.html#参考文献",
    "href": "ja/molmo2/04-vision-language-connector.html#参考文献",
    "title": "Vision-Language Connector",
    "section": "",
    "text": "Clark, C., et al. (2024). Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models. arXiv:2409.17146.\nMiao, X., et al. (2024). LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. arXiv:2410.17434.\nWu, H., et al. (2024). DoubleLLaVA: Efficient Long Video Understanding with Grouped Frame Tokens. arXiv:2410.00907.",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Vision-Language Connector"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html",
    "href": "ja/molmo2/06-token-weighting.html",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting（トークン重み付け）は、訓練データに含まれる様々な長さの出力を持つタスクにおいて、損失関数の寄与度を調整する手法である。Molmo2 の訓練では、単一トークンの選択肢問題から 4,000 トークン以上の長いビデオキャプションまで、多様な出力長を持つデータが混在している。\n\n\n\n\n\n訓練データに出力長の大きな偏りがある場合、以下の問題が発生する:\n\nトークン数の偏り: 長い出力を持つタスク（例: ビデオキャプション）は、サンプリング頻度が低くても、訓練時の損失トークンの大部分を占めてしまう\n短い出力タスクの劣化: 多肢選択問題や短い回答を要求するタスクの性能が低下する\nタスクバランスの崩壊: モデルが長い出力を生成するタスクに過度に最適化され、他のタスクでの性能が犠牲になる\n\n\n\n\n\n\n\nNote例: 極端なケース\n\n\n\n1 サンプルのビデオキャプション（4,000 トークン）と 100 サンプルの多肢選択問題（各 1 トークン）を同じバッチで訓練する場合、ビデオキャプションが損失全体の約 97.5% を占めることになる（4,000 / (4,000 + 100) ≈ 0.975）。\n\n\n\n\n\n\nMolmo2 では、タスクの種類と出力長に応じて、各サンプルの損失に対する重みを調整している。\n\n\n\n\n\n\n\n\n\n\nタスク種別\n重み\n理由\n\n\n\n\nビデオキャプション\n0.1\n非常に長く密な出力（4,000+ トークン）を生成するため\n\n\nポインティング\n0.2\n座標列挙により長く密な出力を生成するため\n\n\nその他のタスク\n\\(\\frac{4}{\\sqrt{n}}\\)\n出力長 \\(n\\) に応じた適応的な重み付け\n\n\n\n\n\n\nサンプル \\(i\\) の損失重み \\(w_i\\) は以下のように定義される:\n\\[\nw_i = \\begin{cases}\n0.1 & \\text{if task is video captioning} \\\\\n0.2 & \\text{if task is pointing} \\\\\n\\frac{4}{\\sqrt{n_i}} & \\text{otherwise}\n\\end{cases}\n\\]\nここで、\\(n_i\\) はサンプル \\(i\\) の回答トークン数である。\n\n\n\nその他のタスクに対する \\(\\frac{4}{\\sqrt{n}}\\) という重み付けは、以下の特性を持つ:\n\n短い出力: \\(n = 1\\) のとき \\(w = 4.0\\)、\\(n = 16\\) のとき \\(w = 1.0\\)\n中程度の出力: \\(n = 100\\) のとき \\(w = 0.4\\)\n長い出力: \\(n = 400\\) のとき \\(w = 0.2\\)\n\nこの平方根による減衰により、長い出力を持つサンプルの影響を抑えつつ、完全に無視することを避けている。\n\n\n\n\n\n\nTip重み付けの直感\n\n\n\n平方根による重み付け \\(\\frac{4}{\\sqrt{n}}\\) は、出力長が 4 倍になると重みが半分になる特性を持つ。これにより、長い出力と短い出力の間でバランスの取れた学習が可能になる。\n例えば:\n\n1 トークン出力: 重み 4.0\n4 トークン出力: 重み 2.0\n16 トークン出力: 重み 1.0\n64 トークン出力: 重み 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant長短出力のバランス改善\n\n\n\nToken Weighting の導入により、以下の効果が得られる:\n\n多様なタスクでの性能維持: 短い回答を要求するタスク（多肢選択問題など）と長い出力を生成するタスク（ビデオキャプションなど）の両方で良好な性能を達成\n訓練の安定性向上: 損失が特定のタスクに支配されることを防ぎ、より安定した訓練が可能\n効率的なデータ活用: 出力長が異なる多様なタスクを単一のモデルで効果的に学習可能\n\n\n\n\n\n\nToken Weighting は損失計算時に適用される:\n\\[\n\\mathcal{L} = \\frac{1}{B} \\sum_{i=1}^{B} w_i \\cdot \\mathcal{L}_i\n\\]\nここで:\n\n\\(B\\) はバッチサイズ\n\\(\\mathcal{L}_i\\) はサンプル \\(i\\) の未調整損失\n\\(w_i\\) はサンプル \\(i\\) の重み\n\nこの重み付けは、各サンプルの損失を個別に計算できるため、実装が容易であり、既存の訓練パイプラインに容易に統合できる。\n\n\n\n\n\n\nサンプリング比率の調整: タスクごとのサンプリング確率を調整する方法だが、長い出力を持つタスクは少数サンプルでも損失を支配してしまうため不十分\n損失の正規化: タスクごとに損失を正規化する方法だが、同一タスク内での出力長の違いに対応できない\n固定重み: 全てのサンプルに固定の重みを使用する方法だが、出力長の多様性に対応できない\n\n\n\n\n\n適応性: 出力長に応じて自動的に重みが調整される\n柔軟性: タスクの特性（ビデオキャプション、ポインティング）に応じた固定重みと、一般的なタスク向けの適応的重みを組み合わせ\nシンプルさ: 実装が容易で、ハイパーパラメータの調整が最小限",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#概要",
    "href": "ja/molmo2/06-token-weighting.html#概要",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting（トークン重み付け）は、訓練データに含まれる様々な長さの出力を持つタスクにおいて、損失関数の寄与度を調整する手法である。Molmo2 の訓練では、単一トークンの選択肢問題から 4,000 トークン以上の長いビデオキャプションまで、多様な出力長を持つデータが混在している。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#問題設定",
    "href": "ja/molmo2/06-token-weighting.html#問題設定",
    "title": "Token Weighting",
    "section": "",
    "text": "訓練データに出力長の大きな偏りがある場合、以下の問題が発生する:\n\nトークン数の偏り: 長い出力を持つタスク（例: ビデオキャプション）は、サンプリング頻度が低くても、訓練時の損失トークンの大部分を占めてしまう\n短い出力タスクの劣化: 多肢選択問題や短い回答を要求するタスクの性能が低下する\nタスクバランスの崩壊: モデルが長い出力を生成するタスクに過度に最適化され、他のタスクでの性能が犠牲になる\n\n\n\n\n\n\n\nNote例: 極端なケース\n\n\n\n1 サンプルのビデオキャプション（4,000 トークン）と 100 サンプルの多肢選択問題（各 1 トークン）を同じバッチで訓練する場合、ビデオキャプションが損失全体の約 97.5% を占めることになる（4,000 / (4,000 + 100) ≈ 0.975）。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#molmo2-の重み付け戦略",
    "href": "ja/molmo2/06-token-weighting.html#molmo2-の重み付け戦略",
    "title": "Token Weighting",
    "section": "",
    "text": "Molmo2 では、タスクの種類と出力長に応じて、各サンプルの損失に対する重みを調整している。\n\n\n\n\n\n\n\n\n\n\nタスク種別\n重み\n理由\n\n\n\n\nビデオキャプション\n0.1\n非常に長く密な出力（4,000+ トークン）を生成するため\n\n\nポインティング\n0.2\n座標列挙により長く密な出力を生成するため\n\n\nその他のタスク\n\\(\\frac{4}{\\sqrt{n}}\\)\n出力長 \\(n\\) に応じた適応的な重み付け\n\n\n\n\n\n\nサンプル \\(i\\) の損失重み \\(w_i\\) は以下のように定義される:\n\\[\nw_i = \\begin{cases}\n0.1 & \\text{if task is video captioning} \\\\\n0.2 & \\text{if task is pointing} \\\\\n\\frac{4}{\\sqrt{n_i}} & \\text{otherwise}\n\\end{cases}\n\\]\nここで、\\(n_i\\) はサンプル \\(i\\) の回答トークン数である。\n\n\n\nその他のタスクに対する \\(\\frac{4}{\\sqrt{n}}\\) という重み付けは、以下の特性を持つ:\n\n短い出力: \\(n = 1\\) のとき \\(w = 4.0\\)、\\(n = 16\\) のとき \\(w = 1.0\\)\n中程度の出力: \\(n = 100\\) のとき \\(w = 0.4\\)\n長い出力: \\(n = 400\\) のとき \\(w = 0.2\\)\n\nこの平方根による減衰により、長い出力を持つサンプルの影響を抑えつつ、完全に無視することを避けている。\n\n\n\n\n\n\nTip重み付けの直感\n\n\n\n平方根による重み付け \\(\\frac{4}{\\sqrt{n}}\\) は、出力長が 4 倍になると重みが半分になる特性を持つ。これにより、長い出力と短い出力の間でバランスの取れた学習が可能になる。\n例えば:\n\n1 トークン出力: 重み 4.0\n4 トークン出力: 重み 2.0\n16 トークン出力: 重み 1.0\n64 トークン出力: 重み 0.5",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#効果",
    "href": "ja/molmo2/06-token-weighting.html#効果",
    "title": "Token Weighting",
    "section": "",
    "text": "Important長短出力のバランス改善\n\n\n\nToken Weighting の導入により、以下の効果が得られる:\n\n多様なタスクでの性能維持: 短い回答を要求するタスク（多肢選択問題など）と長い出力を生成するタスク（ビデオキャプションなど）の両方で良好な性能を達成\n訓練の安定性向上: 損失が特定のタスクに支配されることを防ぎ、より安定した訓練が可能\n効率的なデータ活用: 出力長が異なる多様なタスクを単一のモデルで効果的に学習可能",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#実装上の考慮事項",
    "href": "ja/molmo2/06-token-weighting.html#実装上の考慮事項",
    "title": "Token Weighting",
    "section": "",
    "text": "Token Weighting は損失計算時に適用される:\n\\[\n\\mathcal{L} = \\frac{1}{B} \\sum_{i=1}^{B} w_i \\cdot \\mathcal{L}_i\n\\]\nここで:\n\n\\(B\\) はバッチサイズ\n\\(\\mathcal{L}_i\\) はサンプル \\(i\\) の未調整損失\n\\(w_i\\) はサンプル \\(i\\) の重み\n\nこの重み付けは、各サンプルの損失を個別に計算できるため、実装が容易であり、既存の訓練パイプラインに容易に統合できる。",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/06-token-weighting.html#関連手法との比較",
    "href": "ja/molmo2/06-token-weighting.html#関連手法との比較",
    "title": "Token Weighting",
    "section": "",
    "text": "サンプリング比率の調整: タスクごとのサンプリング確率を調整する方法だが、長い出力を持つタスクは少数サンプルでも損失を支配してしまうため不十分\n損失の正規化: タスクごとに損失を正規化する方法だが、同一タスク内での出力長の違いに対応できない\n固定重み: 全てのサンプルに固定の重みを使用する方法だが、出力長の多様性に対応できない\n\n\n\n\n\n適応性: 出力長に応じて自動的に重みが調整される\n柔軟性: タスクの特性（ビデオキャプション、ポインティング）に応じた固定重みと、一般的なタスク向けの適応的重みを組み合わせ\nシンプルさ: 実装が容易で、ハイパーパラメータの調整が最小限",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2",
      "Token Weighting"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html",
    "href": "ja/molmo2/index.html",
    "title": "Molmo2",
    "section": "",
    "text": "Molmo2 (Multimodal Open Language Model 2) は、Allen Institute for AI (AI2) とワシントン大学が開発した完全オープンな Vision-Language Model (VLM) ファミリーです。最大の特徴は、ビデオグラウンディング（video grounding） 機能を備え、動画内の「いつ、どこで」特定のイベントや物体が発生したかを正確に指し示すことができる点です。\n9つの新規データセット（完全にプロプライエタリモデルに依存せず構築）を使用し、オープンソースモデルの中で最高水準の性能を達成しています。特に、ビデオポインティングとトラッキングでは、Gemini 3 Pro などのプロプライエタリモデルを上回る性能を示しています。\n論文: arXiv:2601.10611\nコード: github.com/allenai/molmo2\nDemo: playground.allenai.org",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#主な貢献",
    "href": "ja/molmo2/index.html#主な貢献",
    "title": "Molmo2",
    "section": "主な貢献",
    "text": "主な貢献\n\n9つの新規データセット: プロプライエタリモデルからの蒸留を一切使用せず構築\nビデオグラウンディング: 時空間的なポインティングとトラッキングを実現\n超詳細なビデオキャプション: 平均924語/動画（既存データセットの約2-12倍）\n完全オープン: モデル、データ、コードを全て公開",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#モデルサイズ",
    "href": "ja/molmo2/index.html#モデルサイズ",
    "title": "Molmo2",
    "section": "モデルサイズ",
    "text": "モデルサイズ\n\nMolmo2-4B: Qwen3 LLM ベース\nMolmo2-8B: Qwen3 LLM ベース\nMolmo2-O-7B: OLMo LLM ベース（完全オープン）",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/molmo2/index.html#目次",
    "href": "ja/molmo2/index.html#目次",
    "title": "Molmo2",
    "section": "目次",
    "text": "目次\n\n全体像\nDense Video Captioning\nVideo Grounding: Pointing & Tracking\nMulti-Image Understanding\nVision-Language Connector\nLong-Context Training\nToken Weighting\nPacking & Message Trees",
    "crumbs": [
      "Naoto Iwase",
      "Molmo2"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html",
    "href": "ja/olmo-3/01-dolma3-dataset.html",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模なデータセットです。Dolma 3 Mix として知られる約 6 兆トークンの多様なデータで構成されており、Web ページ、学術 PDF、コードリポジトリ、数学データなど、複数のデータソースを含んでいます。\n\n\nDolma 3 の主な目的は、Olmo 3 Base モデルに幅広い知識と能力を与えることです。データセットは、最も計算集約的な事前学習ステージ（全体の計算量の 90% 以上を消費）で使用されるため、スケーラビリティと品質が重要視されています。\nデータ戦略の基本原則:\n\n規模の重要性: 事前学習に影響を与えるには、兆トークンスケールで十分な量のデータが必要\nタスクデータの扱い: 構造化されたタスクデータ（QA ペア、チャットインスタンスなど）は、後の中間訓練や長文脈拡張のステージで使用し、事前学習では使用しない\n\n\n\n\nDolma 3 Mix は、複数のデータソースから構成されています。以下の表は、各ソースのトークン数と文書数を示しています。\n\n\n\nTable 1: Dolma 3 Mix のデータソース構成\n\n\n\n\n\n\n\n\n\n\n\n\nデータソース\nタイプ\n9T プール\n6T Mix\n6T Mix 割合\n\n\n\n\nCommon Crawl\nWeb ページ\n8.14T トークン9.67B 文書\n4.51T トークン3.15B 文書\n76.1%\n\n\nolmOCR science PDFs\n学術文書\n972B トークン101M 文書\n805B トークン83.8M 文書\n13.6%\n\n\nStack-Edu (Rebalanced)\nGitHub コード\n137B トークン167M 文書\n409B トークン526M 文書\n6.89%\n\n\narXiv\nLaTeX 論文\n21.4B トークン3.95M 文書\n50.8B トークン9.10M 文書\n0.86%\n\n\nFineMath 3+\n数学 Web ページ\n34.1B トークン21.4M 文書\n152B トークン95.5M 文書\n2.56%\n\n\nWikipedia & Wikibooks\n百科事典\n3.69B トークン6.67M 文書\n2.51B トークン4.24M 文書\n0.04%\n\n\n合計\n\n9.31T トークン9.97B 文書\n5.93T トークン3.87B 文書\n100%\n\n\n\n\n\n\n\n\nCommon Crawl (Web ページ):\n\n最も大きな割合を占めるデータソース（76.1%）\n多様な Web ページから抽出されたテキストデータ\n2024 年 12 月 31 日までのデータを含む\n\nolmOCR science PDFs (学術文書):\n\n学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n238 百万件のユニークな PDF 文書から抽出\nolmOCR ツールを使用してテキスト抽出\n\nStack-Edu (GitHub コード):\n\nThe Stack v2 データセットから厳選した教育的プログラミングコンテンツ\nプログラミング言語別に分割され、最適なミックスが適用される\n\narXiv (LaTeX 論文):\n\nProof-Pile-2 データセットから取得\n元の LaTeX 記法を保持し、数学的内容と適切なフォーマットの両方を学習可能\n\nFineMath 3+ (数学 Web ページ):\n\n数学的教育コンテンツを含む Common Crawl 文書のサブセット\n数学記法を適切に保持するように再処理\n\nWikipedia & Wikibooks (百科事典):\n\n英語版と Simple 版の Wikipedia と Wikibooks\n百科事典的知識のベースソース\n\n\n\n\n\nDolma 3 は、3 つの主要な技術革新を導入しています。\n\n\nDolma 3 では、兆トークンスケールで高速かつスケーラブルなグローバル重複排除を実現するために、Duplodocus という新しいツールを開発しました。\n重複排除は 3 つのステージで実施されます:\nStage 1: 完全重複排除 (Exact Deduplication):\n\n文書テキストハッシュに基づくグローバル重複排除\nすべての完全コピーを削除\n67% のデータを重複として識別し、38.7B から 12.8B 文書に削減\n\nStage 2: 曖昧重複排除 (Fuzzy Deduplication):\n\nMinHash ベースの重複排除で、ほぼ同一の文書を識別・削除\nヘッダーやフッターのみが異なる文書（複数ドメイン間でコピーされた文書）を削除\n23% のデータを重複として識別し、9.8B 文書に削減\n\nStage 3: 部分文字列重複排除 (Substring Deduplication):\n\n新しいファジー suffix-array ベースの重複排除手順\n個別文書内の繰り返しコンテンツ（ボイラープレートテキストや HTML アーティファクト）を削除\n500 バイト以上の繰り返し部分文字列をマーク\n14% のテキストバイトを削除し、9.7B 文書（36.5T バイト）に削減\n\nこの 3 段階の手順により、Web コーパスは 38.7B から 9.7B 文書に削減されました（文書数で 75% 削減）。\n\n\n\nDolma 3 では、データミキシングの 2 つの新しい手法を導入しています。\nToken-constrained Mixing (トークン制約付きミキシング):\n\nSwarm ベースのアプローチを使用して、多数の小型プロキシモデルを訓練・評価\nこれらの結果を使用して最適なミックスを決定\n条件付きミキシング手順により、データソースが継続的に改善・更新される開発サイクルに対応\n\nToken-constrained Mixing の手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\nQuality-aware Upsampling (品質認識アップサンプリング):\n\n各トピック内の品質バリエーションを考慮\n高品質な文書を選択的に繰り返すことで、全体的な繰り返しを最小限に抑えながら、高品質データの繰り返しを集中させる\n\n\n\n\nolmOCR science PDFs は、学術 PDF を線形化プレーンテキストに変換した新しいデータソースです。従来の peS2o データセットを置き換える形で導入されました。\n特徴:\n\nAI2Bot として識別される「礼儀正しい」クローリング\nrobots.txt を遵守し、ペイウォールを回避しない\n学術サイトと論文リポジトリに焦点を当てたクローリング\nolmOCR（バージョン 0.1.49-0.1.53）を使用してテキスト抽出\n\nデータ規模:\n\n238 百万件のユニークな PDF 文書（2024 年 12 月までのカットオフ日）\nテキスト抽出後、160 百万件の PDF 文書\n重複排除後、156 百万件の文書\n\n\n\n\n\nDolma 3 Mix のデータキュレーションは、以下のフローで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│                  Data Curation Pipeline                      │\n├──────────────────────────────────────────────────────────────┤\n│  Common Crawl (Web pages)                                    │\n│    └─&gt; HTML text extraction                                  │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  Academic PDFs                                               │\n│    └─&gt; OCR text extraction                                   │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  GitHub repos (Stack-Edu)                                    │\n│    └─&gt; Language classification                               │\n├──────────────────────────────────────────────────────────────┤\n│  FineMath, arXiv, Wiki                                       │\n│    └─&gt; (Preprocessed)                                        │\n├──────────────────────────────────────────────────────────────┤\n│  Mixing                                                      │\n│    └─&gt; Quality upsampling                                    │\n│        └─&gt; Dolma 3 Mix (6T tokens)                           │\n└──────────────────────────────────────────────────────────────┘\nパイプラインの主要ステップ:\n\nテキスト抽出: HTML または PDF からテキストを抽出\nヒューリスティックフィルタリング: 低品質文書、スパム、アダルトコンテンツを削除\n重複排除: 完全重複、曖昧重複、部分文字列重複を削除\nトピック・品質分類: WebOrganizer ツールで 24 のトピックに分類し、品質スコアを付与\nミキシング: Token-constrained mixing でデータソースの最適な比率を決定\n品質アップサンプリング: 高品質文書を選択的に繰り返す\n\n\n\n\nDolma 3 データセットは、Olmo 3 の訓練プロセスの 3 つのステージで使用されます。\n\n\n使用データ: Dolma 3 Mix（6T トークン）\n目的: 多様な知識と能力を持つ基盤モデルを構築\nデータソース:\n\nCommon Crawl: 76.1%\nolmOCR science PDFs: 13.6%\nStack-Edu: 6.89%\narXiv: 0.86%\nFineMath 3+: 2.56%\nWikipedia & Wikibooks: 0.04%\n\n\n\n\n使用データ: Dolma 3 Dolmino Mix（100B トークン）\n目的: コード、数学、一般知識 QA などの重要な能力を強化\n特徴: Post-training の下準備として、指示データと思考トレースを意図的に含める\n詳細な Midtraining の説明は別の文書で扱われています。\n\n\n\n使用データ: Dolma 3 Longmino Mix（50-100B トークン）\n目的: 最大 65K トークンのコンテキストをサポートする長文脈能力を獲得\nデータ規模:\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n詳細な Long-context Extension の説明は別の文書で扱われています。\n\n\n\n\nToken-constrained mixing により、データソースの最適な比率が決定されました。\nWeb テキストのトピック分布:\n\nSTEM ドメイン（「Science, Math, and Technology」、「Software Development」）を大幅にアップウェイト\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\nStack-Edu のプログラミング言語分布:\n\nPython を Java や Markdown よりも優先\nほぼすべてのコーディングベンチマークで改善を達成\n\n\n\n\n\n\n\nNote実験用のサンプルミックス\n\n\n\n\n\nDolma 3 では、より少ない計算リソースで実験できるように、サンプルミックスも公開しています。\nPretraining サンプルミックス:\n\n150B トークン\nDolma 3 Mix と同じデータソース構成\n\n利点:\n\n小規模な実験が可能\nデータミキシングのアブレーション研究に有用\nコンピューティングリソースが限られた研究者も利用可能\n\n\n\n\n\n\n\nDolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模で高品質なデータセットです。グローバル重複排除、Token-constrained mixing、Quality-aware upsampling などの革新的な手法により、最適なデータミキシングを実現しています。\n主な特徴:\n\n多様なデータソース: Web、学術 PDF、コード、数学、百科事典など\n大規模: 6 兆トークン（Dolma 3 Mix）\n高品質: 重複排除とヒューリスティックフィルタリングによる品質管理\n最適化されたミキシング: Swarm ベースの手法で最適な比率を決定\n3 つのステージ: Pretraining、Midtraining、Long-context Extension で使用\n\nDolma 3 は完全にオープンで、研究者が再現性の高い研究を行えるようにすべてのデータソース、処理パイプライン、ミキシング比率を公開しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#概要と目的",
    "href": "ja/olmo-3/01-dolma3-dataset.html#概要と目的",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 の主な目的は、Olmo 3 Base モデルに幅広い知識と能力を与えることです。データセットは、最も計算集約的な事前学習ステージ（全体の計算量の 90% 以上を消費）で使用されるため、スケーラビリティと品質が重要視されています。\nデータ戦略の基本原則:\n\n規模の重要性: 事前学習に影響を与えるには、兆トークンスケールで十分な量のデータが必要\nタスクデータの扱い: 構造化されたタスクデータ（QA ペア、チャットインスタンスなど）は、後の中間訓練や長文脈拡張のステージで使用し、事前学習では使用しない",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#sec-dolma3-composition",
    "href": "ja/olmo-3/01-dolma3-dataset.html#sec-dolma3-composition",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 Mix は、複数のデータソースから構成されています。以下の表は、各ソースのトークン数と文書数を示しています。\n\n\n\nTable 1: Dolma 3 Mix のデータソース構成\n\n\n\n\n\n\n\n\n\n\n\n\nデータソース\nタイプ\n9T プール\n6T Mix\n6T Mix 割合\n\n\n\n\nCommon Crawl\nWeb ページ\n8.14T トークン9.67B 文書\n4.51T トークン3.15B 文書\n76.1%\n\n\nolmOCR science PDFs\n学術文書\n972B トークン101M 文書\n805B トークン83.8M 文書\n13.6%\n\n\nStack-Edu (Rebalanced)\nGitHub コード\n137B トークン167M 文書\n409B トークン526M 文書\n6.89%\n\n\narXiv\nLaTeX 論文\n21.4B トークン3.95M 文書\n50.8B トークン9.10M 文書\n0.86%\n\n\nFineMath 3+\n数学 Web ページ\n34.1B トークン21.4M 文書\n152B トークン95.5M 文書\n2.56%\n\n\nWikipedia & Wikibooks\n百科事典\n3.69B トークン6.67M 文書\n2.51B トークン4.24M 文書\n0.04%\n\n\n合計\n\n9.31T トークン9.97B 文書\n5.93T トークン3.87B 文書\n100%\n\n\n\n\n\n\n\n\nCommon Crawl (Web ページ):\n\n最も大きな割合を占めるデータソース（76.1%）\n多様な Web ページから抽出されたテキストデータ\n2024 年 12 月 31 日までのデータを含む\n\nolmOCR science PDFs (学術文書):\n\n学術 PDF を線形化プレーンテキストに変換した新しいデータソース\n238 百万件のユニークな PDF 文書から抽出\nolmOCR ツールを使用してテキスト抽出\n\nStack-Edu (GitHub コード):\n\nThe Stack v2 データセットから厳選した教育的プログラミングコンテンツ\nプログラミング言語別に分割され、最適なミックスが適用される\n\narXiv (LaTeX 論文):\n\nProof-Pile-2 データセットから取得\n元の LaTeX 記法を保持し、数学的内容と適切なフォーマットの両方を学習可能\n\nFineMath 3+ (数学 Web ページ):\n\n数学的教育コンテンツを含む Common Crawl 文書のサブセット\n数学記法を適切に保持するように再処理\n\nWikipedia & Wikibooks (百科事典):\n\n英語版と Simple 版の Wikipedia と Wikibooks\n百科事典的知識のベースソース",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#主要な革新点",
    "href": "ja/olmo-3/01-dolma3-dataset.html#主要な革新点",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、3 つの主要な技術革新を導入しています。\n\n\nDolma 3 では、兆トークンスケールで高速かつスケーラブルなグローバル重複排除を実現するために、Duplodocus という新しいツールを開発しました。\n重複排除は 3 つのステージで実施されます:\nStage 1: 完全重複排除 (Exact Deduplication):\n\n文書テキストハッシュに基づくグローバル重複排除\nすべての完全コピーを削除\n67% のデータを重複として識別し、38.7B から 12.8B 文書に削減\n\nStage 2: 曖昧重複排除 (Fuzzy Deduplication):\n\nMinHash ベースの重複排除で、ほぼ同一の文書を識別・削除\nヘッダーやフッターのみが異なる文書（複数ドメイン間でコピーされた文書）を削除\n23% のデータを重複として識別し、9.8B 文書に削減\n\nStage 3: 部分文字列重複排除 (Substring Deduplication):\n\n新しいファジー suffix-array ベースの重複排除手順\n個別文書内の繰り返しコンテンツ（ボイラープレートテキストや HTML アーティファクト）を削除\n500 バイト以上の繰り返し部分文字列をマーク\n14% のテキストバイトを削除し、9.7B 文書（36.5T バイト）に削減\n\nこの 3 段階の手順により、Web コーパスは 38.7B から 9.7B 文書に削減されました（文書数で 75% 削減）。\n\n\n\nDolma 3 では、データミキシングの 2 つの新しい手法を導入しています。\nToken-constrained Mixing (トークン制約付きミキシング):\n\nSwarm ベースのアプローチを使用して、多数の小型プロキシモデルを訓練・評価\nこれらの結果を使用して最適なミックスを決定\n条件付きミキシング手順により、データソースが継続的に改善・更新される開発サイクルに対応\n\nToken-constrained Mixing の手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\nQuality-aware Upsampling (品質認識アップサンプリング):\n\n各トピック内の品質バリエーションを考慮\n高品質な文書を選択的に繰り返すことで、全体的な繰り返しを最小限に抑えながら、高品質データの繰り返しを集中させる\n\n\n\n\nolmOCR science PDFs は、学術 PDF を線形化プレーンテキストに変換した新しいデータソースです。従来の peS2o データセットを置き換える形で導入されました。\n特徴:\n\nAI2Bot として識別される「礼儀正しい」クローリング\nrobots.txt を遵守し、ペイウォールを回避しない\n学術サイトと論文リポジトリに焦点を当てたクローリング\nolmOCR（バージョン 0.1.49-0.1.53）を使用してテキスト抽出\n\nデータ規模:\n\n238 百万件のユニークな PDF 文書（2024 年 12 月までのカットオフ日）\nテキスト抽出後、160 百万件の PDF 文書\n重複排除後、156 百万件の文書",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#データキュレーションのパイプライン",
    "href": "ja/olmo-3/01-dolma3-dataset.html#データキュレーションのパイプライン",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 Mix のデータキュレーションは、以下のフローで実施されます。\n┌──────────────────────────────────────────────────────────────┐\n│                  Data Curation Pipeline                      │\n├──────────────────────────────────────────────────────────────┤\n│  Common Crawl (Web pages)                                    │\n│    └─&gt; HTML text extraction                                  │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  Academic PDFs                                               │\n│    └─&gt; OCR text extraction                                   │\n│        └─&gt; Heuristic filtering                               │\n│            └─&gt; Deduplication                                 │\n│                └─&gt; Topic & quality classification            │\n├──────────────────────────────────────────────────────────────┤\n│  GitHub repos (Stack-Edu)                                    │\n│    └─&gt; Language classification                               │\n├──────────────────────────────────────────────────────────────┤\n│  FineMath, arXiv, Wiki                                       │\n│    └─&gt; (Preprocessed)                                        │\n├──────────────────────────────────────────────────────────────┤\n│  Mixing                                                      │\n│    └─&gt; Quality upsampling                                    │\n│        └─&gt; Dolma 3 Mix (6T tokens)                           │\n└──────────────────────────────────────────────────────────────┘\nパイプラインの主要ステップ:\n\nテキスト抽出: HTML または PDF からテキストを抽出\nヒューリスティックフィルタリング: 低品質文書、スパム、アダルトコンテンツを削除\n重複排除: 完全重複、曖昧重複、部分文字列重複を削除\nトピック・品質分類: WebOrganizer ツールで 24 のトピックに分類し、品質スコアを付与\nミキシング: Token-constrained mixing でデータソースの最適な比率を決定\n品質アップサンプリング: 高品質文書を選択的に繰り返す",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#つのステージでの使用方法",
    "href": "ja/olmo-3/01-dolma3-dataset.html#つのステージでの使用方法",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 データセットは、Olmo 3 の訓練プロセスの 3 つのステージで使用されます。\n\n\n使用データ: Dolma 3 Mix（6T トークン）\n目的: 多様な知識と能力を持つ基盤モデルを構築\nデータソース:\n\nCommon Crawl: 76.1%\nolmOCR science PDFs: 13.6%\nStack-Edu: 6.89%\narXiv: 0.86%\nFineMath 3+: 2.56%\nWikipedia & Wikibooks: 0.04%\n\n\n\n\n使用データ: Dolma 3 Dolmino Mix（100B トークン）\n目的: コード、数学、一般知識 QA などの重要な能力を強化\n特徴: Post-training の下準備として、指示データと思考トレースを意図的に含める\n詳細な Midtraining の説明は別の文書で扱われています。\n\n\n\n使用データ: Dolma 3 Longmino Mix（50-100B トークン）\n目的: 最大 65K トークンのコンテキストをサポートする長文脈能力を獲得\nデータ規模:\n\n7B モデル: 50B トークン\n32B モデル: 100B トークン\n\nデータソースの規模:\n\n8K トークン以上: 22.3M 文書（640B トークン）\n32K トークン以上: 4.5M 文書（380B トークン）\n\nこれは、長文脈研究のための最大のオープン利用可能なコレクションです。\n詳細な Long-context Extension の説明は別の文書で扱われています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#データミキシングの結果",
    "href": "ja/olmo-3/01-dolma3-dataset.html#データミキシングの結果",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Token-constrained mixing により、データソースの最適な比率が決定されました。\nWeb テキストのトピック分布:\n\nSTEM ドメイン（「Science, Math, and Technology」、「Software Development」）を大幅にアップウェイト\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\nStack-Edu のプログラミング言語分布:\n\nPython を Java や Markdown よりも優先\nほぼすべてのコーディングベンチマークで改善を達成\n\n\n\n\n\n\n\nNote実験用のサンプルミックス\n\n\n\n\n\nDolma 3 では、より少ない計算リソースで実験できるように、サンプルミックスも公開しています。\nPretraining サンプルミックス:\n\n150B トークン\nDolma 3 Mix と同じデータソース構成\n\n利点:\n\n小規模な実験が可能\nデータミキシングのアブレーション研究に有用\nコンピューティングリソースが限られた研究者も利用可能",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/01-dolma3-dataset.html#まとめ",
    "href": "ja/olmo-3/01-dolma3-dataset.html#まとめ",
    "title": "Dolma 3 データセット",
    "section": "",
    "text": "Dolma 3 は、Olmo 3 Base モデルの事前学習に使用される大規模で高品質なデータセットです。グローバル重複排除、Token-constrained mixing、Quality-aware upsampling などの革新的な手法により、最適なデータミキシングを実現しています。\n主な特徴:\n\n多様なデータソース: Web、学術 PDF、コード、数学、百科事典など\n大規模: 6 兆トークン（Dolma 3 Mix）\n高品質: 重複排除とヒューリスティックフィルタリングによる品質管理\n最適化されたミキシング: Swarm ベースの手法で最適な比率を決定\n3 つのステージ: Pretraining、Midtraining、Long-context Extension で使用\n\nDolma 3 は完全にオープンで、研究者が再現性の高い研究を行えるようにすべてのデータソース、処理パイプライン、ミキシング比率を公開しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Dolma 3 データセット"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html",
    "href": "ja/olmo-3/03-midtraining.html",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "OLMo 3 の訓練は、事前学習後に Midtraining（中間訓練）という追加ステージを経る。このフェーズでは 100B 個の高品質トークンを使用して、数学推論、コード生成、質問応答、指示追従、推論思考などの重要な能力を強化する。\n\n\nMidtraining は事前学習と後続の SFT（Supervised Fine-Tuning）の橋渡しを行う。データセットとして Dolma 3 Dolmino Mix を使用し、以下の特徴を持つ。\n\n100B トークンの高品質データ\nターゲット能力に特化したデータソースの選定\nDecontamination による評価データセットの汚染除去\nMicroanneal と Integration tests による効果的なデータミックス設計\n\n\n\n\nMidtraining のデータキュレーションは 2部構成のフレームワーク（Figure 11）で行われる。\n+-----------------------+     +-------------------------+\n| Distributed           |     | Centralized             |\n| Exploration           |     | Assessment              |\n+-----------------------+     +-------------------------+\n| - Individual data     | --&gt; | - Combine candidate     |\n|   source testing      |     |   datasets              |\n| - Lightweight         |     | - Full 100B integration |\n|   feedback loops      |     |   tests                 |\n| - Microanneal (10B)   |     | - Post-SFT evaluation   |\n+-----------------------+     +-------------------------+\n\n\n各データソースについて、軽量なフィードバックループで効果を評価する。\n\nMicroanneal: 5B トークンのターゲットデータ + 5B Web データ\nBaseline: Web のみの 10B トークン\n迅速な評価により、有望なデータソースを特定\n\n\n\n\n選定された候補データセットを組み合わせて統合テストを実施。\n\nIntegration tests: 100B トークンの完全な annealing run\nデータソース間の相互作用を評価\nSFT 訓練後の性能も測定\n\n\n\n\n\nTable 5 に示される Dolmino Mix は、以下のターゲット能力ごとにデータソースを構成している。\n\n\n\n\n\n\n\n\n\nCapability\nDataset\nToken Count\nDescription\n\n\n\n\nMath\nTinyMATH\n~5B\nMath problem-solution pairs\n\n\n\nCraneMath\n~3B\nMathematical reasoning\n\n\n\nMegaMatt\n~2B\nAdvanced mathematics\n\n\n\nDolmino Math\n~4B\nCurated math corpus\n\n\nCode\nStack-Edu (FIM)\n~10B\nEducational code with Fill-In-Middle\n\n\n\nCraneCode\n~5B\nHigh-quality code snippets\n\n\nQA\nReddit-to-Flashcards\n~3B\nQuestion-answer extraction\n\n\n\nWiki-to-RCQA\n~4B\nReading comprehension QA\n\n\n\nNemotron\n~2B\nSynthetic QA pairs\n\n\nInstruction\nTulu3 SFT\n~2B\nInstruction-following examples\n\n\n\nFlan\n~3B\nTask-oriented instructions\n\n\nThinking\nMeta-reasoning\n~2B\nChain-of-thought reasoning\n\n\n\nProgram-verifiable\n~1B\nVerifiable reasoning traces\n\n\n\nOMR rewrite\n~1B\nReasoning rewriting\n\n\nWeb\nDolma v1.7 Web\n~50B\nGeneral web content (baseline)\n\n\n\n\n\n\n\n\n\nNoteDolmino Mix の設計方針\n\n\n\n各能力ごとに複数のデータソースを組み合わせることで、単一データセットに依存せず、能力の汎化性能を向上させる。\n\n\n\n\n\n各ターゲット能力での改善結果（Section 3.5.2）。\n\n\n\nTinyMATH: 基本的な算術・代数問題\nCraneMath: 複雑な数式処理と証明\nMegaMatt: 大学レベルの数学問題\nDolmino Math: 上記を統合したキュレーションコーパス\n\n\n\n\n\nStack-Edu (FIM): Fill-In-Middle 形式での教育的コード\nCraneCode: 高品質なコードスニペット（複数言語）\n\n\n\n\n\n\n\nTipFill-In-Middle (FIM)\n\n\n\nコードの中間部分を予測するタスク。実際の IDE での補完シナリオに近い。\n\n\n\n\n\n\nReddit-to-Flashcards: Reddit の議論から QA ペアを抽出\nWiki-to-RCQA: Wikipedia 記事から読解問題を生成\nNemotron: 合成 QA データセット\n\n\n\n\n\nTulu3 SFT: 多様な指示追従タスク\nFlan: タスク指向の指示データ\n\n\n\n\n\nMeta-reasoning: Chain-of-Thought (CoT) スタイルの推論\nProgram-verifiable: プログラムで検証可能な推論トレース\nOMR rewrite: 推論プロセスのリライト\n\n\n\n\n\nSection 3.5.3 で詳述されている decontamination（汚染除去）プロセス。\n新しい decon パッケージ を開発し、評価データセットとの重複を除去。\n\nn-gram ベースのマッチング\n評価ベンチマークの汚染検出\n訓練データからの除外処理\n\n\n\n\n\n\n\nWarning評価データの汚染リスク\n\n\n\n高品質データセットには、評価ベンチマークと重複するサンプルが含まれる可能性がある。Decontamination により公平な評価を保証する。\n\n\n\n\n\nSection 3.5.4 の主要な発見。\n\nMicroanneal の有効性: 10B トークンの軽量テストで、100B の完全 run の結果を予測可能\nデータソースの相補性: 複数データソースの組み合わせが単一データセット以上の効果\nSFT との相性: Midtraining で強化された能力は、SFT 後もさらに向上\nDecontamination の必要性: 汚染除去により評価精度が大幅に改善\n\n\n\n\n\n03-pretraining.qmd: Stage 1 の事前学習\n04-sft.qmd: Stage 3 の SFT 訓練\n05-data-dolma3.qmd: Dolma 3 データセット全体",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#概要",
    "href": "ja/olmo-3/03-midtraining.html#概要",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Midtraining は事前学習と後続の SFT（Supervised Fine-Tuning）の橋渡しを行う。データセットとして Dolma 3 Dolmino Mix を使用し、以下の特徴を持つ。\n\n100B トークンの高品質データ\nターゲット能力に特化したデータソースの選定\nDecontamination による評価データセットの汚染除去\nMicroanneal と Integration tests による効果的なデータミックス設計",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#methodological-framework",
    "href": "ja/olmo-3/03-midtraining.html#methodological-framework",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Midtraining のデータキュレーションは 2部構成のフレームワーク（Figure 11）で行われる。\n+-----------------------+     +-------------------------+\n| Distributed           |     | Centralized             |\n| Exploration           |     | Assessment              |\n+-----------------------+     +-------------------------+\n| - Individual data     | --&gt; | - Combine candidate     |\n|   source testing      |     |   datasets              |\n| - Lightweight         |     | - Full 100B integration |\n|   feedback loops      |     |   tests                 |\n| - Microanneal (10B)   |     | - Post-SFT evaluation   |\n+-----------------------+     +-------------------------+\n\n\n各データソースについて、軽量なフィードバックループで効果を評価する。\n\nMicroanneal: 5B トークンのターゲットデータ + 5B Web データ\nBaseline: Web のみの 10B トークン\n迅速な評価により、有望なデータソースを特定\n\n\n\n\n選定された候補データセットを組み合わせて統合テストを実施。\n\nIntegration tests: 100B トークンの完全な annealing run\nデータソース間の相互作用を評価\nSFT 訓練後の性能も測定",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#midtraining-データの構成",
    "href": "ja/olmo-3/03-midtraining.html#midtraining-データの構成",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Table 5 に示される Dolmino Mix は、以下のターゲット能力ごとにデータソースを構成している。\n\n\n\n\n\n\n\n\n\nCapability\nDataset\nToken Count\nDescription\n\n\n\n\nMath\nTinyMATH\n~5B\nMath problem-solution pairs\n\n\n\nCraneMath\n~3B\nMathematical reasoning\n\n\n\nMegaMatt\n~2B\nAdvanced mathematics\n\n\n\nDolmino Math\n~4B\nCurated math corpus\n\n\nCode\nStack-Edu (FIM)\n~10B\nEducational code with Fill-In-Middle\n\n\n\nCraneCode\n~5B\nHigh-quality code snippets\n\n\nQA\nReddit-to-Flashcards\n~3B\nQuestion-answer extraction\n\n\n\nWiki-to-RCQA\n~4B\nReading comprehension QA\n\n\n\nNemotron\n~2B\nSynthetic QA pairs\n\n\nInstruction\nTulu3 SFT\n~2B\nInstruction-following examples\n\n\n\nFlan\n~3B\nTask-oriented instructions\n\n\nThinking\nMeta-reasoning\n~2B\nChain-of-thought reasoning\n\n\n\nProgram-verifiable\n~1B\nVerifiable reasoning traces\n\n\n\nOMR rewrite\n~1B\nReasoning rewriting\n\n\nWeb\nDolma v1.7 Web\n~50B\nGeneral web content (baseline)\n\n\n\n\n\n\n\n\n\nNoteDolmino Mix の設計方針\n\n\n\n各能力ごとに複数のデータソースを組み合わせることで、単一データセットに依存せず、能力の汎化性能を向上させる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#capability-improvements",
    "href": "ja/olmo-3/03-midtraining.html#capability-improvements",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "各ターゲット能力での改善結果（Section 3.5.2）。\n\n\n\nTinyMATH: 基本的な算術・代数問題\nCraneMath: 複雑な数式処理と証明\nMegaMatt: 大学レベルの数学問題\nDolmino Math: 上記を統合したキュレーションコーパス\n\n\n\n\n\nStack-Edu (FIM): Fill-In-Middle 形式での教育的コード\nCraneCode: 高品質なコードスニペット（複数言語）\n\n\n\n\n\n\n\nTipFill-In-Middle (FIM)\n\n\n\nコードの中間部分を予測するタスク。実際の IDE での補完シナリオに近い。\n\n\n\n\n\n\nReddit-to-Flashcards: Reddit の議論から QA ペアを抽出\nWiki-to-RCQA: Wikipedia 記事から読解問題を生成\nNemotron: 合成 QA データセット\n\n\n\n\n\nTulu3 SFT: 多様な指示追従タスク\nFlan: タスク指向の指示データ\n\n\n\n\n\nMeta-reasoning: Chain-of-Thought (CoT) スタイルの推論\nProgram-verifiable: プログラムで検証可能な推論トレース\nOMR rewrite: 推論プロセスのリライト",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#decontamination",
    "href": "ja/olmo-3/03-midtraining.html#decontamination",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Section 3.5.3 で詳述されている decontamination（汚染除去）プロセス。\n新しい decon パッケージ を開発し、評価データセットとの重複を除去。\n\nn-gram ベースのマッチング\n評価ベンチマークの汚染検出\n訓練データからの除外処理\n\n\n\n\n\n\n\nWarning評価データの汚染リスク\n\n\n\n高品質データセットには、評価ベンチマークと重複するサンプルが含まれる可能性がある。Decontamination により公平な評価を保証する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#key-findings",
    "href": "ja/olmo-3/03-midtraining.html#key-findings",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "Section 3.5.4 の主要な発見。\n\nMicroanneal の有効性: 10B トークンの軽量テストで、100B の完全 run の結果を予測可能\nデータソースの相補性: 複数データソースの組み合わせが単一データセット以上の効果\nSFT との相性: Midtraining で強化された能力は、SFT 後もさらに向上\nDecontamination の必要性: 汚染除去により評価精度が大幅に改善",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/03-midtraining.html#関連セクション",
    "href": "ja/olmo-3/03-midtraining.html#関連セクション",
    "title": "Midtraining: 中間訓練",
    "section": "",
    "text": "03-pretraining.qmd: Stage 1 の事前学習\n04-sft.qmd: Stage 3 の SFT 訓練\n05-data-dolma3.qmd: Dolma 3 データセット全体",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Midtraining: 中間訓練"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html",
    "href": "ja/olmo-3/05-deduplication.html",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Deduplication（重複排除）は、Dolma 3 データセットのキュレーションにおける中核的なプロセスである。兆トークンスケールのデータセットにおいて、重複したコンテンツを効率的に識別・削除することで、訓練の効率性と品質を向上させる。Dolma 3 では、3 段階の重複排除戦略を採用し、38.7B 文書を 9.7B 文書まで削減した（文書数で 75% 削減）。\n\n\n\n\n\n重複排除の主な目的は、トークン効率的な訓練を実現することである:\n\n計算コストの削減: 同一または類似のコンテンツを複数回訓練することは、計算リソースの無駄遣いである\nメモリ効率: 重複データを削除することで、より多様なデータをメモリに保持可能\n訓練時間の短縮: 冗長なデータを排除し、より効率的な訓練サイクルを実現\n\n\n\n\n重複の頻度は、コンテンツの品質を示す弱いシグナルとして機能する:\n\n高品質コンテンツ: 多くの場合、一度だけ出現する（オリジナルコンテンツ）\n低品質コンテンツ: スパム、ボイラープレートテキスト、テンプレート化されたコンテンツは複数のサイトで繰り返される傾向がある\nWeb スクレイピングの副産物: 同一コンテンツが複数のドメインにコピーされる現象（ミラーサイトなど）\n\n\n\n\n同一のコンテンツを複数回訓練することは、収穫逓減の法則に従う:\n\n1 回目: モデルが新しいパターンと知識を学習\n2 回目: 追加的な学習効果が減少\n3 回目以降: ほとんど追加的な利益がなく、過学習のリスクが増加\n\n\n\n\n\nDolma 3 では、異なる粒度での重複を対象とした 3 つの段階を組み合わせている。\n\n\n目的: 完全に同一の文書を識別・削除\n手法:\n\n文書全体のテキストハッシュ（SHA-256 など）を計算\nハッシュが完全に一致する文書を重複として識別\nグローバル重複排除: すべてのデータソース間で実施\n\n結果:\n\n削減率: 67% のデータを重複として識別\n文書数: 38.7B 文書から 12.8B 文書に削減\n対象: 完全コピー、ミラーサイト、クローラーの重複取得\n\n\n\n\n\n\n\nNoteExact Deduplication の効率性\n\n\n\n完全重複排除は計算コストが低く、ハッシュベースの実装により大規模データセットでも高速に動作する。この段階だけで文書数の 2/3 以上を削減できることは、Web データの重複度の高さを示している。\n\n\n\n\n\n目的: ほぼ同一の文書（ヘッダーやフッターのみが異なる文書）を識別・削除\n手法: MinHash ベースのアルゴリズム\nMinHash は、文書間の Jaccard 類似度を効率的に推定する手法である:\n\nShingling: 文書を n-gram（通常は 5-gram や 13-gram）に分割\nMinHash 署名: 各文書に対して固定長の署名を生成\nLSH (Locality-Sensitive Hashing): 類似した署名を持つ文書ペアを効率的に発見\nクラスタリング: 類似度が閾値を超える文書をクラスタ化し、各クラスタから 1 つのみを保持\n\n対象となる重複:\n\n異なるドメイン間でコピーされた文書: ニュース記事、ブログ投稿など\nテンプレートベースのコンテンツ: 同一のヘッダー/フッターを持つ文書\n軽微な編集が加えられたコンテンツ: 日付や名前のみが異なるバージョン\n\n結果:\n\n削減率: 23% のデータを重複として識別\n文書数: 12.8B 文書から 9.8B 文書に削減\n\n\n\n\n\n\n\nTipMinHash の効率性\n\n\n\nMinHash は、文書間の完全な比較（O(n^2) の計算量）を回避し、LSH により O(n) に近い計算量で類似文書を発見できる。これにより、数十億規模の文書に対しても実用的な時間で重複排除が可能になる。\n\n\n\n\n\n目的: 個別文書内の繰り返しコンテンツ（ボイラープレートテキスト、HTML アーティファクト）を削除\n手法: Fuzzy suffix-array ベースのアルゴリズム\nSuffix array は、文字列の全ての接尾辞を辞書順にソートしたデータ構造である:\n\nSuffix array 構築: 各文書に対して suffix array を構築\n繰り返し検出: 500 バイト以上の繰り返し部分文字列を識別\nマーキングと削除: 繰り返し部分をマークし、訓練データから除外\n\n対象となる重複:\n\nボイラープレートテキスト: ナビゲーションメニュー、フッター、サイドバー\nHTML アーティファクト: スクリプトタグ、スタイル定義の残骸\n繰り返しパターン: リスト項目、テーブルデータの反復\n\n結果:\n\n削減率: 14% のテキストバイトを削除\n文書数: 9.8B 文書から 9.7B 文書に削減（文書数自体はほぼ維持）\nデータサイズ: 最終的に 36.5T バイトに削減\n\n\n\n\n\n\n\nImportantStage 3 の重要性\n\n\n\nStage 3 は文書数をほとんど削減しないが、各文書の品質を大幅に向上させる。ボイラープレートテキストや HTML アーティファクトは、モデルが学習すべき有用な情報をほとんど含まないため、これらを削除することで訓練の効率性が向上する。\n\n\n\n\n\n\nDolma 3 の重複排除を実現するために、Duplodocus という新しいツールを開発した。\n\n\nNative Rust 実装:\n\n高性能: メモリ安全性を保ちつつ、C/C++ に匹敵する実行速度を実現\n並列処理: Rayon などの Rust のエコシステムを活用した効率的な並列化\nメモリ効率: 所有権システムにより、メモリリークやデータ競合を防止\n\n大規模分散実行:\n\nスケーラビリティ: 数十億規模の文書を処理可能\n分散ハッシュテーブル: 複数ノード間でハッシュテーブルを分散し、メモリ制約を緩和\nストリーミング処理: 全データをメモリに読み込むことなく、ストリーミング方式で処理\n\n統合された機能:\n\nHash-based exact deduplication: SHA-256 ハッシュによる完全重複排除\nMinHash fuzzy deduplication: Jaccard 類似度ベースの曖昧重複排除\nカスタマイズ可能: パラメータ（n-gram サイズ、類似度閾値など）を柔軟に調整可能\n\n\n\n\n┌───────────────────────────────────────────────────────────────┐\n│                   Duplodocus Workflow                         │\n├───────────────────────────────────────────────────────────────┤\n│  Input: 38.7B documents                                       │\n│    |                                                          │\n│    v                                                          │\n│  Stage 1: Hash-based exact deduplication                      │\n│    | - Compute SHA-256 hash for each document                 │\n│    | - Remove duplicates                                      │\n│    | - Output: 12.8B documents (67% reduction)                │\n│    |                                                          │\n│    v                                                          │\n│  Stage 2: MinHash fuzzy deduplication                         │\n│    | - Generate MinHash signatures                            │\n│    | - LSH for candidate pairs                                │\n│    | - Cluster similar documents                              │\n│    | - Output: 9.8B documents (23% reduction)                 │\n│    |                                                          │\n│    v                                                          │\n│  Stage 3: Suffix array substring deduplication                │\n│    | - Build suffix arrays                                    │\n│    | - Detect repeated substrings (&gt;=500 bytes)               │\n│    | - Mark and remove                                        │\n│    | - Output: 9.7B documents (14% byte reduction)            │\n│    |                                                          │\n│    v                                                          │\n│  Output: 9.7B documents, 36.5T bytes                          │\n└───────────────────────────────────────────────────────────────┘\n\n\n\n\n3 段階の重複排除プロセスにより、以下の結果を達成した。\n\n\n\n\n\n指標\n初期値\n最終値\n削減率\n\n\n\n\n文書数\n38.7B\n9.7B\n75%\n\n\nデータサイズ\n-\n36.5T バイト\n-\n\n\n\n段階別の削減率:\n\nStage 1 (Exact): 67% の文書を削除\nStage 2 (Fuzzy): 残りの 23% を削除\nStage 3 (Substring): 14% のバイトを削除\n\n\n\n\n重複排除されたデータは、Quality-aware Upsampling の基盤として機能する:\n\nクリーンなデータプール: 重複が排除された 9.7B 文書から高品質文書を選択\n品質スコアの信頼性向上: 重複データが品質分類に与えるノイズを削減\n効率的な繰り返し: 高品質文書のみを選択的に繰り返すことで、全体的な繰り返しを最小限に抑制\n\n重複排除により、Dolma 3 Mix は以下の最適化が可能になった:\n\nトピック分類の精度向上: 重複データのノイズを削減\n品質スコアの信頼性: より正確な品質評価が可能\nアップサンプリングの効率化: 高品質データの選択的な繰り返し\n\n詳細は ?@sec-quality-upsampling を参照。\n\n\n\n\n\n\n\n\n\n\nNote他のデータセットの重複排除手法\n\n\n\n\n\nC4 (Colossal Clean Crawled Corpus):\n\n主に exact deduplication に依存\nFuzzy deduplication は限定的\n結果: より多くの近似重複が残る可能性\n\nThe Pile:\n\nデータソースごとに異なる重複排除戦略\n一部のソースは重複排除なし\n結果: データソース間の一貫性が低い\n\nRedPajama:\n\nMinHash ベースの fuzzy deduplication を採用\nSubstring deduplication は実施せず\n結果: 文書内のボイラープレートが残る\n\nDolma 3 の優位性:\n\n3 段階の包括的アプローチ: Exact、Fuzzy、Substring の全てを実施\nグローバル重複排除: すべてのデータソース間で統一的に実施\nスケーラビリティ: Duplodocus により兆トークンスケールで高速実行\nオープン性: ツールとパイプラインを完全公開\n\n\n\n\n\n\n\nDolma 3 の重複排除は、以下の特徴を持つ:\n主な成果:\n\n大規模削減: 38.7B 文書から 9.7B 文書へ（75% 削減）\n3 段階の戦略: Exact、Fuzzy、Substring の包括的アプローチ\n高性能ツール: Duplodocus による効率的な大規模処理\n\n技術的革新:\n\nNative Rust 実装による高性能\n分散実行によるスケーラビリティ\nSuffix array ベースの substring deduplication\n\n下流への影響:\n\nQuality-aware upsampling の基盤を提供\n訓練効率の大幅な向上\nモデル品質の改善\n\n重複排除は、Dolma 3 データセットの品質と効率性を実現する上で不可欠なプロセスであり、Olmo 3 モデルの成功に大きく貢献している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#概要",
    "href": "ja/olmo-3/05-deduplication.html#概要",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Deduplication（重複排除）は、Dolma 3 データセットのキュレーションにおける中核的なプロセスである。兆トークンスケールのデータセットにおいて、重複したコンテンツを効率的に識別・削除することで、訓練の効率性と品質を向上させる。Dolma 3 では、3 段階の重複排除戦略を採用し、38.7B 文書を 9.7B 文書まで削減した（文書数で 75% 削減）。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#重複排除の目的と動機",
    "href": "ja/olmo-3/05-deduplication.html#重複排除の目的と動機",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "重複排除の主な目的は、トークン効率的な訓練を実現することである:\n\n計算コストの削減: 同一または類似のコンテンツを複数回訓練することは、計算リソースの無駄遣いである\nメモリ効率: 重複データを削除することで、より多様なデータをメモリに保持可能\n訓練時間の短縮: 冗長なデータを排除し、より効率的な訓練サイクルを実現\n\n\n\n\n重複の頻度は、コンテンツの品質を示す弱いシグナルとして機能する:\n\n高品質コンテンツ: 多くの場合、一度だけ出現する（オリジナルコンテンツ）\n低品質コンテンツ: スパム、ボイラープレートテキスト、テンプレート化されたコンテンツは複数のサイトで繰り返される傾向がある\nWeb スクレイピングの副産物: 同一コンテンツが複数のドメインにコピーされる現象（ミラーサイトなど）\n\n\n\n\n同一のコンテンツを複数回訓練することは、収穫逓減の法則に従う:\n\n1 回目: モデルが新しいパターンと知識を学習\n2 回目: 追加的な学習効果が減少\n3 回目以降: ほとんど追加的な利益がなく、過学習のリスクが増加",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#段階の重複排除戦略",
    "href": "ja/olmo-3/05-deduplication.html#段階の重複排除戦略",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 では、異なる粒度での重複を対象とした 3 つの段階を組み合わせている。\n\n\n目的: 完全に同一の文書を識別・削除\n手法:\n\n文書全体のテキストハッシュ（SHA-256 など）を計算\nハッシュが完全に一致する文書を重複として識別\nグローバル重複排除: すべてのデータソース間で実施\n\n結果:\n\n削減率: 67% のデータを重複として識別\n文書数: 38.7B 文書から 12.8B 文書に削減\n対象: 完全コピー、ミラーサイト、クローラーの重複取得\n\n\n\n\n\n\n\nNoteExact Deduplication の効率性\n\n\n\n完全重複排除は計算コストが低く、ハッシュベースの実装により大規模データセットでも高速に動作する。この段階だけで文書数の 2/3 以上を削減できることは、Web データの重複度の高さを示している。\n\n\n\n\n\n目的: ほぼ同一の文書（ヘッダーやフッターのみが異なる文書）を識別・削除\n手法: MinHash ベースのアルゴリズム\nMinHash は、文書間の Jaccard 類似度を効率的に推定する手法である:\n\nShingling: 文書を n-gram（通常は 5-gram や 13-gram）に分割\nMinHash 署名: 各文書に対して固定長の署名を生成\nLSH (Locality-Sensitive Hashing): 類似した署名を持つ文書ペアを効率的に発見\nクラスタリング: 類似度が閾値を超える文書をクラスタ化し、各クラスタから 1 つのみを保持\n\n対象となる重複:\n\n異なるドメイン間でコピーされた文書: ニュース記事、ブログ投稿など\nテンプレートベースのコンテンツ: 同一のヘッダー/フッターを持つ文書\n軽微な編集が加えられたコンテンツ: 日付や名前のみが異なるバージョン\n\n結果:\n\n削減率: 23% のデータを重複として識別\n文書数: 12.8B 文書から 9.8B 文書に削減\n\n\n\n\n\n\n\nTipMinHash の効率性\n\n\n\nMinHash は、文書間の完全な比較（O(n^2) の計算量）を回避し、LSH により O(n) に近い計算量で類似文書を発見できる。これにより、数十億規模の文書に対しても実用的な時間で重複排除が可能になる。\n\n\n\n\n\n目的: 個別文書内の繰り返しコンテンツ（ボイラープレートテキスト、HTML アーティファクト）を削除\n手法: Fuzzy suffix-array ベースのアルゴリズム\nSuffix array は、文字列の全ての接尾辞を辞書順にソートしたデータ構造である:\n\nSuffix array 構築: 各文書に対して suffix array を構築\n繰り返し検出: 500 バイト以上の繰り返し部分文字列を識別\nマーキングと削除: 繰り返し部分をマークし、訓練データから除外\n\n対象となる重複:\n\nボイラープレートテキスト: ナビゲーションメニュー、フッター、サイドバー\nHTML アーティファクト: スクリプトタグ、スタイル定義の残骸\n繰り返しパターン: リスト項目、テーブルデータの反復\n\n結果:\n\n削減率: 14% のテキストバイトを削除\n文書数: 9.8B 文書から 9.7B 文書に削減（文書数自体はほぼ維持）\nデータサイズ: 最終的に 36.5T バイトに削減\n\n\n\n\n\n\n\nImportantStage 3 の重要性\n\n\n\nStage 3 は文書数をほとんど削減しないが、各文書の品質を大幅に向上させる。ボイラープレートテキストや HTML アーティファクトは、モデルが学習すべき有用な情報をほとんど含まないため、これらを削除することで訓練の効率性が向上する。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#duplodocus-ツール",
    "href": "ja/olmo-3/05-deduplication.html#duplodocus-ツール",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 の重複排除を実現するために、Duplodocus という新しいツールを開発した。\n\n\nNative Rust 実装:\n\n高性能: メモリ安全性を保ちつつ、C/C++ に匹敵する実行速度を実現\n並列処理: Rayon などの Rust のエコシステムを活用した効率的な並列化\nメモリ効率: 所有権システムにより、メモリリークやデータ競合を防止\n\n大規模分散実行:\n\nスケーラビリティ: 数十億規模の文書を処理可能\n分散ハッシュテーブル: 複数ノード間でハッシュテーブルを分散し、メモリ制約を緩和\nストリーミング処理: 全データをメモリに読み込むことなく、ストリーミング方式で処理\n\n統合された機能:\n\nHash-based exact deduplication: SHA-256 ハッシュによる完全重複排除\nMinHash fuzzy deduplication: Jaccard 類似度ベースの曖昧重複排除\nカスタマイズ可能: パラメータ（n-gram サイズ、類似度閾値など）を柔軟に調整可能\n\n\n\n\n┌───────────────────────────────────────────────────────────────┐\n│                   Duplodocus Workflow                         │\n├───────────────────────────────────────────────────────────────┤\n│  Input: 38.7B documents                                       │\n│    |                                                          │\n│    v                                                          │\n│  Stage 1: Hash-based exact deduplication                      │\n│    | - Compute SHA-256 hash for each document                 │\n│    | - Remove duplicates                                      │\n│    | - Output: 12.8B documents (67% reduction)                │\n│    |                                                          │\n│    v                                                          │\n│  Stage 2: MinHash fuzzy deduplication                         │\n│    | - Generate MinHash signatures                            │\n│    | - LSH for candidate pairs                                │\n│    | - Cluster similar documents                              │\n│    | - Output: 9.8B documents (23% reduction)                 │\n│    |                                                          │\n│    v                                                          │\n│  Stage 3: Suffix array substring deduplication                │\n│    | - Build suffix arrays                                    │\n│    | - Detect repeated substrings (&gt;=500 bytes)               │\n│    | - Mark and remove                                        │\n│    | - Output: 9.7B documents (14% byte reduction)            │\n│    |                                                          │\n│    v                                                          │\n│  Output: 9.7B documents, 36.5T bytes                          │\n└───────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#最終結果",
    "href": "ja/olmo-3/05-deduplication.html#最終結果",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "3 段階の重複排除プロセスにより、以下の結果を達成した。\n\n\n\n\n\n指標\n初期値\n最終値\n削減率\n\n\n\n\n文書数\n38.7B\n9.7B\n75%\n\n\nデータサイズ\n-\n36.5T バイト\n-\n\n\n\n段階別の削減率:\n\nStage 1 (Exact): 67% の文書を削除\nStage 2 (Fuzzy): 残りの 23% を削除\nStage 3 (Substring): 14% のバイトを削除\n\n\n\n\n重複排除されたデータは、Quality-aware Upsampling の基盤として機能する:\n\nクリーンなデータプール: 重複が排除された 9.7B 文書から高品質文書を選択\n品質スコアの信頼性向上: 重複データが品質分類に与えるノイズを削減\n効率的な繰り返し: 高品質文書のみを選択的に繰り返すことで、全体的な繰り返しを最小限に抑制\n\n重複排除により、Dolma 3 Mix は以下の最適化が可能になった:\n\nトピック分類の精度向上: 重複データのノイズを削減\n品質スコアの信頼性: より正確な品質評価が可能\nアップサンプリングの効率化: 高品質データの選択的な繰り返し\n\n詳細は ?@sec-quality-upsampling を参照。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#他の重複排除との比較",
    "href": "ja/olmo-3/05-deduplication.html#他の重複排除との比較",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Note他のデータセットの重複排除手法\n\n\n\n\n\nC4 (Colossal Clean Crawled Corpus):\n\n主に exact deduplication に依存\nFuzzy deduplication は限定的\n結果: より多くの近似重複が残る可能性\n\nThe Pile:\n\nデータソースごとに異なる重複排除戦略\n一部のソースは重複排除なし\n結果: データソース間の一貫性が低い\n\nRedPajama:\n\nMinHash ベースの fuzzy deduplication を採用\nSubstring deduplication は実施せず\n結果: 文書内のボイラープレートが残る\n\nDolma 3 の優位性:\n\n3 段階の包括的アプローチ: Exact、Fuzzy、Substring の全てを実施\nグローバル重複排除: すべてのデータソース間で統一的に実施\nスケーラビリティ: Duplodocus により兆トークンスケールで高速実行\nオープン性: ツールとパイプラインを完全公開",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/05-deduplication.html#まとめ",
    "href": "ja/olmo-3/05-deduplication.html#まとめ",
    "title": "Deduplication: 重複排除",
    "section": "",
    "text": "Dolma 3 の重複排除は、以下の特徴を持つ:\n主な成果:\n\n大規模削減: 38.7B 文書から 9.7B 文書へ（75% 削減）\n3 段階の戦略: Exact、Fuzzy、Substring の包括的アプローチ\n高性能ツール: Duplodocus による効率的な大規模処理\n\n技術的革新:\n\nNative Rust 実装による高性能\n分散実行によるスケーラビリティ\nSuffix array ベースの substring deduplication\n\n下流への影響:\n\nQuality-aware upsampling の基盤を提供\n訓練効率の大幅な向上\nモデル品質の改善\n\n重複排除は、Dolma 3 データセットの品質と効率性を実現する上で不可欠なプロセスであり、Olmo 3 モデルの成功に大きく貢献している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Deduplication: 重複排除"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html",
    "href": "ja/olmo-3/07-data-mixing.html",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing（データミキシング）は、複数のデータソースを最適な比率で組み合わせ、モデルの性能を最大化する手法である。Dolma 3 では、9T トークンのデータプールから 6T トークンの訓練ミックスを構成するために、Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法を導入した。これらの手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n\n\n\n\n\nモデル訓練には、トークン予算という制約が存在する:\n\n計算コスト: 訓練に使用できるトークン数は、計算リソースによって制限される\n最適配分の必要性: 限られた予算内で、どのデータソースをどの程度含めるかを決定する必要がある\n多様性と品質のバランス: データの多様性を保ちながら、高品質なデータを優先する\n\n\n\n\n9T トークンのデータプールから 6T トークンの訓練ミックスを構成する際、以下の要素を考慮する:\n\nデータソースの特性: Web テキスト、学術 PDF、コード、数学など、各ソースの特徴\nトピックのバランス: STEM、ソフトウェア開発、一般知識など、トピックの最適な配分\n品質の考慮: 高品質な文書を優先的に選択\n\n\n\n\n\nToken-constrained Mixing は、トークン予算の制約下で最適なデータミックスを決定する手法である。\n\n\n小規模プロキシモデルを多数訓練し、その結果から最適なミックスを推定する:\n手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\n利点:\n\n計算効率: 小規模モデルでの実験により、大規模モデルの訓練前に最適なミックスを推定可能\n並列化: 複数のプロキシモデルを並列に訓練できる\n反復的改善: 結果に基づいて段階的にミックスを改善可能\n\n\n\n\n\n\n\nNoteSwarm の規模\n\n\n\nDolma 3 では、1B パラメータモデルを多数訓練し、異なるミキシング比率での性能を評価した。これらのプロキシモデルは、5x Chinchilla（通常の 5 倍のトークン数）で訓練され、データミックスの効果を正確に測定している。\n\n\n\n\n\nデータソースの継続的な改善に対応するため、条件付きミキシング手順を採用する:\n特徴:\n\n柔軟性: データソースが更新されても、ミックス全体を再計算する必要がない\nモジュール性: 個別のデータソースを独立して改善可能\nスケーラビリティ: 新しいデータソースの追加が容易\n\n開発サイクルへの対応:\n\nデータソースの継続的な改善\n新しいデータソースの段階的な導入\nミックス比率の動的な調整\n\n\n\n\n\nQuality-aware Upsampling は、重複排除後のクリーンなデータセットに対して、高品質文書を選択的に再導入する手法である。\n\n\n重複排除により削除されたデータの中から、高品質な文書を選択的に復元する:\nアプローチ:\n\n重複排除の基盤: まず、すべての重複を削除したクリーンなデータセットを構築\n品質評価: 各文書の品質スコアを計算\n選択的アップサンプリング: 高品質な文書を選択的に繰り返す\n\n効果:\n\n品質の向上: 高品質データの割合を増やすことで、モデルの性能を向上\n効率的な繰り返し: 全体的な繰り返しを最小限に抑えながら、高品質データに繰り返しを集中\nトークン効率: 限られたトークン予算を高品質データに優先配分\n\n\n\n\n\n\n\nTipQuality-aware Upsampling の戦略\n\n\n\n重複排除により削除された文書の中には、高品質なものも含まれる。これらを選択的に復元することで、重複排除による品質低下を防ぎつつ、データセット全体の品質を向上させる。\n\n\n\n\n\n\nDolma 3 では、Web テキストをトピックと品質の両方の軸で分類し、きめ細かなミキシングを実現している。\n\n\nWebOrganizer は、Web テキストを 24 の主要なトピックに分類するツールである:\n主要トピック（例）:\n\nScience, Math, and Technology\nSoftware Development\nArts and Entertainment\nBusiness and Finance\nHealth and Medicine\nEducation\nNews and Current Events\nその他 17 トピック\n\n分類の利点:\n\nトピックごとの重み付け: 各トピックに最適な重みを割り当てる\nSTEM の強化: Science, Math, Technology などのトピックを優先的に配分\nバランスの取れたミックス: 特定のトピックに偏らないように調整\n\n\n\n\n各トピック内で、品質スコアによりさらに分類する:\n品質分類:\n\n20 の品質階層: 各トピックを 20 の品質ティアに分割\nfastText ベースの分類器: 高速かつ正確な品質推定\n客観的な品質指標: 一貫性のある品質評価を実現\n\n\n\n\n24 トピック × 20 品質ティア = 480 個のサブセットに分割:\nきめ細かなミキシング:\n\nサブセットごとの重み: 各サブセットに個別の重みを割り当て\n品質とトピックの両立: 高品質かつ重要なトピックを優先\n柔軟な調整: 細かい粒度でのデータ配分の最適化\n\n┌──────────────────────────────────────────────────────────────┐\n│              Topic and Quality Classification                │\n├──────────────────────────────────────────────────────────────┤\n│  WebOrganizer (24 topics)                                    │\n│    ├─&gt; Science, Math, and Technology                         │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Software Development                                  │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Arts and Entertainment                                │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    └─&gt; ... (21 more topics)                                  │\n│          └─&gt; Quality tiers (1-20)                            │\n├──────────────────────────────────────────────────────────────┤\n│  Total: 480 subsets (24 x 20)                                │\n└──────────────────────────────────────────────────────────────┘\n\n\n\n\nToken-constrained Mixing と Quality-aware Upsampling により、データソースの最適な比率が決定された。\n\n\nWeb テキストのトピック分布において、以下の傾向が観察される:\n優先されたトピック:\n\nScience, Math, and Technology: STEM ドメインを大幅にアップウェイト\nSoftware Development: プログラミングとソフトウェア開発を強化\nEducation: 教育的コンテンツの重視\n\n抑制されたトピック:\n\nエンターテインメント関連のトピック\n一般的なニュースやソーシャルメディアコンテンツ\n\n結果:\n\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\n\n\n\nDCLM（DataComp for Language Models）Baseline と比較して、以下の改善が確認された:\n改善点:\n\nSTEM タスク: 科学、数学、技術関連のタスクで大幅な性能向上\nコーディングタスク: プログラミング能力の向上\n一般知識: 幅広い知識タスクでの性能改善\n\nトレードオフ:\n\n一部のタスクでは若干の性能低下\n全体として、重要なタスクでの性能向上が優先される\n\n\n\n\n\n\n\nImportantデータミキシングの影響\n\n\n\nデータミキシングの最適化は、モデルの性能に大きな影響を与える。STEM ドメインを優先することで、科学的・技術的タスクでの性能が向上し、Olmo 3 の強みとなっている。\n\n\n\n\n\nコードデータにおいても、プログラミング言語別の最適なミックスが決定された:\n優先された言語:\n\nPython: 最も高い重み付け（機械学習、データサイエンスでの重要性）\nJavaScript/TypeScript: Web 開発の主要言語\nC++/Rust: システムプログラミング言語\n\n抑制された言語:\n\nJava: 相対的に低い重み（冗長性の高いコードが多い）\nMarkdown: ドキュメントファイルの制限\n\n結果:\n\nほぼすべてのコーディングベンチマークで改善を達成\nPython 中心のタスクで特に顕著な改善\n\n\n\n\n\nData Mixing は、Dolma 3 の品質を決定する重要なプロセスである。Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n主な特徴:\n\nToken-constrained Mixing: Swarm-based methods による最適化\nQuality-aware Upsampling: 高品質データの選択的再導入\n480 個のサブセット: トピックと品質による細かい分類\n条件付きミキシング: データソースの継続的改善に対応\n実証された改善: DCLM Baseline と比較して平均 0.056 BPB の改善\n\nこれらの手法により、Dolma 3 は Olmo 3 Base モデルの高い性能を支える基盤となっている。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#概要",
    "href": "ja/olmo-3/07-data-mixing.html#概要",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing（データミキシング）は、複数のデータソースを最適な比率で組み合わせ、モデルの性能を最大化する手法である。Dolma 3 では、9T トークンのデータプールから 6T トークンの訓練ミックスを構成するために、Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法を導入した。これらの手法により、限られたトークン予算の下で最適なデータ配分を実現している。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#データミキシングの目的",
    "href": "ja/olmo-3/07-data-mixing.html#データミキシングの目的",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "モデル訓練には、トークン予算という制約が存在する:\n\n計算コスト: 訓練に使用できるトークン数は、計算リソースによって制限される\n最適配分の必要性: 限られた予算内で、どのデータソースをどの程度含めるかを決定する必要がある\n多様性と品質のバランス: データの多様性を保ちながら、高品質なデータを優先する\n\n\n\n\n9T トークンのデータプールから 6T トークンの訓練ミックスを構成する際、以下の要素を考慮する:\n\nデータソースの特性: Web テキスト、学術 PDF、コード、数学など、各ソースの特徴\nトピックのバランス: STEM、ソフトウェア開発、一般知識など、トピックの最適な配分\n品質の考慮: 高品質な文書を優先的に選択",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#token-constrained-mixing",
    "href": "ja/olmo-3/07-data-mixing.html#token-constrained-mixing",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Token-constrained Mixing は、トークン予算の制約下で最適なデータミックスを決定する手法である。\n\n\n小規模プロキシモデルを多数訓練し、その結果から最適なミックスを推定する:\n手順:\n\nSwarm 構築: 異なるミキシング比率で多数の小型プロキシモデルを訓練\nタスクごとの回帰: 各プロキシモデルがミキシング重みをタスク性能にマッピング\nミックス最適化: 平均タスク BPB（bits-per-byte）を最小化するミキシングを発見\n\n利点:\n\n計算効率: 小規模モデルでの実験により、大規模モデルの訓練前に最適なミックスを推定可能\n並列化: 複数のプロキシモデルを並列に訓練できる\n反復的改善: 結果に基づいて段階的にミックスを改善可能\n\n\n\n\n\n\n\nNoteSwarm の規模\n\n\n\nDolma 3 では、1B パラメータモデルを多数訓練し、異なるミキシング比率での性能を評価した。これらのプロキシモデルは、5x Chinchilla（通常の 5 倍のトークン数）で訓練され、データミックスの効果を正確に測定している。\n\n\n\n\n\nデータソースの継続的な改善に対応するため、条件付きミキシング手順を採用する:\n特徴:\n\n柔軟性: データソースが更新されても、ミックス全体を再計算する必要がない\nモジュール性: 個別のデータソースを独立して改善可能\nスケーラビリティ: 新しいデータソースの追加が容易\n\n開発サイクルへの対応:\n\nデータソースの継続的な改善\n新しいデータソースの段階的な導入\nミックス比率の動的な調整",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#quality-aware-upsampling",
    "href": "ja/olmo-3/07-data-mixing.html#quality-aware-upsampling",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Quality-aware Upsampling は、重複排除後のクリーンなデータセットに対して、高品質文書を選択的に再導入する手法である。\n\n\n重複排除により削除されたデータの中から、高品質な文書を選択的に復元する:\nアプローチ:\n\n重複排除の基盤: まず、すべての重複を削除したクリーンなデータセットを構築\n品質評価: 各文書の品質スコアを計算\n選択的アップサンプリング: 高品質な文書を選択的に繰り返す\n\n効果:\n\n品質の向上: 高品質データの割合を増やすことで、モデルの性能を向上\n効率的な繰り返し: 全体的な繰り返しを最小限に抑えながら、高品質データに繰り返しを集中\nトークン効率: 限られたトークン予算を高品質データに優先配分\n\n\n\n\n\n\n\nTipQuality-aware Upsampling の戦略\n\n\n\n重複排除により削除された文書の中には、高品質なものも含まれる。これらを選択的に復元することで、重複排除による品質低下を防ぎつつ、データセット全体の品質を向上させる。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#topic-と-quality-による分類",
    "href": "ja/olmo-3/07-data-mixing.html#topic-と-quality-による分類",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Dolma 3 では、Web テキストをトピックと品質の両方の軸で分類し、きめ細かなミキシングを実現している。\n\n\nWebOrganizer は、Web テキストを 24 の主要なトピックに分類するツールである:\n主要トピック（例）:\n\nScience, Math, and Technology\nSoftware Development\nArts and Entertainment\nBusiness and Finance\nHealth and Medicine\nEducation\nNews and Current Events\nその他 17 トピック\n\n分類の利点:\n\nトピックごとの重み付け: 各トピックに最適な重みを割り当てる\nSTEM の強化: Science, Math, Technology などのトピックを優先的に配分\nバランスの取れたミックス: 特定のトピックに偏らないように調整\n\n\n\n\n各トピック内で、品質スコアによりさらに分類する:\n品質分類:\n\n20 の品質階層: 各トピックを 20 の品質ティアに分割\nfastText ベースの分類器: 高速かつ正確な品質推定\n客観的な品質指標: 一貫性のある品質評価を実現\n\n\n\n\n24 トピック × 20 品質ティア = 480 個のサブセットに分割:\nきめ細かなミキシング:\n\nサブセットごとの重み: 各サブセットに個別の重みを割り当て\n品質とトピックの両立: 高品質かつ重要なトピックを優先\n柔軟な調整: 細かい粒度でのデータ配分の最適化\n\n┌──────────────────────────────────────────────────────────────┐\n│              Topic and Quality Classification                │\n├──────────────────────────────────────────────────────────────┤\n│  WebOrganizer (24 topics)                                    │\n│    ├─&gt; Science, Math, and Technology                         │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Software Development                                  │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    ├─&gt; Arts and Entertainment                                │\n│    │     └─&gt; Quality tiers (1-20)                            │\n│    └─&gt; ... (21 more topics)                                  │\n│          └─&gt; Quality tiers (1-20)                            │\n├──────────────────────────────────────────────────────────────┤\n│  Total: 480 subsets (24 x 20)                                │\n└──────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#混合戦略の結果",
    "href": "ja/olmo-3/07-data-mixing.html#混合戦略の結果",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Token-constrained Mixing と Quality-aware Upsampling により、データソースの最適な比率が決定された。\n\n\nWeb テキストのトピック分布において、以下の傾向が観察される:\n優先されたトピック:\n\nScience, Math, and Technology: STEM ドメインを大幅にアップウェイト\nSoftware Development: プログラミングとソフトウェア開発を強化\nEducation: 教育的コンテンツの重視\n\n抑制されたトピック:\n\nエンターテインメント関連のトピック\n一般的なニュースやソーシャルメディアコンテンツ\n\n結果:\n\n1B パラメータモデルで 5x Chinchilla の訓練を行った結果、平均 0.056 BPB の改善を達成\n54 タスク中 13 タスクでのみ性能低下が見られ、最大でも 0.035 BPB の低下に留まる\n\n\n\n\nDCLM（DataComp for Language Models）Baseline と比較して、以下の改善が確認された:\n改善点:\n\nSTEM タスク: 科学、数学、技術関連のタスクで大幅な性能向上\nコーディングタスク: プログラミング能力の向上\n一般知識: 幅広い知識タスクでの性能改善\n\nトレードオフ:\n\n一部のタスクでは若干の性能低下\n全体として、重要なタスクでの性能向上が優先される\n\n\n\n\n\n\n\nImportantデータミキシングの影響\n\n\n\nデータミキシングの最適化は、モデルの性能に大きな影響を与える。STEM ドメインを優先することで、科学的・技術的タスクでの性能が向上し、Olmo 3 の強みとなっている。\n\n\n\n\n\nコードデータにおいても、プログラミング言語別の最適なミックスが決定された:\n優先された言語:\n\nPython: 最も高い重み付け（機械学習、データサイエンスでの重要性）\nJavaScript/TypeScript: Web 開発の主要言語\nC++/Rust: システムプログラミング言語\n\n抑制された言語:\n\nJava: 相対的に低い重み（冗長性の高いコードが多い）\nMarkdown: ドキュメントファイルの制限\n\n結果:\n\nほぼすべてのコーディングベンチマークで改善を達成\nPython 中心のタスクで特に顕著な改善",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/07-data-mixing.html#まとめ",
    "href": "ja/olmo-3/07-data-mixing.html#まとめ",
    "title": "Data Mixing: データミキシング手法",
    "section": "",
    "text": "Data Mixing は、Dolma 3 の品質を決定する重要なプロセスである。Token-constrained Mixing と Quality-aware Upsampling という 2 つの革新的な手法により、限られたトークン予算の下で最適なデータ配分を実現している。\n主な特徴:\n\nToken-constrained Mixing: Swarm-based methods による最適化\nQuality-aware Upsampling: 高品質データの選択的再導入\n480 個のサブセット: トピックと品質による細かい分類\n条件付きミキシング: データソースの継続的改善に対応\n実証された改善: DCLM Baseline と比較して平均 0.056 BPB の改善\n\nこれらの手法により、Dolma 3 は Olmo 3 Base モデルの高い性能を支える基盤となっている。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Base Model Training",
      "Data Mixing 手法"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html",
    "href": "ja/olmo-3/09-delta-learning.html",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning は、Preference tuning（選好調整）における新しいアプローチです。本手法は、SFT（Supervised Fine-Tuning）モデルと Base モデルの「差分」を活用することで、高品質な contrastive data を生成し、DPO（Direct Preference Optimization）の効果を最大化します。\n\n\nDelta Learning の核心は、モデル間の能力差を明示的に捉えることにあります。\n+------------------------------------------------------------------+\n|                      Delta Learning Concept                      |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model    --&gt;  Limited reasoning capability                |\n|  SFT Model     --&gt;  Enhanced reasoning capability                |\n|  Delta         --&gt;  The \"learned\" reasoning ability              |\n|                                                                  |\n|  Goal: Amplify the delta through preference optimization        |\n+------------------------------------------------------------------+\n\n\nSFT モデルは、Base モデルに対して以下の能力を獲得しています。\n\nより構造化された推論プロセス\n段階的な問題解決アプローチ\nタスク固有の知識の適用\n\nDelta Learning は、この「獲得された能力」を優先応答（Preferred response）の生成に活用します。\n\n\n\n\nDolci Think では、Delta Learning を用いて推論能力の向上を図ります（Section 4.3）。\n\n\n+------------------------------------------------------------------+\n|                  Dolci Think Data Generation                     |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Step 1: Sample question from training set                      |\n|  Step 2: Generate response using SFT model (Preferred)          |\n|  Step 3: Generate response using Base model (Dispreferred)      |\n|  Step 4: Apply quality filtering                                |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nPreferred 応答:\n\nDolci Think SFT モデルで生成\n段階的推論プロセスを含む\n最終的な正答に到達\n\nDispreferred 応答:\n\nOLMo2 7B Base モデルで生成\n推論の深さが不足\n誤った結論または不完全な推論\n\n\n\n\n生成されたペアに対して、以下の基準でフィルタリングを実施します。\n\nPreferred 応答が正答を含む\nDispreferred 応答が誤答または不完全\n両応答間に明確な品質差が存在\n\nこの結果、約 1M の高品質な preference pair が作成されました。\n\n\n\n\nDolci Instruct では、Delta Learning を multi-turn 対話の最適化に使用します（Section 5.3）。\n\n\n+------------------------------------------------------------------+\n|                Dolci Instruct Data Generation                    |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Source: Approximately 500K multi-turn prompts                   |\n|                                                                  |\n|  Preferred:                                                      |\n|    - Generated by Dolci Instruct SFT                             |\n|    - Concise, well-structured responses                          |\n|                                                                  |\n|  Dispreferred:                                                   |\n|    - Generated by OLMo2 7B Base                                  |\n|    - Verbose or poorly structured responses                      |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning により、以下の改善が実現されます。\n\n簡潔さの維持: 不要な冗長性を排除\n情報密度の向上: 重要な情報を効率的に伝達\n構造の改善: 論理的な流れを持つ応答\n\n\n\n\n約 500K の multi-turn プロンプトから preference pair を生成し、応答品質の向上を図ります。\n\n\n\n\nDelta Learning による Preference tuning は、複数の利点をもたらします。\n\n\nDPO による追加の最適化により、SFT 単独では到達できない性能レベルを実現します。\n+------------------------------------------------------------------+\n|                    Performance Progression                       |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model  --&gt;  SFT Model  --&gt;  DPO Model (with Delta)        |\n|                                                                  |\n|  Limited     --&gt;  Enhanced   --&gt;  Optimized reasoning            |\n|  reasoning        reasoning       and preference alignment       |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning による DPO は、将来の Reinforcement Learning の基盤となります。\n\n報酬モデルとの整合性: 人間の選好との alignment を改善\n探索の効率化: より良い初期方策を提供\n安定性の向上: RL 訓練の収束を促進\n\n\n\n\nDolci Think での適用により、以下の改善が確認されました。\n\n複雑な問題に対する段階的アプローチの強化\n推論の深さと正確性の向上\n推論フロンティアの拡大\n\n\n\n\n\n\n\nNote他の Preference Tuning 手法との比較\n\n\n\n従来の DPO:\n\n人間によるラベル付けデータを使用\nデータ収集のコストが高い\nスケールに限界がある\n\nRLHF (Reinforcement Learning from Human Feedback):\n\n報酬モデルの訓練が必要\n複雑な実装と調整が必要\n計算コストが高い\n\nDelta Learning の利点:\n\nスケーラビリティ: 合成データにより大規模な訓練が可能\nコスト効率: 人間のアノテーションが不要\n品質保証: モデル間の能力差により、明確な contrastive signal を生成\n柔軟性: 異なるタスクやドメインに容易に適用可能\n\nDelta Learning は、SFT で獲得した能力を最大限に活用し、効率的かつ効果的な preference tuning を実現します。\n\n\n\n\n\n\nDelta Learning は、OLMo2 3B の preference tuning において中心的な役割を果たします。\n主要なポイント:\n\nSFT モデルと Base モデルの差分を活用\n高品質な contrastive data を自動生成\n推論能力と応答品質の両面で性能向上\nスケーラブルで cost-effective な手法\n\nこの手法により、Dolci Think と Dolci Instruct は、それぞれの領域で最先端の性能を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#基本原理",
    "href": "ja/olmo-3/09-delta-learning.html#基本原理",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning の核心は、モデル間の能力差を明示的に捉えることにあります。\n+------------------------------------------------------------------+\n|                      Delta Learning Concept                      |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model    --&gt;  Limited reasoning capability                |\n|  SFT Model     --&gt;  Enhanced reasoning capability                |\n|  Delta         --&gt;  The \"learned\" reasoning ability              |\n|                                                                  |\n|  Goal: Amplify the delta through preference optimization        |\n+------------------------------------------------------------------+\n\n\nSFT モデルは、Base モデルに対して以下の能力を獲得しています。\n\nより構造化された推論プロセス\n段階的な問題解決アプローチ\nタスク固有の知識の適用\n\nDelta Learning は、この「獲得された能力」を優先応答（Preferred response）の生成に活用します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#dolci-think-dpo-での適用",
    "href": "ja/olmo-3/09-delta-learning.html#dolci-think-dpo-での適用",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Dolci Think では、Delta Learning を用いて推論能力の向上を図ります（Section 4.3）。\n\n\n+------------------------------------------------------------------+\n|                  Dolci Think Data Generation                     |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Step 1: Sample question from training set                      |\n|  Step 2: Generate response using SFT model (Preferred)          |\n|  Step 3: Generate response using Base model (Dispreferred)      |\n|  Step 4: Apply quality filtering                                |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nPreferred 応答:\n\nDolci Think SFT モデルで生成\n段階的推論プロセスを含む\n最終的な正答に到達\n\nDispreferred 応答:\n\nOLMo2 7B Base モデルで生成\n推論の深さが不足\n誤った結論または不完全な推論\n\n\n\n\n生成されたペアに対して、以下の基準でフィルタリングを実施します。\n\nPreferred 応答が正答を含む\nDispreferred 応答が誤答または不完全\n両応答間に明確な品質差が存在\n\nこの結果、約 1M の高品質な preference pair が作成されました。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#dolci-instruct-dpo-での適用",
    "href": "ja/olmo-3/09-delta-learning.html#dolci-instruct-dpo-での適用",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Dolci Instruct では、Delta Learning を multi-turn 対話の最適化に使用します（Section 5.3）。\n\n\n+------------------------------------------------------------------+\n|                Dolci Instruct Data Generation                    |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Source: Approximately 500K multi-turn prompts                   |\n|                                                                  |\n|  Preferred:                                                      |\n|    - Generated by Dolci Instruct SFT                             |\n|    - Concise, well-structured responses                          |\n|                                                                  |\n|  Dispreferred:                                                   |\n|    - Generated by OLMo2 7B Base                                  |\n|    - Verbose or poorly structured responses                      |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning により、以下の改善が実現されます。\n\n簡潔さの維持: 不要な冗長性を排除\n情報密度の向上: 重要な情報を効率的に伝達\n構造の改善: 論理的な流れを持つ応答\n\n\n\n\n約 500K の multi-turn プロンプトから preference pair を生成し、応答品質の向上を図ります。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#効果と利点",
    "href": "ja/olmo-3/09-delta-learning.html#効果と利点",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning による Preference tuning は、複数の利点をもたらします。\n\n\nDPO による追加の最適化により、SFT 単独では到達できない性能レベルを実現します。\n+------------------------------------------------------------------+\n|                    Performance Progression                       |\n+------------------------------------------------------------------+\n|                                                                  |\n|  Base Model  --&gt;  SFT Model  --&gt;  DPO Model (with Delta)        |\n|                                                                  |\n|  Limited     --&gt;  Enhanced   --&gt;  Optimized reasoning            |\n|  reasoning        reasoning       and preference alignment       |\n|                                                                  |\n+------------------------------------------------------------------+\n\n\n\nDelta Learning による DPO は、将来の Reinforcement Learning の基盤となります。\n\n報酬モデルとの整合性: 人間の選好との alignment を改善\n探索の効率化: より良い初期方策を提供\n安定性の向上: RL 訓練の収束を促進\n\n\n\n\nDolci Think での適用により、以下の改善が確認されました。\n\n複雑な問題に対する段階的アプローチの強化\n推論の深さと正確性の向上\n推論フロンティアの拡大\n\n\n\n\n\n\n\nNote他の Preference Tuning 手法との比較\n\n\n\n従来の DPO:\n\n人間によるラベル付けデータを使用\nデータ収集のコストが高い\nスケールに限界がある\n\nRLHF (Reinforcement Learning from Human Feedback):\n\n報酬モデルの訓練が必要\n複雑な実装と調整が必要\n計算コストが高い\n\nDelta Learning の利点:\n\nスケーラビリティ: 合成データにより大規模な訓練が可能\nコスト効率: 人間のアノテーションが不要\n品質保証: モデル間の能力差により、明確な contrastive signal を生成\n柔軟性: 異なるタスクやドメインに容易に適用可能\n\nDelta Learning は、SFT で獲得した能力を最大限に活用し、効率的かつ効果的な preference tuning を実現します。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/09-delta-learning.html#まとめ",
    "href": "ja/olmo-3/09-delta-learning.html#まとめ",
    "title": "Delta Learning: 選好調整の新手法",
    "section": "",
    "text": "Delta Learning は、OLMo2 3B の preference tuning において中心的な役割を果たします。\n主要なポイント:\n\nSFT モデルと Base モデルの差分を活用\n高品質な contrastive data を自動生成\n推論能力と応答品質の両面で性能向上\nスケーラブルで cost-effective な手法\n\nこの手法により、Dolci Think と Dolci Instruct は、それぞれの領域で最先端の性能を達成しています。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3",
      "Post-training",
      "Delta Learning"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html",
    "href": "ja/olmo-3/index.html",
    "title": "Olmo 3",
    "section": "",
    "text": "Olmo 3 は、Allen Institute for AI (AI2) が開発した 7B および 32B パラメータスケールの完全オープンな言語モデルファミリーです。このリリースは、モデルの全ライフサイクル（すべてのステージ、チェックポイント、データポイント、依存関係）を含む「モデルフロー (Model Flow)」全体を公開しています。\n主な特徴:\n論文: arXiv:2512.13961",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#目次",
    "href": "ja/olmo-3/index.html#目次",
    "title": "Olmo 3",
    "section": "目次",
    "text": "目次\n\n全体像\n\n\nBase Model Training\n\nDolma 3 データセット\nOlmoBaseEval: 評価スイート\nMidtraining: 中間訓練\nLong-context Extension: 長文脈拡張\nDeduplication: 重複排除\nolmOCR science PDFs\nData Mixing: データミキシング手法\n\n\n\nPost-training\n\nDolci: Post-training データスイート\nDelta Learning: 選好調整の新手法\nOlmoRL / GRPO: 効率的な強化学習",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#モデルバリエーション",
    "href": "ja/olmo-3/index.html#モデルバリエーション",
    "title": "Olmo 3",
    "section": "モデルバリエーション",
    "text": "モデルバリエーション\nOlmo 3 Base: 基盤モデル（7B, 32B）- 最強の完全オープン Base モデル\nOlmo 3 Think: 段階的推論を行う思考型モデル - Qwen 2.5、Gemma 2/3、DeepSeek R1 を上回る\nOlmo 3 Instruct: 簡潔で直接的な応答を生成するモデル - 関数呼び出しに最適化\nOlmo 3 RL-Zero: Base モデルから直接 RL で訓練 - 完全オープンな RL ベンチマーク",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#主な成果",
    "href": "ja/olmo-3/index.html#主な成果",
    "title": "Olmo 3",
    "section": "主な成果",
    "text": "主な成果\nOlmo 3.1 Think 32B の主要ベンチマーク結果:\n\n\n\nカテゴリ\nベンチマーク\nスコア\n\n\n\n\nMath\nMATH\n96.2\n\n\nMath\nAIME 2024\n80.6\n\n\nReasoning\nBigBenchHard\n88.6\n\n\nReasoning\nZebraLogic\n80.1\n\n\nCoding\nHumanEvalPlus\n91.5\n\n\nCoding\nLiveCodeBench v3\n83.3\n\n\nIF\nIFEval\n93.8\n\n\nKnowledge\nMMLU\n86.4",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#訓練コスト",
    "href": "ja/olmo-3/index.html#訓練コスト",
    "title": "Olmo 3",
    "section": "訓練コスト",
    "text": "訓練コスト\n1024 台の H100 GPU を使用して約 56 日（推定コスト: $2.75M）\n\nPretraining: 約 47 日\nPost-training: 約 9 日",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/olmo-3/index.html#オープンアーティファクト",
    "href": "ja/olmo-3/index.html#オープンアーティファクト",
    "title": "Olmo 3",
    "section": "オープンアーティファクト",
    "text": "オープンアーティファクト\nすべての中間チェックポイント、学習データ、コード、評価ツールを公開:\n\nモデル: Base, Think, Instruct, RL-Zero のすべてのチェックポイント\nデータ: Dolma 3（事前学習）、Dolci（後訓練）\nコード: OLMo-core、Open Instruct、duplodocus、OLMES\n\nコア理念: 真にオープンソース AI を推進するには、最終的なモデルだけでなく、そこに至る「道筋」全体を透明かつアクセス可能にする必要がある。",
    "crumbs": [
      "Naoto Iwase",
      "Olmo 3"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html",
    "href": "ja/pdlt/01-effective-theory.html",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "有効理論（Effective Theory）とは、物理学において、多数の微視的構成要素から成る系の巨視的な振る舞いを記述するための理論的枠組みである。このアプローチは、個々の要素の詳細な動作を追跡するのではなく、系全体の統計的性質から創発する法則を導き出すことに焦点を当てる。\nPDLT では、この物理学の有効理論アプローチを深層ニューラルネットワークの理解に応用する。深層ニューラルネットワークは数十億のパラメータを持つ複雑な系だが、その巨視的な計算能力は、微視的なパラメータの統計的性質から理解できる。\n\n\n\n\n\n19世紀の産業革命において、蒸気機関は社会を変革する技術であったが、その動作原理は「ブラックボックス」として扱われていた。蒸気機関は膨大な数の水分子から構成されており、個々の分子の運動を追跡することは不可能であった。\n熱力学（Thermodynamics）は、この巨視的な系の振る舞いを経験的に記述するために生まれた。熱力学の法則（エネルギー保存則、エントロピー増大則など）は、実験的観察から導出され、蒸気機関の効率向上に大きく貢献した。\nしかし、熱力学の法則は現象論的（phenomenological）であり、なぜそのような法則が成り立つのかという根本的な理由は説明できなかった。\n\n\n\n後に、Maxwell、Boltzmann、Gibbs らによって統計力学（Statistical Mechanics）が確立され、熱力学の法則が微視的な分子の統計的振る舞いから創発（emergent）することが示された。\n統計力学は、以下のような重要な洞察をもたらした:\n\n巨視的な法則は、多数の微視的要素の統計的振る舞いから創発する\n個々の分子の詳細な動作は、巨視的な性質に直接的な影響を与えない\n確率的な記述により、決定論的な微視的法則から巨視的な確率論的法則が導かれる\n\n\n\n\n\n\n\nNote統計力学の歴史的意義\n\n\n\n\n\n統計力学は、単に蒸気機関の理論的理解に留まらず、以下の発展につながった:\n\n原子・分子の存在の実証: 統計力学の精密な予測が実験で確認され、物質が原子・分子から構成されていることが科学的に受け入れられた\n量子力学の発見: 統計力学の精密化により、古典力学では説明できない現象が明らかになり、量子力学の発見につながった\nトランジスタとコンピュータ: 量子力学の応用により、トランジスタが発明され、情報時代の基盤となった\n現代の AI への道: コンピュータの発展により、深層学習などの現代的な AI 技術が可能になった\n\nこの歴史的な流れは、人工的なシステム（蒸気機関）の理解が、自然界の根本的な法則の発見につながるという重要な教訓を示している。\n\n\n\n\n\n\n\n\n\n深層ニューラルネットワークと蒸気機関には、以下のような類似点がある:\n┌──────────────────────────────────────────────────────────┐\n│                                                          │\n│  Steam Engine (19th century)                             │\n│  --------------------------------                        │\n│  - Many molecules (~10^23)                               │\n│  - Macroscopic: temperature, pressure, work              │\n│  - Microscopic: molecular motion                         │\n│  - Black box: empirical understanding                    │\n│                                                          │\n│                                                          │\n│  Deep Neural Network (21st century)                      │\n│  -----------------------------------                     │\n│  - Many parameters (~10^9-10^11)                         │\n│  - Macroscopic: function approximation                   │\n│  - Microscopic: neuron computations                      │\n│  - Black box: empirical understanding                    │\n│                                                          │\n└──────────────────────────────────────────────────────────┘\n両者とも、膨大な数の微視的要素から構成され、その巨視的な振る舞いは経験的には理解されているが、理論的な第一原理からの導出は困難であった。\n\n\n\n深層学習において、この二つのスケールは以下のように対応する:\n微視的（Microscopic）:\n\n個々のニューロンの計算\nパラメータ（重み、バイアス）の値\n活性化関数の選択\n層ごとの順伝播・逆伝播\n\n巨視的（Macroscopic）:\n\nネットワーク全体が計算する関数 \\(f(x; \\theta)\\)\n汎化性能\n訓練の収束性\n表現学習の能力\n\n\n\n\n深層学習に有効理論アプローチを適用する目標は、以下の通りである:\n\n微視的記述から巨視的振る舞いを導出する: パラメータの統計的性質から、ネットワークが計算する関数の性質を予測する\n創発現象を理解する: 表現学習や汎化性能などの創発的性質が、どのように微視的構造から生まれるかを説明する\n理論と実践の橋渡し: 実際に使われるニューラルネットワークの振る舞いを、理論的に予測可能にする\n\n\n\n\n\nPDLT では、以下のような具体的な方法で有効理論アプローチを適用する:\n\n\nニューラルネットワークのパラメータ \\(\\theta\\) を、確率分布 \\(p(\\theta)\\) からサンプリングされたランダム変数として扱う。\n初期化時には、パラメータは単純な確率分布（例: ガウス分布）からサンプリングされる:\n\\[\np(\\theta) = \\mathcal{N}(0, \\sigma^2 I)\n\\]\nこの確率的な初期化により、ネットワークが計算する関数 \\(f(x; \\theta)\\) も確率分布を持つ:\n\\[\np(\\theta) \\rightarrow p(f)\n\\]\n\n\n\n深層ニューラルネットワークでは、各ニューロンは多数の入力信号を受け取る。幅 \\(n\\) が大きいとき、中心極限定理により、各ニューロンの preactivation はガウス分布に近づく。\nこのガウス性は、統計力学におけるガウス積分やWick の定理などの強力な計算手法を適用可能にする。\n\n\n\n入力層から出力層に向かって信号が伝播する過程を、繰り込み群フロー（Renormalization Group Flow）として捉える。\n各層で、preactivation の分布がどのように変化するかを追跡し、その effective theory を構築する:\n┌──────────────────────────────────────────────────────┐\n│                                                      │\n│  Input Layer                                         │\n│       |                                              │\n│       v                                              │\n│  Layer 1: Gaussian distribution                      │\n│       |                                              │\n│       v                                              │\n│  Layer 2: Slightly non-Gaussian                      │\n│       |                                              │\n│       v                                              │\n│  Layer 3: Accumulation of non-Gaussianity            │\n│       |                                              │\n│       v                                              │\n│  Output Layer                                        │\n│                                                      │\n└──────────────────────────────────────────────────────┘\nこの RG Flow の解析により、深層ネットワークにおける臨界性（Criticality）や普遍性クラス（Universality Classes）が理解できる。\n\n\n\n無限幅極限 \\(n \\to \\infty\\) では、ニューラルネットワークは完全なガウス過程に収束し、解析が容易になる。しかし、この極限では表現学習（Representation Learning）が起こらず、実際の深層学習を記述できない。\nそこで、PDLT では有限幅の摂動論（1/n expansion）を用いる:\n\\[\np(f^*) = p^{\\{0\\}}(f^*) + \\frac{1}{n} p^{\\{1\\}}(f^*) + O\\left(\\frac{1}{n^2}\\right)\n\\]\nここで:\n\n\\(p^{\\{0\\}}(f^*)\\): 無限幅極限（ガウス過程）\n\\(\\frac{1}{n} p^{\\{1\\}}(f^*)\\): 有限幅の第一補正（表現学習を含む）\n\nこの摂動論により、実際の深層ニューラルネットワークの振る舞いを理論的に記述できる。\n\n\n\n訓練中のネットワークの dynamics を記述するために、Neural Tangent Kernel (NTK) を導入する。\n無限幅極限では NTK は訓練中に固定され、ネットワークは線形モデルとして振る舞う。しかし、有限幅では NTK が変化し、これが表現学習を可能にする。\nNTK の変化量は \\(1/n\\) のオーダーで、深さ \\(L\\) とともに蓄積される:\n\\[\n\\text{NTK change} \\sim \\frac{L}{n} = r\n\\]\nここで、\\(r = L/n\\) はアスペクト比（Aspect Ratio）と呼ばれ、有効理論の適用可能性を決定する重要なパラメータである。\n\n\n\n\n有効理論アプローチは、以下のような利点をもたらす:\n予測可能性:\n初期化やアーキテクチャの選択が訓練にどのように影響するかを理論的に予測できる。\n普遍性:\n特定のモデルやデータセットに依存しない、一般的な原理を発見できる。異なる活性化関数が同じ普遍性クラスに属する場合、同様の統計的性質を示すことが予測できる。\n拡張性:\nMLP（Multilayer Perceptron）で開発された理論は、ResNets、Transformers などの他のアーキテクチャにも拡張可能である。\n理論と実践の橋渡し:\n実際に使われる深層学習モデルの振る舞いを、物理学の第一原理から理解できるようになる。\n\n\n\n\n\n\nTip有効理論の哲学\n\n\n\n有効理論アプローチの本質は、「複雑な系を完全に理解しなくても、その巨視的な振る舞いを予測できる」という洞察にある。\n物理学では、この哲学により、素粒子物理学を知らなくても固体物理学を理解でき、分子動力学を知らなくても流体力学を理解できる。\n同様に、深層学習では、個々のニューロンの詳細な動作を追跡しなくても、ネットワーク全体の学習能力や汎化性能を理解できる。\n\n\n\n\n\nPDLT は、この有効理論アプローチを以下のように展開する:\nPart 1: 初期化時の理論（Chapters 0-5）:\n初期化されたニューラルネットワークの統計的性質を解析し、臨界性や普遍性クラスを理解する。\nPart 2: 学習の理論（Chapters 6-11）:\n訓練プロセスを有効理論で記述し、無限幅極限と有限幅の違い、表現学習のメカニズムを明らかにする。\nAppendices:\n情報理論的な視点や、ResNets への拡張を議論する。\nこの構成により、読者は段階的に有効理論の手法を習得し、最終的には実際の深層学習システムの理論的理解に到達できる。\n\n\n\n有効理論アプローチは、物理学で多大な成功を収めた手法であり、深層学習の理論的理解にも適用できる。\n蒸気機関の理解が熱力学から統計力学へと発展したように、深層学習も経験的な理解から理論的な第一原理へと発展する段階にある。\nPDLT は、この発展を推進するための理論的枠組みを提供し、深層学習を「なぜ動くのか」を理解可能な科学へと引き上げることを目指している。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#有効理論アプローチとは",
    "href": "ja/pdlt/01-effective-theory.html#有効理論アプローチとは",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "有効理論（Effective Theory）とは、物理学において、多数の微視的構成要素から成る系の巨視的な振る舞いを記述するための理論的枠組みである。このアプローチは、個々の要素の詳細な動作を追跡するのではなく、系全体の統計的性質から創発する法則を導き出すことに焦点を当てる。\nPDLT では、この物理学の有効理論アプローチを深層ニューラルネットワークの理解に応用する。深層ニューラルネットワークは数十億のパラメータを持つ複雑な系だが、その巨視的な計算能力は、微視的なパラメータの統計的性質から理解できる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#物理学における有効理論の成功例-熱力学と統計力学",
    "href": "ja/pdlt/01-effective-theory.html#物理学における有効理論の成功例-熱力学と統計力学",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "19世紀の産業革命において、蒸気機関は社会を変革する技術であったが、その動作原理は「ブラックボックス」として扱われていた。蒸気機関は膨大な数の水分子から構成されており、個々の分子の運動を追跡することは不可能であった。\n熱力学（Thermodynamics）は、この巨視的な系の振る舞いを経験的に記述するために生まれた。熱力学の法則（エネルギー保存則、エントロピー増大則など）は、実験的観察から導出され、蒸気機関の効率向上に大きく貢献した。\nしかし、熱力学の法則は現象論的（phenomenological）であり、なぜそのような法則が成り立つのかという根本的な理由は説明できなかった。\n\n\n\n後に、Maxwell、Boltzmann、Gibbs らによって統計力学（Statistical Mechanics）が確立され、熱力学の法則が微視的な分子の統計的振る舞いから創発（emergent）することが示された。\n統計力学は、以下のような重要な洞察をもたらした:\n\n巨視的な法則は、多数の微視的要素の統計的振る舞いから創発する\n個々の分子の詳細な動作は、巨視的な性質に直接的な影響を与えない\n確率的な記述により、決定論的な微視的法則から巨視的な確率論的法則が導かれる\n\n\n\n\n\n\n\nNote統計力学の歴史的意義\n\n\n\n\n\n統計力学は、単に蒸気機関の理論的理解に留まらず、以下の発展につながった:\n\n原子・分子の存在の実証: 統計力学の精密な予測が実験で確認され、物質が原子・分子から構成されていることが科学的に受け入れられた\n量子力学の発見: 統計力学の精密化により、古典力学では説明できない現象が明らかになり、量子力学の発見につながった\nトランジスタとコンピュータ: 量子力学の応用により、トランジスタが発明され、情報時代の基盤となった\n現代の AI への道: コンピュータの発展により、深層学習などの現代的な AI 技術が可能になった\n\nこの歴史的な流れは、人工的なシステム（蒸気機関）の理解が、自然界の根本的な法則の発見につながるという重要な教訓を示している。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#深層学習への類推",
    "href": "ja/pdlt/01-effective-theory.html#深層学習への類推",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "深層ニューラルネットワークと蒸気機関には、以下のような類似点がある:\n┌──────────────────────────────────────────────────────────┐\n│                                                          │\n│  Steam Engine (19th century)                             │\n│  --------------------------------                        │\n│  - Many molecules (~10^23)                               │\n│  - Macroscopic: temperature, pressure, work              │\n│  - Microscopic: molecular motion                         │\n│  - Black box: empirical understanding                    │\n│                                                          │\n│                                                          │\n│  Deep Neural Network (21st century)                      │\n│  -----------------------------------                     │\n│  - Many parameters (~10^9-10^11)                         │\n│  - Macroscopic: function approximation                   │\n│  - Microscopic: neuron computations                      │\n│  - Black box: empirical understanding                    │\n│                                                          │\n└──────────────────────────────────────────────────────────┘\n両者とも、膨大な数の微視的要素から構成され、その巨視的な振る舞いは経験的には理解されているが、理論的な第一原理からの導出は困難であった。\n\n\n\n深層学習において、この二つのスケールは以下のように対応する:\n微視的（Microscopic）:\n\n個々のニューロンの計算\nパラメータ（重み、バイアス）の値\n活性化関数の選択\n層ごとの順伝播・逆伝播\n\n巨視的（Macroscopic）:\n\nネットワーク全体が計算する関数 \\(f(x; \\theta)\\)\n汎化性能\n訓練の収束性\n表現学習の能力\n\n\n\n\n深層学習に有効理論アプローチを適用する目標は、以下の通りである:\n\n微視的記述から巨視的振る舞いを導出する: パラメータの統計的性質から、ネットワークが計算する関数の性質を予測する\n創発現象を理解する: 表現学習や汎化性能などの創発的性質が、どのように微視的構造から生まれるかを説明する\n理論と実践の橋渡し: 実際に使われるニューラルネットワークの振る舞いを、理論的に予測可能にする",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#原著における有効理論の具体的な適用",
    "href": "ja/pdlt/01-effective-theory.html#原著における有効理論の具体的な適用",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "PDLT では、以下のような具体的な方法で有効理論アプローチを適用する:\n\n\nニューラルネットワークのパラメータ \\(\\theta\\) を、確率分布 \\(p(\\theta)\\) からサンプリングされたランダム変数として扱う。\n初期化時には、パラメータは単純な確率分布（例: ガウス分布）からサンプリングされる:\n\\[\np(\\theta) = \\mathcal{N}(0, \\sigma^2 I)\n\\]\nこの確率的な初期化により、ネットワークが計算する関数 \\(f(x; \\theta)\\) も確率分布を持つ:\n\\[\np(\\theta) \\rightarrow p(f)\n\\]\n\n\n\n深層ニューラルネットワークでは、各ニューロンは多数の入力信号を受け取る。幅 \\(n\\) が大きいとき、中心極限定理により、各ニューロンの preactivation はガウス分布に近づく。\nこのガウス性は、統計力学におけるガウス積分やWick の定理などの強力な計算手法を適用可能にする。\n\n\n\n入力層から出力層に向かって信号が伝播する過程を、繰り込み群フロー（Renormalization Group Flow）として捉える。\n各層で、preactivation の分布がどのように変化するかを追跡し、その effective theory を構築する:\n┌──────────────────────────────────────────────────────┐\n│                                                      │\n│  Input Layer                                         │\n│       |                                              │\n│       v                                              │\n│  Layer 1: Gaussian distribution                      │\n│       |                                              │\n│       v                                              │\n│  Layer 2: Slightly non-Gaussian                      │\n│       |                                              │\n│       v                                              │\n│  Layer 3: Accumulation of non-Gaussianity            │\n│       |                                              │\n│       v                                              │\n│  Output Layer                                        │\n│                                                      │\n└──────────────────────────────────────────────────────┘\nこの RG Flow の解析により、深層ネットワークにおける臨界性（Criticality）や普遍性クラス（Universality Classes）が理解できる。\n\n\n\n無限幅極限 \\(n \\to \\infty\\) では、ニューラルネットワークは完全なガウス過程に収束し、解析が容易になる。しかし、この極限では表現学習（Representation Learning）が起こらず、実際の深層学習を記述できない。\nそこで、PDLT では有限幅の摂動論（1/n expansion）を用いる:\n\\[\np(f^*) = p^{\\{0\\}}(f^*) + \\frac{1}{n} p^{\\{1\\}}(f^*) + O\\left(\\frac{1}{n^2}\\right)\n\\]\nここで:\n\n\\(p^{\\{0\\}}(f^*)\\): 無限幅極限（ガウス過程）\n\\(\\frac{1}{n} p^{\\{1\\}}(f^*)\\): 有限幅の第一補正（表現学習を含む）\n\nこの摂動論により、実際の深層ニューラルネットワークの振る舞いを理論的に記述できる。\n\n\n\n訓練中のネットワークの dynamics を記述するために、Neural Tangent Kernel (NTK) を導入する。\n無限幅極限では NTK は訓練中に固定され、ネットワークは線形モデルとして振る舞う。しかし、有限幅では NTK が変化し、これが表現学習を可能にする。\nNTK の変化量は \\(1/n\\) のオーダーで、深さ \\(L\\) とともに蓄積される:\n\\[\n\\text{NTK change} \\sim \\frac{L}{n} = r\n\\]\nここで、\\(r = L/n\\) はアスペクト比（Aspect Ratio）と呼ばれ、有効理論の適用可能性を決定する重要なパラメータである。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#有効理論アプローチの利点",
    "href": "ja/pdlt/01-effective-theory.html#有効理論アプローチの利点",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "有効理論アプローチは、以下のような利点をもたらす:\n予測可能性:\n初期化やアーキテクチャの選択が訓練にどのように影響するかを理論的に予測できる。\n普遍性:\n特定のモデルやデータセットに依存しない、一般的な原理を発見できる。異なる活性化関数が同じ普遍性クラスに属する場合、同様の統計的性質を示すことが予測できる。\n拡張性:\nMLP（Multilayer Perceptron）で開発された理論は、ResNets、Transformers などの他のアーキテクチャにも拡張可能である。\n理論と実践の橋渡し:\n実際に使われる深層学習モデルの振る舞いを、物理学の第一原理から理解できるようになる。\n\n\n\n\n\n\nTip有効理論の哲学\n\n\n\n有効理論アプローチの本質は、「複雑な系を完全に理解しなくても、その巨視的な振る舞いを予測できる」という洞察にある。\n物理学では、この哲学により、素粒子物理学を知らなくても固体物理学を理解でき、分子動力学を知らなくても流体力学を理解できる。\n同様に、深層学習では、個々のニューロンの詳細な動作を追跡しなくても、ネットワーク全体の学習能力や汎化性能を理解できる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#原著の構成との関連",
    "href": "ja/pdlt/01-effective-theory.html#原著の構成との関連",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "PDLT は、この有効理論アプローチを以下のように展開する:\nPart 1: 初期化時の理論（Chapters 0-5）:\n初期化されたニューラルネットワークの統計的性質を解析し、臨界性や普遍性クラスを理解する。\nPart 2: 学習の理論（Chapters 6-11）:\n訓練プロセスを有効理論で記述し、無限幅極限と有限幅の違い、表現学習のメカニズムを明らかにする。\nAppendices:\n情報理論的な視点や、ResNets への拡張を議論する。\nこの構成により、読者は段階的に有効理論の手法を習得し、最終的には実際の深層学習システムの理論的理解に到達できる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/01-effective-theory.html#まとめ",
    "href": "ja/pdlt/01-effective-theory.html#まとめ",
    "title": "Effective Theory Approach",
    "section": "",
    "text": "有効理論アプローチは、物理学で多大な成功を収めた手法であり、深層学習の理論的理解にも適用できる。\n蒸気機関の理解が熱力学から統計力学へと発展したように、深層学習も経験的な理解から理論的な第一原理へと発展する段階にある。\nPDLT は、この発展を推進するための理論的枠組みを提供し、深層学習を「なぜ動くのか」を理解可能な科学へと引き上げることを目指している。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Effective Theory Approach"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html",
    "href": "ja/pdlt/03-criticality.html",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality（臨界性）は、深層ニューラルネットワークの初期化において最も重要な概念の一つである。適切な初期化ハイパーパラメータの選択により、信号が層を通じて指数的に増大（exploding）または減衰（vanishing）することなく、臨界点（critical point）に保たれる。この criticality の概念は、統計物理学における相転移（phase transition）の理論から着想を得ており、深層学習の訓練可能性（trainability）と汎化性能（generalization）の両方に深く関わっている。\n\n\n\n深層ニューラルネットワークにおいて、信号が層を通過する際の振る舞いは、初期化ハイパーパラメータに強く依存する。適切でない初期化は、以下の二つの問題のいずれかを引き起こす。\n\n\n重みの分散 \\(C_W &gt; 1\\) の場合、preactivation の共分散（covariance）\\(G^{(\\ell)}_{\\alpha\\beta}\\) は層を通じて指数的に増大する:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta} \\to \\infty \\quad \\text{as } \\ell \\to \\infty\n\\]\nこの状況では:\n\nネットワーク出力の発散: 出力値が exponentially large となり、数値的不安定性を引き起こす\n勾配の爆発: バックプロパゲーション時の勾配が exponentially large となり、パラメータ更新が不安定になる\n訓練の失敗: 損失関数の値が発散し、学習が進まない\n\n\n\n\n逆に、重みの分散 \\(C_W &lt; 1\\) の場合、共分散は層を通じて指数的に減衰する:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta} \\to 0 \\quad \\text{as } \\ell \\to \\infty\n\\]\nこの状況では:\n\n情報の損失: 入力データの構造が深層で消失し、全ての入力が同じ出力に写像される\n勾配の消失: バックプロパゲーション時の勾配が exponentially small となり、浅い層のパラメータが更新されない\n表現学習の不能: ネットワークが有用な内部表現を学習できない\n\n\n\n\n\n\n\nImportantExploding と Vanishing は双対問題\n\n\n\nExploding problem と vanishing problem は、同じ数学的構造の両端に位置する双対的な問題である。Forward pass における kernel の exploding/vanishing は、backward pass における gradient の exploding/vanishing に対応する（Chapter 9.4 参照）。\n\n\n\n\n\n\n\n\nExploding と vanishing の二つの regime の境界に、特別な点が存在する。この点では、重みの分散が \\(C_W = 1\\) に調整されており、共分散が層を通じて保存される:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = G^{(0)}_{\\alpha\\beta} \\quad \\text{for all } \\ell\n\\]\nこの点を critical point（臨界点）と呼び、このような初期化を critical initialization（臨界初期化）と呼ぶ。Critical point は以下の特徴を持つ:\n\nSelf-similarity（自己相似性）: 共分散の統計的性質が層を通じて不変\nNontrivial fixed point（非自明固定点）: 入力データの構造を保持する固定点（\\(G^* \\neq 0, \\infty\\)）\nStable information flow（安定的情報伝播）: 信号が exponential behavior なしに伝播\n\n\n\n\n\n\n\nNote統計物理学との類似\n\n\n\nCritical point の概念は、統計物理学における臨界温度（critical temperature）の類推である。例えば、磁石は高温では常磁性（paramagnetic）相、低温では強磁性（ferromagnetic）相を示すが、臨界温度では自己相似性を示す特別な状態になる。深層ニューラルネットワークにおいても、\\(C_W\\) が臨界値から外れると、vanishing 相（\\(C_W &lt; 1\\)）または exploding 相（\\(C_W &gt; 1\\)）に遷移する。\n\n\n\n\n\n\n\n\n最も単純なケースとして、活性化関数が恒等写像である deep linear networks を考える。この場合、共分散の recursion は以下の形をとる:\n\\[\nG^{(\\ell+1)}_{\\alpha\\beta} = C_W G^{(\\ell)}_{\\alpha\\beta}\n\\]\n初期条件 \\(G^{(0)}_{\\alpha\\beta} = \\frac{1}{n_0} \\sum_{i=1}^{n_0} x_{i;\\alpha} x_{i;\\beta}\\) の下で、この recursion の解は:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta}\n\\]\nCriticality condition は:\n\\[\nC_W = 1\n\\]\nこの条件下で、共分散は層を通じて完全に保存される。\n\n\n\n一般の活性化関数 \\(\\sigma(z)\\) を持つ MLP の場合、kernel \\(K^{(\\ell)}_{\\alpha\\beta}\\) の recursion は以下の形をとる:\n\\[\nK^{(\\ell+1)}_{\\alpha\\beta} = C_b + C_W \\langle \\sigma_\\alpha \\sigma_\\beta \\rangle_{K^{(\\ell)}}\n\\]\nここで、\\(\\langle \\cdot \\rangle_{K^{(\\ell)}}\\) は kernel \\(K^{(\\ell)}\\) に関する Gaussian expectation である。\n\n\n単一入力 \\(\\alpha = 0\\) の場合、diagonal component \\(K^{(\\ell)}_{00}\\) は self-consistent に解ける:\n\\[\nK^{(\\ell+1)}_{00} = C_b + C_W g(K^{(\\ell)}_{00})\n\\]\nここで、helper function \\(g(K)\\) は:\n\\[\ng(K) \\equiv \\langle \\sigma(z) \\sigma(z) \\rangle_K = \\frac{1}{\\sqrt{2\\pi K}} \\int_{-\\infty}^{\\infty} dz \\, e^{-\\frac{z^2}{2K}} \\sigma(z) \\sigma(z)\n\\]\nFixed point \\(K^*_{00}\\) の周りで線形化すると:\n\\[\n\\Delta K^{(\\ell+1)}_{00} = \\chi_\\parallel(K^*_{00}) \\Delta K^{(\\ell)}_{00} + O(\\Delta^2)\n\\]\nここで、parallel susceptibility（平行感受率）は:\n\\[\n\\chi_\\parallel(K) \\equiv C_W g'(K) = \\frac{C_W}{2K^2} \\left\\langle \\sigma(z) \\sigma(z) (z^2 - K) \\right\\rangle_K\n\\]\n第一の criticality condition は:\n\\[\n\\chi_\\parallel(K^*_{00}) = 1\n\\]\nこの条件により、kernel が exponentially explode または vanish することを防ぐ。\n\n\n\n二つの異なる入力 \\(\\alpha = \\pm\\) の場合、off-diagonal component \\(K^{(\\ell)}_{+-}\\) の evolution を考える必要がある。Coincident limit \\(x_{i;+}, x_{i;-} \\to x_{i;0}\\) の周りで線形化すると、perpendicular susceptibility（垂直感受率）が現れる:\n\\[\n\\chi_\\perp(K) \\equiv C_W \\langle \\sigma'(z) \\sigma'(z) \\rangle_K\n\\]\n第二の criticality condition は:\n\\[\n\\chi_\\perp(K^*) = 1\n\\]\nこの条件により、異なる入力間の correlation が適切に保存される。\n\n\n\n\n\n\nTipParallel と Perpendicular の意味\n\n\n\n\nParallel susceptibility \\(\\chi_\\parallel\\): Kernel matrix の diagonal direction に沿った perturbation の増幅率\nPerpendicular susceptibility \\(\\chi_\\perp\\): Kernel matrix の off-diagonal direction に沿った perturbation の増幅率\n\n両方の susceptibility が 1 に調整されることで、kernel matrix の全ての component が安定的に伝播する。\n\n\n\n\n\n\n\n異なる活性化関数は、criticality の振る舞いに基づいて universality classes（普遍性クラス）に分類される（Chapter 5.2-5.3 参照）。\n\n\n代表的な活性化関数: ReLU, leaky ReLU, absolute value\n特徴:\n\nPerfect self-similarity: \\(K^{(\\ell)}_{00} = K^{(1)}_{00}\\) for all \\(\\ell\\)\nFixed point が入力依存: \\(K^*_{00} = C_b + C_W \\sum_i x_{i;0} x_{i;0} / n_0\\)\nHigher-order corrections が消失: \\(O(\\Delta^{p&gt;1}) = 0\\)\n\nCritical initialization:\n\\[\nC_W = 1, \\quad C_b = 0\n\\]\n\n\n\n代表的な活性化関数: tanh, sin, erf\n特徴:\n\nFixed point が zero: \\(K^*_{00} = 0\\)\nPower-law decay: \\(K^{(\\ell)}_{00} \\sim 1/\\ell^q\\) with \\(0 &lt; q \\leq 1\\)\nHigher-order corrections が重要\n\nCritical initialization: Activation function に依存した調整が必要\n\n\n\n代表的な活性化関数: SWISH, GELU\n特徴:\n\nPower-law decay towards nonzero fixed point: \\(K^*_{00} \\neq 0\\)\n部分的な stability\n\nこれらの universality class の理解により、異なる活性化関数が深層ネットワークでどう振る舞うかを予測できる。\n\n\n\n\nCriticality の概念は、もともと exploding and vanishing gradient problem を解決するために導入された。\n\n\nBackpropagation において、損失関数の勾配は chain rule により以下の形をとる:\n\\[\n\\frac{dL_A}{d\\theta^{(\\ell)}_\\mu} = \\sum_{\\alpha \\in D} \\sum_{i_L} \\sum_{i_\\ell} \\epsilon_{i_L;\\alpha} \\frac{dz^{(L)}_{i_L;\\alpha}}{dz^{(\\ell)}_{i_\\ell;\\alpha}} \\frac{dz^{(\\ell)}_{i_\\ell;\\alpha}}{d\\theta^{(\\ell)}_\\mu}\n\\]\nここで、中間の chain-rule factor は:\n\\[\n\\frac{dz^{(L)}_{i_L;\\alpha}}{dz^{(\\ell)}_{i_\\ell;\\alpha}} = \\sum_{i_{\\ell+1}, \\ldots, i_{L-1}} \\prod_{\\ell'=\\ell}^{L-1} \\left[ W^{(\\ell'+1)}_{i_{\\ell'+1} i_{\\ell'}} \\sigma'^{(\\ell')}_{i_{\\ell'};\\alpha} \\right]\n\\]\nこの product of matrices が、層 \\(\\ell\\) から層 \\(L\\) にかけて exponential behavior を示すことが、gradient exploding/vanishing problem の本質である。\n\n\n\nCriticality は、この問題を以下の三つの観点から解決する:\n1. Error Factor の制御:\nMean Squared Error (MSE) loss の場合、error factor は:\n\\[\n\\epsilon_{i;\\alpha} = z^{(L)}_{i;\\alpha} - y_{i;\\alpha}\n\\]\nKernel が explode すると、出力 \\(z^{(L)}_{i;\\alpha}\\) も explode し、error factor が発散する。Criticality condition \\(\\chi_\\parallel(K^*) \\leq 1\\) により、これを防ぐ。\n2. Trivial Factor の制御:\n重みに関する trivial factor は:\n\\[\n\\frac{dz^{(\\ell)}_{i;\\alpha}}{dW^{(\\ell)}_{jk}} = \\delta_{ij} \\sigma^{(\\ell-1)}_{k;\\alpha}\n\\]\nKernel が vanish すると、activation \\(\\sigma^{(\\ell-1)}_{k;\\alpha}\\) も vanish し、深い層の重みが訓練されない。Criticality condition \\(\\chi_\\parallel(K^*) \\geq 1\\) により、これを防ぐ。\n3. Chain-rule Factor の制御:\nChain-rule factor は、Neural Tangent Kernel (NTK) の中に \\(\\prod_{\\ell'=\\ell}^{\\ell-1} \\chi_\\perp^{(\\ell')}\\) の形で encode されている。Criticality condition \\(\\chi_\\perp(K^*) = 1\\) により、この積が exponentially grow/decay しないことが保証される。\n結論: 二つの criticality condition\n\\[\n\\chi_\\parallel(K^*) = 1, \\quad \\chi_\\perp(K^*) = 1\n\\]\nは、kernel の exploding/vanishing と gradient の exploding/vanishing の両方を同時に解決する。\n\n\n\n\n\n\nImportantCriticality の普遍性\n\n\n\nCriticality は、forward pass における kernel の安定性だけでなく、backward pass における gradient の安定性も同時に保証する。この意味で、criticality は深層学習における普遍的な原理である。\n\n\n\n\n\n\nCriticality の理論は、実践的に用いられている初期化手法の理論的基盤を提供する。\n\n\n提案: Glorot & Bengio (2010)\n初期化規則:\n\\[\nW^{(\\ell)}_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell-1} + n_\\ell}\\right)\n\\]\n動機: Forward pass と backward pass の両方で variance を保存\nCriticality との関連: Scale-invariant universality class において、\\(C_W = 1\\) を満たす\n\n\n\n提案: He et al. (2015)\n初期化規則:\n\\[\nW^{(\\ell)}_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell-1}}\\right)\n\\]\n動機: ReLU などの非対称な活性化関数に対して variance を保存\nCriticality との関連: ReLU (scale-invariant class) において、\\(C_W = 1\\) を正確に満たす\n\n\n\n\n\n\nTip理論と実践の一致\n\n\n\nHe initialization と Xavier initialization は、経験的に良い性能を示すことが知られていたが、PDLT の criticality 理論により、これらが 数学的に最適な初期化 であることが証明された。Criticality 理論は、既存の実践的手法を統一的に理解し、新しい活性化関数やアーキテクチャに対する初期化手法を systematic に導出する枠組みを提供する。\n\n\n\n\n\n\nCriticality の効果を視覚的に理解するため、以下の diagram を用いる。\n\n\n┌──────────────────────────────────────────────────────────────┐\n│           Kernel Evolution Phase Diagram                     │\n├──────────────────────────────────────────────────────────────┤\n│                                                              │\n│  K^(l)                                                       │\n│   ^                                                          │\n│   |    Exploding Phase (C_W &gt; 1)                             │\n│   |   /                                                      │\n│   |  /                                                       │\n│   | /                                                        │\n│   |/  Critical Point (C_W = 1)                               │\n│   +─────────────────────────────────────&gt; Layer l            │\n│   |\\                                                         │\n│   | \\                                                        │\n│   |  \\                                                       │\n│   |   \\ Vanishing Phase (C_W &lt; 1)                            │\n│   |    v                                                     │\n│   |     0                                                    │\n│                                                              │\n└──────────────────────────────────────────────────────────────┘\n\nExploding phase (\\(C_W &gt; 1\\)): Kernel が exponentially grow し、\\(K^* = \\infty\\) に到達\nCritical point (\\(C_W = 1\\)): Kernel が一定値に保たれる（nontrivial fixed point）\nVanishing phase (\\(C_W &lt; 1\\)): Kernel が exponentially decay し、\\(K^* = 0\\) に到達\n\n\n\n\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│      Kernel Evolution across Universality Classes                                           │\n├─────────────────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                             │\n│  K^(l)                                                                                      │\n│   ^                                                                                         │\n│   |  Scale-Invariant Class (ReLU)                                                           │\n│   |  ════════════════════════════════ (constant)            │\n│   |                                                                                         │\n│   |  Half-Stable Class (GELU)                                                               │\n│   |  ─────────────.                   (power-law decay)                                     │\n│   |                \\                                                                        │\n│   |                 `─────────────────                                                      │\n│   |                                                                                         │\n│   |  K* = 0 Class (tanh)                                                                    │\n│   |  ───────                          (power-law to zero)                                   │\n│   |         `──                                                                             │\n│   |            `─────                                                                       │\n│   |                  `────────────────                                                      │\n│   +─────────────────────────────────────&gt; Layer l                                           │\n│                                                                                             │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n異なる universality class は、critical initialization 下でも異なる kernel evolution を示す:\n\nScale-Invariant: Perfect preservation (constant)\nHalf-Stable: Power-law decay to nonzero fixed point\nK* = 0: Power-law decay to zero\n\n\n\n\n\nCriticality は、深層ニューラルネットワークの初期化における最重要原理である。\n主要な概念:\n\nExploding/Vanishing Problem: 不適切な初期化により、信号が exponentially grow または decay する問題\nCritical Point: Exploding と vanishing の境界にある特別な点（\\(C_W = 1\\)）\nCriticality Conditions: \\(\\chi_\\parallel(K^*) = 1\\) と \\(\\chi_\\perp(K^*) = 1\\) の二つの条件\nUniversality Classes: 活性化関数の criticality 挙動による分類\n\n実践的意義:\n\nHe/Xavier Initialization の理論的基盤: 既存の実践的手法が数学的に最適であることの証明\n新しい活性化関数への適用: Criticality 理論により、任意の活性化関数に対する最適初期化を systematic に導出可能\nForward と Backward の統一: Kernel の安定性と gradient の安定性が同じ条件で達成される\n\n理論的意義:\n\n統計物理学との接続: Phase transition の理論を深層学習に応用\n有効理論の枠組み: Criticality は有効理論アプローチの中心的概念\n深層極限の理解: 深層ニューラルネットワークの asymptotic behavior の理解\n\nCriticality は、深層学習を「なぜ動くのか」から「どう設計すべきか」へと導く、理論と実践の橋渡しとなる概念である。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#概要",
    "href": "ja/pdlt/03-criticality.html#概要",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality（臨界性）は、深層ニューラルネットワークの初期化において最も重要な概念の一つである。適切な初期化ハイパーパラメータの選択により、信号が層を通じて指数的に増大（exploding）または減衰（vanishing）することなく、臨界点（critical point）に保たれる。この criticality の概念は、統計物理学における相転移（phase transition）の理論から着想を得ており、深層学習の訓練可能性（trainability）と汎化性能（generalization）の両方に深く関わっている。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#exploding-と-vanishing-の問題",
    "href": "ja/pdlt/03-criticality.html#exploding-と-vanishing-の問題",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "深層ニューラルネットワークにおいて、信号が層を通過する際の振る舞いは、初期化ハイパーパラメータに強く依存する。適切でない初期化は、以下の二つの問題のいずれかを引き起こす。\n\n\n重みの分散 \\(C_W &gt; 1\\) の場合、preactivation の共分散（covariance）\\(G^{(\\ell)}_{\\alpha\\beta}\\) は層を通じて指数的に増大する:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta} \\to \\infty \\quad \\text{as } \\ell \\to \\infty\n\\]\nこの状況では:\n\nネットワーク出力の発散: 出力値が exponentially large となり、数値的不安定性を引き起こす\n勾配の爆発: バックプロパゲーション時の勾配が exponentially large となり、パラメータ更新が不安定になる\n訓練の失敗: 損失関数の値が発散し、学習が進まない\n\n\n\n\n逆に、重みの分散 \\(C_W &lt; 1\\) の場合、共分散は層を通じて指数的に減衰する:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta} \\to 0 \\quad \\text{as } \\ell \\to \\infty\n\\]\nこの状況では:\n\n情報の損失: 入力データの構造が深層で消失し、全ての入力が同じ出力に写像される\n勾配の消失: バックプロパゲーション時の勾配が exponentially small となり、浅い層のパラメータが更新されない\n表現学習の不能: ネットワークが有用な内部表現を学習できない\n\n\n\n\n\n\n\nImportantExploding と Vanishing は双対問題\n\n\n\nExploding problem と vanishing problem は、同じ数学的構造の両端に位置する双対的な問題である。Forward pass における kernel の exploding/vanishing は、backward pass における gradient の exploding/vanishing に対応する（Chapter 9.4 参照）。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#critical-initialization臨界初期化",
    "href": "ja/pdlt/03-criticality.html#critical-initialization臨界初期化",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Exploding と vanishing の二つの regime の境界に、特別な点が存在する。この点では、重みの分散が \\(C_W = 1\\) に調整されており、共分散が層を通じて保存される:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = G^{(0)}_{\\alpha\\beta} \\quad \\text{for all } \\ell\n\\]\nこの点を critical point（臨界点）と呼び、このような初期化を critical initialization（臨界初期化）と呼ぶ。Critical point は以下の特徴を持つ:\n\nSelf-similarity（自己相似性）: 共分散の統計的性質が層を通じて不変\nNontrivial fixed point（非自明固定点）: 入力データの構造を保持する固定点（\\(G^* \\neq 0, \\infty\\)）\nStable information flow（安定的情報伝播）: 信号が exponential behavior なしに伝播\n\n\n\n\n\n\n\nNote統計物理学との類似\n\n\n\nCritical point の概念は、統計物理学における臨界温度（critical temperature）の類推である。例えば、磁石は高温では常磁性（paramagnetic）相、低温では強磁性（ferromagnetic）相を示すが、臨界温度では自己相似性を示す特別な状態になる。深層ニューラルネットワークにおいても、\\(C_W\\) が臨界値から外れると、vanishing 相（\\(C_W &lt; 1\\)）または exploding 相（\\(C_W &gt; 1\\)）に遷移する。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#criticality-の数学的定式化",
    "href": "ja/pdlt/03-criticality.html#criticality-の数学的定式化",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "最も単純なケースとして、活性化関数が恒等写像である deep linear networks を考える。この場合、共分散の recursion は以下の形をとる:\n\\[\nG^{(\\ell+1)}_{\\alpha\\beta} = C_W G^{(\\ell)}_{\\alpha\\beta}\n\\]\n初期条件 \\(G^{(0)}_{\\alpha\\beta} = \\frac{1}{n_0} \\sum_{i=1}^{n_0} x_{i;\\alpha} x_{i;\\beta}\\) の下で、この recursion の解は:\n\\[\nG^{(\\ell)}_{\\alpha\\beta} = (C_W)^\\ell G^{(0)}_{\\alpha\\beta}\n\\]\nCriticality condition は:\n\\[\nC_W = 1\n\\]\nこの条件下で、共分散は層を通じて完全に保存される。\n\n\n\n一般の活性化関数 \\(\\sigma(z)\\) を持つ MLP の場合、kernel \\(K^{(\\ell)}_{\\alpha\\beta}\\) の recursion は以下の形をとる:\n\\[\nK^{(\\ell+1)}_{\\alpha\\beta} = C_b + C_W \\langle \\sigma_\\alpha \\sigma_\\beta \\rangle_{K^{(\\ell)}}\n\\]\nここで、\\(\\langle \\cdot \\rangle_{K^{(\\ell)}}\\) は kernel \\(K^{(\\ell)}\\) に関する Gaussian expectation である。\n\n\n単一入力 \\(\\alpha = 0\\) の場合、diagonal component \\(K^{(\\ell)}_{00}\\) は self-consistent に解ける:\n\\[\nK^{(\\ell+1)}_{00} = C_b + C_W g(K^{(\\ell)}_{00})\n\\]\nここで、helper function \\(g(K)\\) は:\n\\[\ng(K) \\equiv \\langle \\sigma(z) \\sigma(z) \\rangle_K = \\frac{1}{\\sqrt{2\\pi K}} \\int_{-\\infty}^{\\infty} dz \\, e^{-\\frac{z^2}{2K}} \\sigma(z) \\sigma(z)\n\\]\nFixed point \\(K^*_{00}\\) の周りで線形化すると:\n\\[\n\\Delta K^{(\\ell+1)}_{00} = \\chi_\\parallel(K^*_{00}) \\Delta K^{(\\ell)}_{00} + O(\\Delta^2)\n\\]\nここで、parallel susceptibility（平行感受率）は:\n\\[\n\\chi_\\parallel(K) \\equiv C_W g'(K) = \\frac{C_W}{2K^2} \\left\\langle \\sigma(z) \\sigma(z) (z^2 - K) \\right\\rangle_K\n\\]\n第一の criticality condition は:\n\\[\n\\chi_\\parallel(K^*_{00}) = 1\n\\]\nこの条件により、kernel が exponentially explode または vanish することを防ぐ。\n\n\n\n二つの異なる入力 \\(\\alpha = \\pm\\) の場合、off-diagonal component \\(K^{(\\ell)}_{+-}\\) の evolution を考える必要がある。Coincident limit \\(x_{i;+}, x_{i;-} \\to x_{i;0}\\) の周りで線形化すると、perpendicular susceptibility（垂直感受率）が現れる:\n\\[\n\\chi_\\perp(K) \\equiv C_W \\langle \\sigma'(z) \\sigma'(z) \\rangle_K\n\\]\n第二の criticality condition は:\n\\[\n\\chi_\\perp(K^*) = 1\n\\]\nこの条件により、異なる入力間の correlation が適切に保存される。\n\n\n\n\n\n\nTipParallel と Perpendicular の意味\n\n\n\n\nParallel susceptibility \\(\\chi_\\parallel\\): Kernel matrix の diagonal direction に沿った perturbation の増幅率\nPerpendicular susceptibility \\(\\chi_\\perp\\): Kernel matrix の off-diagonal direction に沿った perturbation の増幅率\n\n両方の susceptibility が 1 に調整されることで、kernel matrix の全ての component が安定的に伝播する。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#universality-classes普遍性クラス",
    "href": "ja/pdlt/03-criticality.html#universality-classes普遍性クラス",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "異なる活性化関数は、criticality の振る舞いに基づいて universality classes（普遍性クラス）に分類される（Chapter 5.2-5.3 参照）。\n\n\n代表的な活性化関数: ReLU, leaky ReLU, absolute value\n特徴:\n\nPerfect self-similarity: \\(K^{(\\ell)}_{00} = K^{(1)}_{00}\\) for all \\(\\ell\\)\nFixed point が入力依存: \\(K^*_{00} = C_b + C_W \\sum_i x_{i;0} x_{i;0} / n_0\\)\nHigher-order corrections が消失: \\(O(\\Delta^{p&gt;1}) = 0\\)\n\nCritical initialization:\n\\[\nC_W = 1, \\quad C_b = 0\n\\]\n\n\n\n代表的な活性化関数: tanh, sin, erf\n特徴:\n\nFixed point が zero: \\(K^*_{00} = 0\\)\nPower-law decay: \\(K^{(\\ell)}_{00} \\sim 1/\\ell^q\\) with \\(0 &lt; q \\leq 1\\)\nHigher-order corrections が重要\n\nCritical initialization: Activation function に依存した調整が必要\n\n\n\n代表的な活性化関数: SWISH, GELU\n特徴:\n\nPower-law decay towards nonzero fixed point: \\(K^*_{00} \\neq 0\\)\n部分的な stability\n\nこれらの universality class の理解により、異なる活性化関数が深層ネットワークでどう振る舞うかを予測できる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#gradient-explodingvanishing-との関連chapter-9.1-9.4",
    "href": "ja/pdlt/03-criticality.html#gradient-explodingvanishing-との関連chapter-9.1-9.4",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality の概念は、もともと exploding and vanishing gradient problem を解決するために導入された。\n\n\nBackpropagation において、損失関数の勾配は chain rule により以下の形をとる:\n\\[\n\\frac{dL_A}{d\\theta^{(\\ell)}_\\mu} = \\sum_{\\alpha \\in D} \\sum_{i_L} \\sum_{i_\\ell} \\epsilon_{i_L;\\alpha} \\frac{dz^{(L)}_{i_L;\\alpha}}{dz^{(\\ell)}_{i_\\ell;\\alpha}} \\frac{dz^{(\\ell)}_{i_\\ell;\\alpha}}{d\\theta^{(\\ell)}_\\mu}\n\\]\nここで、中間の chain-rule factor は:\n\\[\n\\frac{dz^{(L)}_{i_L;\\alpha}}{dz^{(\\ell)}_{i_\\ell;\\alpha}} = \\sum_{i_{\\ell+1}, \\ldots, i_{L-1}} \\prod_{\\ell'=\\ell}^{L-1} \\left[ W^{(\\ell'+1)}_{i_{\\ell'+1} i_{\\ell'}} \\sigma'^{(\\ell')}_{i_{\\ell'};\\alpha} \\right]\n\\]\nこの product of matrices が、層 \\(\\ell\\) から層 \\(L\\) にかけて exponential behavior を示すことが、gradient exploding/vanishing problem の本質である。\n\n\n\nCriticality は、この問題を以下の三つの観点から解決する:\n1. Error Factor の制御:\nMean Squared Error (MSE) loss の場合、error factor は:\n\\[\n\\epsilon_{i;\\alpha} = z^{(L)}_{i;\\alpha} - y_{i;\\alpha}\n\\]\nKernel が explode すると、出力 \\(z^{(L)}_{i;\\alpha}\\) も explode し、error factor が発散する。Criticality condition \\(\\chi_\\parallel(K^*) \\leq 1\\) により、これを防ぐ。\n2. Trivial Factor の制御:\n重みに関する trivial factor は:\n\\[\n\\frac{dz^{(\\ell)}_{i;\\alpha}}{dW^{(\\ell)}_{jk}} = \\delta_{ij} \\sigma^{(\\ell-1)}_{k;\\alpha}\n\\]\nKernel が vanish すると、activation \\(\\sigma^{(\\ell-1)}_{k;\\alpha}\\) も vanish し、深い層の重みが訓練されない。Criticality condition \\(\\chi_\\parallel(K^*) \\geq 1\\) により、これを防ぐ。\n3. Chain-rule Factor の制御:\nChain-rule factor は、Neural Tangent Kernel (NTK) の中に \\(\\prod_{\\ell'=\\ell}^{\\ell-1} \\chi_\\perp^{(\\ell')}\\) の形で encode されている。Criticality condition \\(\\chi_\\perp(K^*) = 1\\) により、この積が exponentially grow/decay しないことが保証される。\n結論: 二つの criticality condition\n\\[\n\\chi_\\parallel(K^*) = 1, \\quad \\chi_\\perp(K^*) = 1\n\\]\nは、kernel の exploding/vanishing と gradient の exploding/vanishing の両方を同時に解決する。\n\n\n\n\n\n\nImportantCriticality の普遍性\n\n\n\nCriticality は、forward pass における kernel の安定性だけでなく、backward pass における gradient の安定性も同時に保証する。この意味で、criticality は深層学習における普遍的な原理である。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#実践的な初期化手法との比較",
    "href": "ja/pdlt/03-criticality.html#実践的な初期化手法との比較",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality の理論は、実践的に用いられている初期化手法の理論的基盤を提供する。\n\n\n提案: Glorot & Bengio (2010)\n初期化規則:\n\\[\nW^{(\\ell)}_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell-1} + n_\\ell}\\right)\n\\]\n動機: Forward pass と backward pass の両方で variance を保存\nCriticality との関連: Scale-invariant universality class において、\\(C_W = 1\\) を満たす\n\n\n\n提案: He et al. (2015)\n初期化規則:\n\\[\nW^{(\\ell)}_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\ell-1}}\\right)\n\\]\n動機: ReLU などの非対称な活性化関数に対して variance を保存\nCriticality との関連: ReLU (scale-invariant class) において、\\(C_W = 1\\) を正確に満たす\n\n\n\n\n\n\nTip理論と実践の一致\n\n\n\nHe initialization と Xavier initialization は、経験的に良い性能を示すことが知られていたが、PDLT の criticality 理論により、これらが 数学的に最適な初期化 であることが証明された。Criticality 理論は、既存の実践的手法を統一的に理解し、新しい活性化関数やアーキテクチャに対する初期化手法を systematic に導出する枠組みを提供する。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#criticality-の可視化",
    "href": "ja/pdlt/03-criticality.html#criticality-の可視化",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality の効果を視覚的に理解するため、以下の diagram を用いる。\n\n\n┌──────────────────────────────────────────────────────────────┐\n│           Kernel Evolution Phase Diagram                     │\n├──────────────────────────────────────────────────────────────┤\n│                                                              │\n│  K^(l)                                                       │\n│   ^                                                          │\n│   |    Exploding Phase (C_W &gt; 1)                             │\n│   |   /                                                      │\n│   |  /                                                       │\n│   | /                                                        │\n│   |/  Critical Point (C_W = 1)                               │\n│   +─────────────────────────────────────&gt; Layer l            │\n│   |\\                                                         │\n│   | \\                                                        │\n│   |  \\                                                       │\n│   |   \\ Vanishing Phase (C_W &lt; 1)                            │\n│   |    v                                                     │\n│   |     0                                                    │\n│                                                              │\n└──────────────────────────────────────────────────────────────┘\n\nExploding phase (\\(C_W &gt; 1\\)): Kernel が exponentially grow し、\\(K^* = \\infty\\) に到達\nCritical point (\\(C_W = 1\\)): Kernel が一定値に保たれる（nontrivial fixed point）\nVanishing phase (\\(C_W &lt; 1\\)): Kernel が exponentially decay し、\\(K^* = 0\\) に到達\n\n\n\n\n┌─────────────────────────────────────────────────────────────────────────────────────────────┐\n│      Kernel Evolution across Universality Classes                                           │\n├─────────────────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                             │\n│  K^(l)                                                                                      │\n│   ^                                                                                         │\n│   |  Scale-Invariant Class (ReLU)                                                           │\n│   |  ════════════════════════════════ (constant)            │\n│   |                                                                                         │\n│   |  Half-Stable Class (GELU)                                                               │\n│   |  ─────────────.                   (power-law decay)                                     │\n│   |                \\                                                                        │\n│   |                 `─────────────────                                                      │\n│   |                                                                                         │\n│   |  K* = 0 Class (tanh)                                                                    │\n│   |  ───────                          (power-law to zero)                                   │\n│   |         `──                                                                             │\n│   |            `─────                                                                       │\n│   |                  `────────────────                                                      │\n│   +─────────────────────────────────────&gt; Layer l                                           │\n│                                                                                             │\n└─────────────────────────────────────────────────────────────────────────────────────────────┘\n異なる universality class は、critical initialization 下でも異なる kernel evolution を示す:\n\nScale-Invariant: Perfect preservation (constant)\nHalf-Stable: Power-law decay to nonzero fixed point\nK* = 0: Power-law decay to zero",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/03-criticality.html#まとめ",
    "href": "ja/pdlt/03-criticality.html#まとめ",
    "title": "Criticality: 臨界性",
    "section": "",
    "text": "Criticality は、深層ニューラルネットワークの初期化における最重要原理である。\n主要な概念:\n\nExploding/Vanishing Problem: 不適切な初期化により、信号が exponentially grow または decay する問題\nCritical Point: Exploding と vanishing の境界にある特別な点（\\(C_W = 1\\)）\nCriticality Conditions: \\(\\chi_\\parallel(K^*) = 1\\) と \\(\\chi_\\perp(K^*) = 1\\) の二つの条件\nUniversality Classes: 活性化関数の criticality 挙動による分類\n\n実践的意義:\n\nHe/Xavier Initialization の理論的基盤: 既存の実践的手法が数学的に最適であることの証明\n新しい活性化関数への適用: Criticality 理論により、任意の活性化関数に対する最適初期化を systematic に導出可能\nForward と Backward の統一: Kernel の安定性と gradient の安定性が同じ条件で達成される\n\n理論的意義:\n\n統計物理学との接続: Phase transition の理論を深層学習に応用\n有効理論の枠組み: Criticality は有効理論アプローチの中心的概念\n深層極限の理解: 深層ニューラルネットワークの asymptotic behavior の理解\n\nCriticality は、深層学習を「なぜ動くのか」から「どう設計すべきか」へと導く、理論と実践の橋渡しとなる概念である。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Criticality"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html",
    "href": "ja/pdlt/05-infinite-width-limit.html",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限（Infinite Width Limit） は、ニューラルネットワークの各層の幅 \\(n\\) を無限大に取る極限操作です。この極限下では、ネットワークの挙動が劇的に簡単化される一方で、深層学習の重要な性質の一部が失われます。\n\n\nニューラルネットワークの層 \\(\\ell\\) における幅 \\(n_\\ell\\) を無限大に取る極限:\n\\[\nn_\\ell \\to \\infty \\quad \\text{for all layers } \\ell\n\\]\nこの極限下で、ネットワークの出力分布は以下のようなシンプルな ガウス過程（Gaussian Process） に収束します:\n\\[\np\\left(z^{(L)}_D \\mid H\\right) = \\frac{1}{\\sqrt{|2\\pi K|^{n_L}}} \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^{n_L} \\sum_{\\delta_1, \\delta_2 \\in D} K^{\\delta_1 \\delta_2} z^{(L)}_{i;\\delta_1} z^{(L)}_{i;\\delta_2}\\right)\n\\]\nここで:\n\n\\(K_{\\delta_1 \\delta_2} = K^{(L)}(x_{\\delta_1}, x_{\\delta_2})\\) は カーネル（Kernel） で、入力ペア間の相関を表す\n\\(K^{\\delta_1 \\delta_2}\\) はカーネルの逆行列\n分布は平均ゼロのガウス分布\n\n\n\n\n無限幅極限下では、各ニューロンの出力が独立になり、中心極限定理により出力分布がガウス分布に収束します。このとき:\n\n出力成分間が独立: 異なるニューロン \\(i\\) と \\(j\\) の出力 \\(z^{(L)}_i\\) と \\(z^{(L)}_j\\) は独立\nカーネルのみで決まる: 分布の形はカーネル \\(K(x_{\\delta_1}, x_{\\delta_2})\\) のみで完全に決定される\n重みのランダム性が消える: 個々の重み \\(W^{(\\ell)}_{ij}\\) の影響は平均化される\n\n\n\n\n\n\n\nNoteGaussian Process と Neural Network Gaussian Process (NNGP)\n\n\n\n\n\n無限幅ニューラルネットワークは Neural Network Gaussian Process (NNGP) と呼ばれるガウス過程に収束します。このカーネル \\(K(x, x')\\) は、以下の再帰的な式で計算できます（ReLU 活性化の場合）:\n\\[\nK^{(\\ell+1)}(x, x') = \\sigma_W^2 \\mathbb{E}_{\\phi \\sim \\mathcal{N}(0, \\Sigma^{(\\ell)})} \\left[\\phi(z^{(\\ell)}(x)) \\phi(z^{(\\ell)}(x'))\\right] + \\sigma_b^2\n\\]\nここで \\(\\Sigma^{(\\ell)}\\) は層 \\(\\ell\\) の共分散行列です。この再帰式により、深層ネットワークのカーネルを層ごとに順次計算できます。\n\n\n\n\n\n\n無限幅極限のもう一つの重要な帰結は、Neural Tangent Kernel (NTK) が訓練中に 固定される ことです。\n\n\nNTK は、ネットワークの出力 \\(z^{(L)}_i(x)\\) のパラメータ \\(\\theta\\) に関する勾配の内積として定義されます:\n\\[\n\\Theta^{(L)}_{\\delta_1 \\delta_2} = \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\mathbb{E}\\left[\\frac{\\partial z^{(L)}_i(x_{\\delta_1})}{\\partial \\theta} \\cdot \\frac{\\partial z^{(L)}_i(x_{\\delta_2})}{\\partial \\theta}\\right]\n\\]\n\n\n\n無限幅極限では、訓練中に NTK が初期値 \\(\\Theta^{(L)}_{\\delta_1 \\delta_2}(t=0)\\) から変化しません:\n\\[\n\\Theta^{(L)}_{\\delta_1 \\delta_2}(t) = \\Theta^{(L)}_{\\delta_1 \\delta_2}(0) + O\\left(\\frac{1}{n}\\right)\n\\]\nこの固定化は以下を意味します:\n\n線形化された学習: ネットワークは初期化点周りの線形近似で学習\nカーネル法との等価性: 無限幅ネットワークは固定カーネルを使うカーネル法と等価\n特徴学習の欠如: NTK が変化しないため、新しい特徴を学習できない\n\n\n\n\n\n無限幅極限下では、表現学習（Representation Learning） が完全に失われます。これは第6.3.3節で示される重要な結果です。\n\n\n表現学習とは、隠れ層の表現 \\(z^{(\\ell)}_D\\) が訓練データ \\(y_A\\) の観測によって更新されることを指します。ベイズの定理により、事後分布を考えると:\n\\[\np\\left(z^{(L-1)}_D \\mid y_A\\right) = \\frac{p\\left(y_A \\mid z^{(L-1)}_D\\right) p\\left(z^{(L-1)}_D\\right)}{p(y_A)}\n\\]\n\n\n\n無限幅極限では、尤度が前の層の表現に依存しなくなります:\n\\[\np\\left(y_A \\mid z^{(L-1)}_D\\right) = p(y_A)\n\\]\nその結果、事後分布が事前分布と一致 します:\n\\[\np\\left(z^{(L-1)}_D \\mid y_A\\right) = p\\left(z^{(L-1)}_D\\right)\n\\]\nこれは以下を意味します:\n\n観測が無意味: 訓練データ \\(y_A\\) を観測しても、隠れ層の表現は更新されない\n層間相関の欠如: 異なる層 \\(z^{(\\ell)}_D\\) と \\(z^{(\\ell+1)}_D\\) の間に相関がない\n深層の意義が失われる: 深い層を重ねる主要な動機である複雑な表現学習が不可能\n\n\n\n\n\n\n\nImportantなぜ表現学習が失われるのか？\n\n\n\n無限幅極限では、各層のニューロンが完全に独立になるため、層間の情報伝達が平均値（カーネル）のみで行われます。個々のニューロンの活性パターンが隠れ層の表現を形成できないため、表現学習が起こりません。\nこの問題は、有限幅ネットワーク に戻ることで解決されます（第6.4節）。\n\n\n\n\n\n\n無限幅極限と有限幅ネットワークの主要な違いを表にまとめます:\n\n\n\nTable 1: 無限幅極限と有限幅ネットワークの比較\n\n\n\n\n\n性質\n無限幅極限\n有限幅\n\n\n\n\n出力分布\nガウス過程\n非ガウス分布\n\n\nNTK\n訓練中に固定\n訓練中に変化\n\n\n表現学習\nなし\nあり\n\n\n層間相関\nなし\nあり（\\(O(1/n)\\)）\n\n\n学習アルゴリズム\nカーネル法と等価\n真の特徴学習\n\n\n出力成分間の相関\n独立\n相関あり\n\n\n計算の複雑さ\n理論的に単純\n複雑だが実用的\n\n\n\n\n\n\nTable 1 に示すように、有限幅ネットワークは \\(O(1/n)\\) の相関により表現学習が可能になります。\n\n\n\nChapter 6.3 では、無限幅極限下でのベイズ学習について3つの重要な教訓を示しています:\n\n臨界性の証拠（§6.3.1）:\n\n証拠 \\(p(y_A \\mid H)\\) を計算し、ベイズモデル比較が臨界性を好むことを示す\n十分に深いネットワークでは、臨界条件 \\(\\chi_\\parallel(K^*) = 1\\) および \\(\\chi_\\perp(K^*) = 1\\) が最適\n\n独立な出力成分（§6.3.2）:\n\n事後分布 \\(p(z^{(L)}_B \\mid y_A)\\) を計算し、異なる出力成分が完全に独立であることを示す\nこれは “Let’s Not Wire Together”（配線されない）という状態を意味する\n\n表現学習の欠如（§6.3.3）:\n\n事後分布 \\(p(z^{(L-1)}_D \\mid y_A)\\) が事前分布 \\(p(z^{(L-1)}_D)\\) と一致することを示す\n隠れ層の表現が訓練データから学習されないことを証明\n\n\n\n\n\n\n\n\nNote臨界性とベイズ証拠\n\n\n\n\n\n単一入力の場合、証拠は以下のように書けます:\n\\[\np(y \\mid H) = \\frac{1}{\\sqrt{2\\pi K}^{n_L}} \\exp\\left(-\\frac{1}{2K} \\sum_{i=1}^{n_L} y_i^2\\right)\n\\]\n\n\\(K \\to \\infty\\) の場合: 証拠は多項式的にゼロに収束（分布が広すぎる）\n\\(K \\to 0\\) の場合: 証拠は指数関数的にゼロに収束（ゼロ出力のみ予測）\n\\(K = O(1)\\) の場合: 証拠が最大化される（臨界性が最適）\n\n二入力の場合、証拠を最大化するには \\(K^{[0]} \\approx Y^{[0]}/n_L = O(1)\\) かつ \\(K^{[2]} \\approx Y^{[2]}/n_L = O(1)\\) が必要で、これは並行感受率条件 \\(\\chi_\\parallel(K^*) = 1\\) と垂直感受率条件 \\(\\chi_\\perp(K^*) = 1\\) の両方を満たす臨界性に対応します。\n\n\n\n\n\n\n無限幅極限の研究は、以下の理由で重要です:\n\n理論的ベンチマーク: 実際の有限幅ネットワークの挙動を理解するための基準点\n幅の重要性: なぜ実用的なネットワークで適度な幅が必要かを説明\n初期化の設計: 臨界性条件は実際の初期化スキーム（He初期化など）の理論的根拠を提供\n特徴学習の必要性: カーネル法を超えた深層学習の本質的価値を明確化\n\n実際の深層学習では、有限幅ネットワーク を使用することで無限幅極限の制約を回避し、真の表現学習と特徴学習を実現します。第6.4節と第11章では、有限幅効果がどのように表現学習を可能にするかが詳述されます。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#無限幅極限の定義",
    "href": "ja/pdlt/05-infinite-width-limit.html#無限幅極限の定義",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "ニューラルネットワークの層 \\(\\ell\\) における幅 \\(n_\\ell\\) を無限大に取る極限:\n\\[\nn_\\ell \\to \\infty \\quad \\text{for all layers } \\ell\n\\]\nこの極限下で、ネットワークの出力分布は以下のようなシンプルな ガウス過程（Gaussian Process） に収束します:\n\\[\np\\left(z^{(L)}_D \\mid H\\right) = \\frac{1}{\\sqrt{|2\\pi K|^{n_L}}} \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^{n_L} \\sum_{\\delta_1, \\delta_2 \\in D} K^{\\delta_1 \\delta_2} z^{(L)}_{i;\\delta_1} z^{(L)}_{i;\\delta_2}\\right)\n\\]\nここで:\n\n\\(K_{\\delta_1 \\delta_2} = K^{(L)}(x_{\\delta_1}, x_{\\delta_2})\\) は カーネル（Kernel） で、入力ペア間の相関を表す\n\\(K^{\\delta_1 \\delta_2}\\) はカーネルの逆行列\n分布は平均ゼロのガウス分布",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#gaussian-process-への収束",
    "href": "ja/pdlt/05-infinite-width-limit.html#gaussian-process-への収束",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限下では、各ニューロンの出力が独立になり、中心極限定理により出力分布がガウス分布に収束します。このとき:\n\n出力成分間が独立: 異なるニューロン \\(i\\) と \\(j\\) の出力 \\(z^{(L)}_i\\) と \\(z^{(L)}_j\\) は独立\nカーネルのみで決まる: 分布の形はカーネル \\(K(x_{\\delta_1}, x_{\\delta_2})\\) のみで完全に決定される\n重みのランダム性が消える: 個々の重み \\(W^{(\\ell)}_{ij}\\) の影響は平均化される\n\n\n\n\n\n\n\nNoteGaussian Process と Neural Network Gaussian Process (NNGP)\n\n\n\n\n\n無限幅ニューラルネットワークは Neural Network Gaussian Process (NNGP) と呼ばれるガウス過程に収束します。このカーネル \\(K(x, x')\\) は、以下の再帰的な式で計算できます（ReLU 活性化の場合）:\n\\[\nK^{(\\ell+1)}(x, x') = \\sigma_W^2 \\mathbb{E}_{\\phi \\sim \\mathcal{N}(0, \\Sigma^{(\\ell)})} \\left[\\phi(z^{(\\ell)}(x)) \\phi(z^{(\\ell)}(x'))\\right] + \\sigma_b^2\n\\]\nここで \\(\\Sigma^{(\\ell)}\\) は層 \\(\\ell\\) の共分散行列です。この再帰式により、深層ネットワークのカーネルを層ごとに順次計算できます。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#ntk-の固定化",
    "href": "ja/pdlt/05-infinite-width-limit.html#ntk-の固定化",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限のもう一つの重要な帰結は、Neural Tangent Kernel (NTK) が訓練中に 固定される ことです。\n\n\nNTK は、ネットワークの出力 \\(z^{(L)}_i(x)\\) のパラメータ \\(\\theta\\) に関する勾配の内積として定義されます:\n\\[\n\\Theta^{(L)}_{\\delta_1 \\delta_2} = \\frac{1}{n_L} \\sum_{i=1}^{n_L} \\mathbb{E}\\left[\\frac{\\partial z^{(L)}_i(x_{\\delta_1})}{\\partial \\theta} \\cdot \\frac{\\partial z^{(L)}_i(x_{\\delta_2})}{\\partial \\theta}\\right]\n\\]\n\n\n\n無限幅極限では、訓練中に NTK が初期値 \\(\\Theta^{(L)}_{\\delta_1 \\delta_2}(t=0)\\) から変化しません:\n\\[\n\\Theta^{(L)}_{\\delta_1 \\delta_2}(t) = \\Theta^{(L)}_{\\delta_1 \\delta_2}(0) + O\\left(\\frac{1}{n}\\right)\n\\]\nこの固定化は以下を意味します:\n\n線形化された学習: ネットワークは初期化点周りの線形近似で学習\nカーネル法との等価性: 無限幅ネットワークは固定カーネルを使うカーネル法と等価\n特徴学習の欠如: NTK が変化しないため、新しい特徴を学習できない",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#表現学習が起こらない理由",
    "href": "ja/pdlt/05-infinite-width-limit.html#表現学習が起こらない理由",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限下では、表現学習（Representation Learning） が完全に失われます。これは第6.3.3節で示される重要な結果です。\n\n\n表現学習とは、隠れ層の表現 \\(z^{(\\ell)}_D\\) が訓練データ \\(y_A\\) の観測によって更新されることを指します。ベイズの定理により、事後分布を考えると:\n\\[\np\\left(z^{(L-1)}_D \\mid y_A\\right) = \\frac{p\\left(y_A \\mid z^{(L-1)}_D\\right) p\\left(z^{(L-1)}_D\\right)}{p(y_A)}\n\\]\n\n\n\n無限幅極限では、尤度が前の層の表現に依存しなくなります:\n\\[\np\\left(y_A \\mid z^{(L-1)}_D\\right) = p(y_A)\n\\]\nその結果、事後分布が事前分布と一致 します:\n\\[\np\\left(z^{(L-1)}_D \\mid y_A\\right) = p\\left(z^{(L-1)}_D\\right)\n\\]\nこれは以下を意味します:\n\n観測が無意味: 訓練データ \\(y_A\\) を観測しても、隠れ層の表現は更新されない\n層間相関の欠如: 異なる層 \\(z^{(\\ell)}_D\\) と \\(z^{(\\ell+1)}_D\\) の間に相関がない\n深層の意義が失われる: 深い層を重ねる主要な動機である複雑な表現学習が不可能\n\n\n\n\n\n\n\nImportantなぜ表現学習が失われるのか？\n\n\n\n無限幅極限では、各層のニューロンが完全に独立になるため、層間の情報伝達が平均値（カーネル）のみで行われます。個々のニューロンの活性パターンが隠れ層の表現を形成できないため、表現学習が起こりません。\nこの問題は、有限幅ネットワーク に戻ることで解決されます（第6.4節）。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#有限幅との対比",
    "href": "ja/pdlt/05-infinite-width-limit.html#有限幅との対比",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限と有限幅ネットワークの主要な違いを表にまとめます:\n\n\n\nTable 1: 無限幅極限と有限幅ネットワークの比較\n\n\n\n\n\n性質\n無限幅極限\n有限幅\n\n\n\n\n出力分布\nガウス過程\n非ガウス分布\n\n\nNTK\n訓練中に固定\n訓練中に変化\n\n\n表現学習\nなし\nあり\n\n\n層間相関\nなし\nあり（\\(O(1/n)\\)）\n\n\n学習アルゴリズム\nカーネル法と等価\n真の特徴学習\n\n\n出力成分間の相関\n独立\n相関あり\n\n\n計算の複雑さ\n理論的に単純\n複雑だが実用的\n\n\n\n\n\n\nTable 1 に示すように、有限幅ネットワークは \\(O(1/n)\\) の相関により表現学習が可能になります。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#chapter-6.3-の要点",
    "href": "ja/pdlt/05-infinite-width-limit.html#chapter-6.3-の要点",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "Chapter 6.3 では、無限幅極限下でのベイズ学習について3つの重要な教訓を示しています:\n\n臨界性の証拠（§6.3.1）:\n\n証拠 \\(p(y_A \\mid H)\\) を計算し、ベイズモデル比較が臨界性を好むことを示す\n十分に深いネットワークでは、臨界条件 \\(\\chi_\\parallel(K^*) = 1\\) および \\(\\chi_\\perp(K^*) = 1\\) が最適\n\n独立な出力成分（§6.3.2）:\n\n事後分布 \\(p(z^{(L)}_B \\mid y_A)\\) を計算し、異なる出力成分が完全に独立であることを示す\nこれは “Let’s Not Wire Together”（配線されない）という状態を意味する\n\n表現学習の欠如（§6.3.3）:\n\n事後分布 \\(p(z^{(L-1)}_D \\mid y_A)\\) が事前分布 \\(p(z^{(L-1)}_D)\\) と一致することを示す\n隠れ層の表現が訓練データから学習されないことを証明\n\n\n\n\n\n\n\n\nNote臨界性とベイズ証拠\n\n\n\n\n\n単一入力の場合、証拠は以下のように書けます:\n\\[\np(y \\mid H) = \\frac{1}{\\sqrt{2\\pi K}^{n_L}} \\exp\\left(-\\frac{1}{2K} \\sum_{i=1}^{n_L} y_i^2\\right)\n\\]\n\n\\(K \\to \\infty\\) の場合: 証拠は多項式的にゼロに収束（分布が広すぎる）\n\\(K \\to 0\\) の場合: 証拠は指数関数的にゼロに収束（ゼロ出力のみ予測）\n\\(K = O(1)\\) の場合: 証拠が最大化される（臨界性が最適）\n\n二入力の場合、証拠を最大化するには \\(K^{[0]} \\approx Y^{[0]}/n_L = O(1)\\) かつ \\(K^{[2]} \\approx Y^{[2]}/n_L = O(1)\\) が必要で、これは並行感受率条件 \\(\\chi_\\parallel(K^*) = 1\\) と垂直感受率条件 \\(\\chi_\\perp(K^*) = 1\\) の両方を満たす臨界性に対応します。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/05-infinite-width-limit.html#実用的な含意",
    "href": "ja/pdlt/05-infinite-width-limit.html#実用的な含意",
    "title": "Infinite Width Limit",
    "section": "",
    "text": "無限幅極限の研究は、以下の理由で重要です:\n\n理論的ベンチマーク: 実際の有限幅ネットワークの挙動を理解するための基準点\n幅の重要性: なぜ実用的なネットワークで適度な幅が必要かを説明\n初期化の設計: 臨界性条件は実際の初期化スキーム（He初期化など）の理論的根拠を提供\n特徴学習の必要性: カーネル法を超えた深層学習の本質的価値を明確化\n\n実際の深層学習では、有限幅ネットワーク を使用することで無限幅極限の制約を回避し、真の表現学習と特徴学習を実現します。第6.4節と第11章では、有限幅効果がどのように表現学習を可能にするかが詳述されます。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Infinite Width Limit"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html",
    "href": "ja/pdlt/07-universality-classes.html",
    "title": "Universality Classes",
    "section": "",
    "text": "普遍性クラス（Universality Classes）は、異なる活性化関数が深層極限において示す共通の統計的挙動を分類する概念である。PDLT の Chapter 5.2, 5.3 で詳述されており、活性化関数の選択が深層ニューラルネットワークの初期化時の挙動に与える影響を理解する上で重要である。\n物理学における臨界現象の理論では、異なる物理系が renormalization group (RG) flow の下で同じ fixed point に収束する場合、それらは同じ普遍性クラスに属するとされる。PDLT はこの概念を深層学習に適用し、異なる活性化関数が representation group flow（層を通る信号の伝播）において同様の挙動を示す場合、それらを同じ普遍性クラスに分類する。\n\n\n\n\n\n\nNote物理学における普遍性との比較\n\n\n\n\n\n物理学では、臨界点近傍の系の挙動は microscopic な詳細に依存せず、少数の普遍的な量（critical exponents など）によって特徴づけられる。例えば、水の液体-気体相転移と磁性体の相転移は、異なる物理系であるにもかかわらず、同じ Ising 普遍性クラスに属する。\n深層学習における普遍性クラスも同様に、活性化関数の詳細によらず、深層極限での kernel の挙動や critical exponents によって特徴づけられる。この類似性により、物理学の RG flow の手法を深層学習に適用できる。\n\n\n\n\n\n\nPDLT では、主に以下の3つの普遍性クラスが識別されている：\n\nScale-Invariant Universality Class (ReLU, leaky ReLU など)\nK* = 0 Universality Class (tanh, sin など)\nHalf-Stable Universality Classes (SWISH, GELU など)\n\nそれぞれのクラスは、臨界点における kernel の fixed point 値 \\(K^*_{00}\\) と、その fixed point 周りでの挙動によって特徴づけられる。\n\n\nすべての普遍性クラスに共通する臨界性の条件は、parallel susceptibility と perpendicular susceptibility が以下を満たすことである：\n\\[\n\\chi_\\parallel(K^*_{00}) = \\chi_\\perp(K^*_{00}) = 1\n\\]\nここで、susceptibility は kernel の recursion における摂動の増幅率を表す。\\(\\chi &gt; 1\\) なら信号が explode し、\\(\\chi &lt; 1\\) なら vanish する。臨界点 \\(\\chi = 1\\) では、信号が深層を通じて保存される。\n\n\n\n\n\n\nScale-invariant activation は、任意の正のスケーリング \\(\\lambda &gt; 0\\) に対して以下を満たす：\n\\[\n\\sigma(\\lambda z) = \\lambda \\sigma(z)\n\\]\nこの条件を満たす活性化関数は、以下の piecewise linear 形式をとる：\n\\[\n\\sigma(z) = \\begin{cases}\na_+ z, & z \\geq 0 \\\\\na_- z, & z &lt; 0\n\\end{cases}\n\\]\n代表的な例：\n\nLinear activation: \\(a_+ = a_- = 1\\)\nReLU: \\(a_+ = 1, a_- = 0\\)\nLeaky ReLU: \\(a_+ = 1, a_- = 0.01\\) (など)\n\n\n\n\nScale-invariant activation の場合、susceptibility は kernel 値によらず一定となる：\n\\[\n\\chi_\\parallel(K) = \\chi_\\perp(K) = A^2 C_W \\equiv \\chi\n\\]\nここで、\\(A^2 = (a_+^2 + a_-^2)/2\\) は活性化関数依存の定数、\\(C_W\\) は weight variance の初期化 hyperparameter である。\n臨界的な初期化は以下で与えられる：\n\\[\n(C_b, C_W)_{\\text{critical}} = \\left(0, \\frac{1}{A^2}\\right)\n\\]\nReLU の場合（\\(A^2 = 1/2\\)）、これは Kaiming initialization \\((C_b, C_W) = (0, 2)\\) に対応する。\n\n\n\n臨界初期化において、kernel recursion は以下のように単純化される：\n\\[\nK^{(\\ell+1)}_{00} = C_b + \\chi K^{(\\ell)}_{00}\n\\]\n\\(\\chi = 1\\) かつ \\(C_b = 0\\) のとき、\\(K^{(\\ell)}_{00}\\) は一定に保たれる（line of fixed points）。ただし、非線形 scale-invariant activation の場合、perpendicular perturbations は power law で減衰する（\\(\\sim 1/\\ell^2\\)）。\n\n\n\n\n\n\nImportant実践的な意味\n\n\n\nScale-invariant universality class に属する活性化関数は、深層ネットワークで最も安定した挙動を示す。特に ReLU は、実装の単純さと臨界性の保ちやすさから、深層学習で広く採用されている。\n\n\n\n\n\n\n\n\nこのクラスに属する活性化関数は、原点で zero を通り、非ゼロの微分を持つ：\n\\[\n\\sigma(0) = 0, \\quad \\sigma'(0) \\neq 0\n\\]\nTaylor 展開係数を \\(\\sigma(z) = \\sum_{p=0}^\\infty \\frac{\\sigma_p}{p!} z^p\\) とすると、条件は \\(\\sigma_0 = 0, \\sigma_1 \\neq 0\\) となる。\n代表的な例：\n\ntanh\nsin\nその他、原点で線形な奇関数\n\n\n\n\nこのクラスの臨界的な初期化は以下で与えられる：\n\\[\n(C_b, C_W)_{\\text{critical}} = \\left(0, \\frac{1}{\\sigma_1^2}\\right)\n\\]\nnontrivial fixed point は \\(K^*_{00} = 0\\) にあり、kernel は power law で減衰する：\n\\[\n\\Delta K^{(\\ell)}_{00} = K^{(\\ell)}_{00} - K^*_{00} \\sim \\frac{1}{(-a_1)} \\frac{1}{\\ell}\n\\]\nここで、\\(a_1 = \\sigma_3/\\sigma_1 + \\frac{3}{4}(\\sigma_2/\\sigma_1)^2\\) は活性化関数の Taylor 係数から決まる定数である。安定性のためには \\((-a_1) &gt; 0\\) が必要。\n\n\n\nK* = 0 universality class は、以下の universal critical exponents によって特徴づけられる：\n\nMidpoint kernel: \\(p_0 = 1\\) （\\(\\Delta K^{(\\ell)}_{00} \\sim 1/\\ell\\)）\nParallel perturbations: \\(p_\\parallel = 2\\) （\\(\\delta K^{(\\ell)}_{[1]} \\sim 1/\\ell^2\\)）\nPerpendicular perturbations: \\(p_\\perp = b_1/a_1\\) （活性化関数依存）\n\n奇関数の場合、\\(a_1 = b_1\\) となり、\\(p_\\perp = 1\\) となる。これは、入力間の角度が深層を通じて保存されることを意味する。\n\n\n\n\n\n\nTip実践的な意味\n\n\n\ntanh などの K* = 0 class の活性化関数は、深層極限で kernel が power law で減衰するため、ReLU よりも signal の保存性が劣る。ただし、指数減衰よりは遥かに穏やかであり、中程度の深さのネットワークでは実用的である。\n\n\n\n\n\n\n\n\nSWISH は以下で定義される：\n\\[\n\\sigma(z) = \\frac{z}{1 + e^{-z}}\n\\]\nSWISH は、ReLU の smooth approximation として設計されたが、2つの nontrivial fixed point を持つ：\n\n\\(K^*_{00} = 0\\) with \\((C_b, C_W) = (0, 4)\\) → 不安定 (unstable)\n\\(K^*_{00} \\approx 14.32\\) with \\((C_b, C_W) \\approx (0.555, 1.988)\\) → 半安定 (half-stable)\n\n半安定 fixed point では：\n\n\\(K^{(\\ell)}_{00} &lt; K^*_{00}\\) のとき、kernel は fixed point に引き寄せられる\n\\(K^{(\\ell)}_{00} &gt; K^*_{00}\\) のとき、kernel は fixed point から反発される\n\n\\(K^*_{00} \\approx 14.3\\) 周りで、SWISH はほぼ scale-invariant に振る舞う。\n\n\n\nGELU (Gaussian Error Linear Unit) は以下で定義される：\n\\[\n\\sigma(z) = \\frac{z}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)\n\\]\nGELU も2つの nontrivial fixed point を持つ：\n\n\\(K^*_{00} = 0\\) with \\((C_b, C_W) = (0, 4)\\) → 不安定\n\\(K^*_{00} = \\frac{3 + \\sqrt{17}}{2}\\) with \\((C_b, C_W) \\approx (0.173, 1.983)\\) → 半安定\n\nただし、SWISH とは逆の安定性を示す：\n\n\\(K^{(\\ell)}_{00} &gt; K^*_{00}\\) のとき、kernel は fixed point に引き寄せられる\n\\(K^{(\\ell)}_{00} &lt; K^*_{00}\\) のとき、kernel は fixed point から反発される\n\n\n\n\n\n\n\nWarning実践的な意味\n\n\n\nSWISH と GELU は、ReLU の smooth 版として人気があるが、理論的には ReLU より劣る。半安定 fixed point の存在は、scale invariance の破れを示唆しており、特定の kernel 値に依存する振る舞いをもたらす。\nもし smooth activation を使いたい場合、tanh の方が理論的に健全である（K* = 0 class の安定な fixed point を持つ）。\n\n\n\n\n\n\n以下の活性化関数は、nontrivial critical fixed point を持たないため、深層ネットワークには不適切である：\n\n\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\n\\(\\sigma(0) = 0.5 \\neq 0\\) であるため、\\(K^*_{00} = 0\\) での criticality 条件を満たすには \\(C_b &lt; 0\\) が必要となり、物理的に不可能。\n\n\n\n\\[\n\\sigma(z) = \\log(1 + e^z)\n\\]\ncriticality 条件 \\(\\chi_\\perp(K^*_{00})/\\chi_\\parallel(K^*_{00}) = 1\\) をどの \\(K^*_{00} \\geq 0\\) でも満たせない。\n\n\n\n\\[\n\\sigma(z) = z^p, \\quad p = 2, 3, 4, \\ldots\n\\]\ncriticality 条件は \\(p/(2p-1) = 1\\) となり、\\(p = 1\\)（線形）以外では満たせない。\n\n\n\n\n\n\nCaution深層ネットワークでの影響\n\n\n\nこれらの活性化関数を使うと、kernel が exponentially explode または vanish し、gradient の exploding/vanishing problem を引き起こす。浅いネットワークでは問題が顕在化しにくいが、深層化するほど致命的となる。\n\n\n\n\n\n\n\n\n\nTable 1: 主要な活性化関数の普遍性クラス\n\n\n\n\n\n\n\n\n\n\n\n\n\n普遍性クラス\n代表例\nFixed Point\nCritical Exponent \\(p_0\\)\n臨界初期化\n実用性\n\n\n\n\nScale-Invariant\nReLU, Leaky ReLU\nLine of fixed points\nN/A (constant)\n\\((0, 1/A^2)\\)\n最高\n\n\nK* = 0\ntanh, sin\n\\(K^*_{00} = 0\\)\n\\(p_0 = 1\\)\n\\((0, 1/\\sigma_1^2)\\)\n良好\n\n\nHalf-Stable (SWISH)\nSWISH\n\\(K^*_{00} \\approx 14.3\\)\n\\(p_0 = 1\\)\n\\((0.555, 1.988)\\)\nやや劣る\n\n\nHalf-Stable (GELU)\nGELU\n\\(K^*_{00} = (3+\\sqrt{17})/2\\)\n\\(p_0 = 1\\)\n\\((0.173, 1.983)\\)\nやや劣る\n\n\nNo Criticality\nsigmoid, softplus, \\(z^p\\)\nなし\nN/A\nN/A\n不適切\n\n\n\n\n\n\n\n\n\n\n\n\n深層ネットワーク（50層以上）: ReLU またはその variants（Leaky ReLU, PReLU）\n\nScale invariance による安定性\n計算効率の高さ\n\n中程度の深さ（10-50層）: tanh も選択肢\n\nK* = 0 class の power law 減衰は実用的\nSmooth な微分が必要な場合に有用\n\n避けるべき: sigmoid, softplus\n\nCriticality を達成できない\n深層化で exponential behavior を引き起こす\n\nSWISH/GELU の利用: 注意が必要\n\n実践では良好な性能を示すこともある\n理論的には ReLU/tanh に劣る\n特定のタスクで実験的に検証する価値はある\n\n\n\n\n\n普遍性クラスの理論は、適切な初期化の重要性を強調している：\n\nReLU: He initialization \\((C_b, C_W) = (0, 2)\\)\ntanh: Xavier/Glorot initialization に近い \\((C_b, C_W) = (0, 1)\\)\n\nこれらの初期化により、訓練開始時に kernel が critical fixed point に保たれる。\n\n\n\n\n普遍性クラスの理論により、以下が明らかになった：\n\n活性化関数の選択は深層極限で重要: 浅いネットワークでは差が小さいが、深層化すると普遍性クラスの違いが顕著になる\nScale-invariant class が最も安定: ReLU が深層学習で支配的な理由の理論的根拠\nCriticality は必須: 適切な初期化により、signal の exploding/vanishing を防ぐ\nUniversality の力: 異なる活性化関数が深層極限で同様の挙動を示すことで、理論的予測が可能になる\n\nPDLT の普遍性クラスの分析は、活性化関数とアーキテクチャ設計の理論的基盤を提供し、経験的知見（ReLU の優位性など）に物理学的な説明を与えている。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#概要",
    "href": "ja/pdlt/07-universality-classes.html#概要",
    "title": "Universality Classes",
    "section": "",
    "text": "普遍性クラス（Universality Classes）は、異なる活性化関数が深層極限において示す共通の統計的挙動を分類する概念である。PDLT の Chapter 5.2, 5.3 で詳述されており、活性化関数の選択が深層ニューラルネットワークの初期化時の挙動に与える影響を理解する上で重要である。\n物理学における臨界現象の理論では、異なる物理系が renormalization group (RG) flow の下で同じ fixed point に収束する場合、それらは同じ普遍性クラスに属するとされる。PDLT はこの概念を深層学習に適用し、異なる活性化関数が representation group flow（層を通る信号の伝播）において同様の挙動を示す場合、それらを同じ普遍性クラスに分類する。\n\n\n\n\n\n\nNote物理学における普遍性との比較\n\n\n\n\n\n物理学では、臨界点近傍の系の挙動は microscopic な詳細に依存せず、少数の普遍的な量（critical exponents など）によって特徴づけられる。例えば、水の液体-気体相転移と磁性体の相転移は、異なる物理系であるにもかかわらず、同じ Ising 普遍性クラスに属する。\n深層学習における普遍性クラスも同様に、活性化関数の詳細によらず、深層極限での kernel の挙動や critical exponents によって特徴づけられる。この類似性により、物理学の RG flow の手法を深層学習に適用できる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#普遍性クラスの分類",
    "href": "ja/pdlt/07-universality-classes.html#普遍性クラスの分類",
    "title": "Universality Classes",
    "section": "",
    "text": "PDLT では、主に以下の3つの普遍性クラスが識別されている：\n\nScale-Invariant Universality Class (ReLU, leaky ReLU など)\nK* = 0 Universality Class (tanh, sin など)\nHalf-Stable Universality Classes (SWISH, GELU など)\n\nそれぞれのクラスは、臨界点における kernel の fixed point 値 \\(K^*_{00}\\) と、その fixed point 周りでの挙動によって特徴づけられる。\n\n\nすべての普遍性クラスに共通する臨界性の条件は、parallel susceptibility と perpendicular susceptibility が以下を満たすことである：\n\\[\n\\chi_\\parallel(K^*_{00}) = \\chi_\\perp(K^*_{00}) = 1\n\\]\nここで、susceptibility は kernel の recursion における摂動の増幅率を表す。\\(\\chi &gt; 1\\) なら信号が explode し、\\(\\chi &lt; 1\\) なら vanish する。臨界点 \\(\\chi = 1\\) では、信号が深層を通じて保存される。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#scale-invariant-universality-class",
    "href": "ja/pdlt/07-universality-classes.html#scale-invariant-universality-class",
    "title": "Universality Classes",
    "section": "",
    "text": "Scale-invariant activation は、任意の正のスケーリング \\(\\lambda &gt; 0\\) に対して以下を満たす：\n\\[\n\\sigma(\\lambda z) = \\lambda \\sigma(z)\n\\]\nこの条件を満たす活性化関数は、以下の piecewise linear 形式をとる：\n\\[\n\\sigma(z) = \\begin{cases}\na_+ z, & z \\geq 0 \\\\\na_- z, & z &lt; 0\n\\end{cases}\n\\]\n代表的な例：\n\nLinear activation: \\(a_+ = a_- = 1\\)\nReLU: \\(a_+ = 1, a_- = 0\\)\nLeaky ReLU: \\(a_+ = 1, a_- = 0.01\\) (など)\n\n\n\n\nScale-invariant activation の場合、susceptibility は kernel 値によらず一定となる：\n\\[\n\\chi_\\parallel(K) = \\chi_\\perp(K) = A^2 C_W \\equiv \\chi\n\\]\nここで、\\(A^2 = (a_+^2 + a_-^2)/2\\) は活性化関数依存の定数、\\(C_W\\) は weight variance の初期化 hyperparameter である。\n臨界的な初期化は以下で与えられる：\n\\[\n(C_b, C_W)_{\\text{critical}} = \\left(0, \\frac{1}{A^2}\\right)\n\\]\nReLU の場合（\\(A^2 = 1/2\\)）、これは Kaiming initialization \\((C_b, C_W) = (0, 2)\\) に対応する。\n\n\n\n臨界初期化において、kernel recursion は以下のように単純化される：\n\\[\nK^{(\\ell+1)}_{00} = C_b + \\chi K^{(\\ell)}_{00}\n\\]\n\\(\\chi = 1\\) かつ \\(C_b = 0\\) のとき、\\(K^{(\\ell)}_{00}\\) は一定に保たれる（line of fixed points）。ただし、非線形 scale-invariant activation の場合、perpendicular perturbations は power law で減衰する（\\(\\sim 1/\\ell^2\\)）。\n\n\n\n\n\n\nImportant実践的な意味\n\n\n\nScale-invariant universality class に属する活性化関数は、深層ネットワークで最も安定した挙動を示す。特に ReLU は、実装の単純さと臨界性の保ちやすさから、深層学習で広く採用されている。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#k-0-universality-class",
    "href": "ja/pdlt/07-universality-classes.html#k-0-universality-class",
    "title": "Universality Classes",
    "section": "",
    "text": "このクラスに属する活性化関数は、原点で zero を通り、非ゼロの微分を持つ：\n\\[\n\\sigma(0) = 0, \\quad \\sigma'(0) \\neq 0\n\\]\nTaylor 展開係数を \\(\\sigma(z) = \\sum_{p=0}^\\infty \\frac{\\sigma_p}{p!} z^p\\) とすると、条件は \\(\\sigma_0 = 0, \\sigma_1 \\neq 0\\) となる。\n代表的な例：\n\ntanh\nsin\nその他、原点で線形な奇関数\n\n\n\n\nこのクラスの臨界的な初期化は以下で与えられる：\n\\[\n(C_b, C_W)_{\\text{critical}} = \\left(0, \\frac{1}{\\sigma_1^2}\\right)\n\\]\nnontrivial fixed point は \\(K^*_{00} = 0\\) にあり、kernel は power law で減衰する：\n\\[\n\\Delta K^{(\\ell)}_{00} = K^{(\\ell)}_{00} - K^*_{00} \\sim \\frac{1}{(-a_1)} \\frac{1}{\\ell}\n\\]\nここで、\\(a_1 = \\sigma_3/\\sigma_1 + \\frac{3}{4}(\\sigma_2/\\sigma_1)^2\\) は活性化関数の Taylor 係数から決まる定数である。安定性のためには \\((-a_1) &gt; 0\\) が必要。\n\n\n\nK* = 0 universality class は、以下の universal critical exponents によって特徴づけられる：\n\nMidpoint kernel: \\(p_0 = 1\\) （\\(\\Delta K^{(\\ell)}_{00} \\sim 1/\\ell\\)）\nParallel perturbations: \\(p_\\parallel = 2\\) （\\(\\delta K^{(\\ell)}_{[1]} \\sim 1/\\ell^2\\)）\nPerpendicular perturbations: \\(p_\\perp = b_1/a_1\\) （活性化関数依存）\n\n奇関数の場合、\\(a_1 = b_1\\) となり、\\(p_\\perp = 1\\) となる。これは、入力間の角度が深層を通じて保存されることを意味する。\n\n\n\n\n\n\nTip実践的な意味\n\n\n\ntanh などの K* = 0 class の活性化関数は、深層極限で kernel が power law で減衰するため、ReLU よりも signal の保存性が劣る。ただし、指数減衰よりは遥かに穏やかであり、中程度の深さのネットワークでは実用的である。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#half-stable-universality-classes",
    "href": "ja/pdlt/07-universality-classes.html#half-stable-universality-classes",
    "title": "Universality Classes",
    "section": "",
    "text": "SWISH は以下で定義される：\n\\[\n\\sigma(z) = \\frac{z}{1 + e^{-z}}\n\\]\nSWISH は、ReLU の smooth approximation として設計されたが、2つの nontrivial fixed point を持つ：\n\n\\(K^*_{00} = 0\\) with \\((C_b, C_W) = (0, 4)\\) → 不安定 (unstable)\n\\(K^*_{00} \\approx 14.32\\) with \\((C_b, C_W) \\approx (0.555, 1.988)\\) → 半安定 (half-stable)\n\n半安定 fixed point では：\n\n\\(K^{(\\ell)}_{00} &lt; K^*_{00}\\) のとき、kernel は fixed point に引き寄せられる\n\\(K^{(\\ell)}_{00} &gt; K^*_{00}\\) のとき、kernel は fixed point から反発される\n\n\\(K^*_{00} \\approx 14.3\\) 周りで、SWISH はほぼ scale-invariant に振る舞う。\n\n\n\nGELU (Gaussian Error Linear Unit) は以下で定義される：\n\\[\n\\sigma(z) = \\frac{z}{2}\\left(1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right)\n\\]\nGELU も2つの nontrivial fixed point を持つ：\n\n\\(K^*_{00} = 0\\) with \\((C_b, C_W) = (0, 4)\\) → 不安定\n\\(K^*_{00} = \\frac{3 + \\sqrt{17}}{2}\\) with \\((C_b, C_W) \\approx (0.173, 1.983)\\) → 半安定\n\nただし、SWISH とは逆の安定性を示す：\n\n\\(K^{(\\ell)}_{00} &gt; K^*_{00}\\) のとき、kernel は fixed point に引き寄せられる\n\\(K^{(\\ell)}_{00} &lt; K^*_{00}\\) のとき、kernel は fixed point から反発される\n\n\n\n\n\n\n\nWarning実践的な意味\n\n\n\nSWISH と GELU は、ReLU の smooth 版として人気があるが、理論的には ReLU より劣る。半安定 fixed point の存在は、scale invariance の破れを示唆しており、特定の kernel 値に依存する振る舞いをもたらす。\nもし smooth activation を使いたい場合、tanh の方が理論的に健全である（K* = 0 class の安定な fixed point を持つ）。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#使うべきでない活性化関数",
    "href": "ja/pdlt/07-universality-classes.html#使うべきでない活性化関数",
    "title": "Universality Classes",
    "section": "",
    "text": "以下の活性化関数は、nontrivial critical fixed point を持たないため、深層ネットワークには不適切である：\n\n\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\n\\(\\sigma(0) = 0.5 \\neq 0\\) であるため、\\(K^*_{00} = 0\\) での criticality 条件を満たすには \\(C_b &lt; 0\\) が必要となり、物理的に不可能。\n\n\n\n\\[\n\\sigma(z) = \\log(1 + e^z)\n\\]\ncriticality 条件 \\(\\chi_\\perp(K^*_{00})/\\chi_\\parallel(K^*_{00}) = 1\\) をどの \\(K^*_{00} \\geq 0\\) でも満たせない。\n\n\n\n\\[\n\\sigma(z) = z^p, \\quad p = 2, 3, 4, \\ldots\n\\]\ncriticality 条件は \\(p/(2p-1) = 1\\) となり、\\(p = 1\\)（線形）以外では満たせない。\n\n\n\n\n\n\nCaution深層ネットワークでの影響\n\n\n\nこれらの活性化関数を使うと、kernel が exponentially explode または vanish し、gradient の exploding/vanishing problem を引き起こす。浅いネットワークでは問題が顕在化しにくいが、深層化するほど致命的となる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#普遍性クラスの比較表",
    "href": "ja/pdlt/07-universality-classes.html#普遍性クラスの比較表",
    "title": "Universality Classes",
    "section": "",
    "text": "Table 1: 主要な活性化関数の普遍性クラス\n\n\n\n\n\n\n\n\n\n\n\n\n\n普遍性クラス\n代表例\nFixed Point\nCritical Exponent \\(p_0\\)\n臨界初期化\n実用性\n\n\n\n\nScale-Invariant\nReLU, Leaky ReLU\nLine of fixed points\nN/A (constant)\n\\((0, 1/A^2)\\)\n最高\n\n\nK* = 0\ntanh, sin\n\\(K^*_{00} = 0\\)\n\\(p_0 = 1\\)\n\\((0, 1/\\sigma_1^2)\\)\n良好\n\n\nHalf-Stable (SWISH)\nSWISH\n\\(K^*_{00} \\approx 14.3\\)\n\\(p_0 = 1\\)\n\\((0.555, 1.988)\\)\nやや劣る\n\n\nHalf-Stable (GELU)\nGELU\n\\(K^*_{00} = (3+\\sqrt{17})/2\\)\n\\(p_0 = 1\\)\n\\((0.173, 1.983)\\)\nやや劣る\n\n\nNo Criticality\nsigmoid, softplus, \\(z^p\\)\nなし\nN/A\nN/A\n不適切",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#実践的な指針",
    "href": "ja/pdlt/07-universality-classes.html#実践的な指針",
    "title": "Universality Classes",
    "section": "",
    "text": "深層ネットワーク（50層以上）: ReLU またはその variants（Leaky ReLU, PReLU）\n\nScale invariance による安定性\n計算効率の高さ\n\n中程度の深さ（10-50層）: tanh も選択肢\n\nK* = 0 class の power law 減衰は実用的\nSmooth な微分が必要な場合に有用\n\n避けるべき: sigmoid, softplus\n\nCriticality を達成できない\n深層化で exponential behavior を引き起こす\n\nSWISH/GELU の利用: 注意が必要\n\n実践では良好な性能を示すこともある\n理論的には ReLU/tanh に劣る\n特定のタスクで実験的に検証する価値はある\n\n\n\n\n\n普遍性クラスの理論は、適切な初期化の重要性を強調している：\n\nReLU: He initialization \\((C_b, C_W) = (0, 2)\\)\ntanh: Xavier/Glorot initialization に近い \\((C_b, C_W) = (0, 1)\\)\n\nこれらの初期化により、訓練開始時に kernel が critical fixed point に保たれる。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  },
  {
    "objectID": "ja/pdlt/07-universality-classes.html#まとめ",
    "href": "ja/pdlt/07-universality-classes.html#まとめ",
    "title": "Universality Classes",
    "section": "",
    "text": "普遍性クラスの理論により、以下が明らかになった：\n\n活性化関数の選択は深層極限で重要: 浅いネットワークでは差が小さいが、深層化すると普遍性クラスの違いが顕著になる\nScale-invariant class が最も安定: ReLU が深層学習で支配的な理由の理論的根拠\nCriticality は必須: 適切な初期化により、signal の exploding/vanishing を防ぐ\nUniversality の力: 異なる活性化関数が深層極限で同様の挙動を示すことで、理論的予測が可能になる\n\nPDLT の普遍性クラスの分析は、活性化関数とアーキテクチャ設計の理論的基盤を提供し、経験的知見（ReLU の優位性など）に物理学的な説明を与えている。",
    "crumbs": [
      "Naoto Iwase",
      "Principles of Deep Learning Theory",
      "Universality Classes"
    ]
  }
]