# Multi-Image Understanding

Multi-Image Understanding は、複数の画像を同時に処理し、それらの関連性や違いを理解する能力です。従来の単一画像処理では各画像を独立して扱うのに対し、Multi-Image Understanding では複数の画像間の関係性を捉えることができます。

## 単一画像処理との違い

**単一画像処理**:
- 1枚の画像に対して質問応答やキャプション生成を実行
- 画像間の比較や関連性の理解は不可能
- 文書の複数ページやビフォー・アフター比較などには対応困難

**Multi-Image Understanding**:
- 2〜5枚の意味的に関連する画像セットを処理
- 画像間の共通点・相違点を理解
- 複数画像にまたがる質問応答やグラウンディングが可能

## Molmo2-MultiImageQA データセット

Molmo2-MultiImageQA は、意味的に関連する画像セットに対する質問応答データセットです。

**データセット規模**:
- 45,000 画像セット（96,000 ユニーク画像から構成）
- 72,000 QA ペア
- 1セットあたり 2〜5 枚の画像（平均 2.73 枚）

**収集方法**:
人手によるアノテーションで構築されており、以下のプロセスで作成されました。

1. PixMoCap で訓練されたモデルで各画像のキャプションを生成
2. キャプションの文レベルの類似度に基づいて画像をグルーピング
3. アノテーターが各セットに対して質問を作成
4. Claude Sonnet 4.5 との反復ループで回答を改善

このアプローチにより、実世界でのマルチイメージクエリをサポートする高品質なデータセットが構築されました。

## Molmo2-MultiImagePoint データセット

Molmo2-MultiImagePoint は、複数画像にわたるポインティングとカウンティングのデータセットです。

**データセット規模**:
- 470,000 以上のポインティング・カウンティング例
- 1セットあたり 2〜5 枚の画像（平均 3.24 枚）

**収集方法**:
合成的に構築されており、以下のパイプラインで作成されました。

### データ収集パイプライン

```
┌─────────────────────────────────────────────────────────────┐
│  Step 1: Soft Clustering of Images                          │
│  - Use images from PixMo-Points                             │
│  - Combine single-token & sentence-level embedding          │
│  - Generate semantically related sets (2-5 images)          │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│  Step 2: Label Normalization                                │
│  - Lowercase, punctuation/whitespace normalization          │
│  - Synonym consolidation                                    │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│  Step 3: Canonical Label Generation                         │
│  - Use LLM to merge normalized labels                       │
│  - Create single canonical description                      │
│  - Defines shared entity/concept across all images          │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│  Step 4: Training-time Sampling                             │
│  - Sample from original annotations (not just canonical)    │
│  - Preserve lexical diversity & improve robustness          │
└─────────────────────────────────────────────────────────────┘
```

::: {.callout-note}

## Canonical Label の役割

Canonical Label は、画像セット内の複数の人手アノテーションを統合した標準的な記述です。例えば、「waterfall」「滝」「瀑布」などの異なる表現を「waterfall」という単一の canonical label に統合します。

ただし、トレーニング時には常に canonical label を使用するのではなく、元のアノテーションからも確率的にサンプリングすることで、多様な表現に対応できるモデルを構築しています。

:::

## Molmo2-SynMultiImageQA データセット

Molmo2-SynMultiImageQA は、テキストリッチな画像に特化した合成マルチイメージデータセットです。

**データセット規模**:
- 188,000 例の合成マルチイメージ QA

**収集方法**:
CoSyn [172] を拡張して構築されました。CoSyn は、チャート、表、文書などのテキストリッチな画像に対する質問応答を合成的に生成するフレームワークです。

**対象画像タイプ**:
- チャート（charts）
- 表（tables）
- 文書（documents）

これらのテキストリッチな画像は、文書理解や複数文書間の比較など、実用的なタスクに直結する重要なデータです。

::: {.callout-tip}

## 実用例: Multi-Image Understanding の活用

**文書理解**:
- 契約書の複数ページにわたる条項の比較
- レポートの異なるセクション間の整合性チェック
- 複数の請求書の内容比較

**複数画像の比較**:
- 製品の異なる角度からの写真を比較して特徴を理解
- ビフォー・アフター写真の変化検出
- 複数のチャートやグラフを横断した傾向分析

**グラウンディング**:
- 「すべての画像で滝を指し示して」のような複数画像にわたるポインティング
- 「赤い車が何枚の画像に写っているか？」のようなカウンティング
- セット全体での共通オブジェクトの検出

:::

## データセット統計

| データセット | 規模 | 画像セットサイズ | 収集方法 | 用途 |
|---|---|---|---|---|
| Molmo2-MultiImageQA | 45k セット<br/>72k QA | 2-5枚<br/>(平均2.73) | 人手 | 一般的な QA |
| Molmo2-MultiImagePoint | 470k 例 | 2-5枚<br/>(平均3.24) | 合成 | ポインティング・カウンティング |
| Molmo2-SynMultiImageQA | 188k 例 | - | 合成<br/>(CoSyn拡張) | テキストリッチ画像の QA |

## Multi-Image Understanding の重要性

Multi-Image Understanding は、単一画像処理では不可能だった以下のタスクを実現します。

**情報の統合**:
複数の情報源（画像）から情報を統合し、包括的な理解を提供します。

**比較・対照**:
画像間の共通点や相違点を明確に識別できます。

**文書処理**:
複数ページの文書や、複数の関連文書を横断した理解が可能になります。

**現実世界への適用**:
実際のアプリケーションでは、複数の画像を扱うシナリオが頻繁に発生します（例: ECサイトの商品画像、医療画像の時系列比較、監視カメラの複数アングルなど）。

Molmo2 は、これらの3つのデータセットを活用することで、オープンソースモデルの中で最高水準の Multi-Image Understanding 能力を実現しています。
