# Dense Video Captioning

## 概要

**Molmo2-Cap** は、Molmo2 の事前学習に使用される超詳細（dense）なビデオキャプションデータセットです。このデータセットは、従来のビデオキャプションデータセットと比較して桁違いに詳細な記述を含み、平均 **924 語/動画** という驚異的な記述量を実現しています。

従来の VLM が生成する短く表面的なキャプションとは異なり、Molmo2-Cap は動的イベントと細かな視覚的詳細の両方を捉えることで、モデルがビデオの時空間的理解を深める基盤を提供します。

**データセット規模**:

- **104k のビデオレベルキャプション**
- **431k のクリップレベルキャプション**
- 平均 **924 語/動画** の超詳細記述

## なぜ Dense Captioning が重要か

### ビデオ理解の課題

ビデオキャプションは画像キャプションよりも本質的に困難です。なぜなら、アノテーターは以下の両方を記述する必要があるからです:

1. **動的イベント**: 時間とともに変化する出来事、動作、状態遷移
2. **細かな視覚的詳細**: オブジェクトの外観、空間配置、属性の変化

既存のビデオキャプションデータセットの多くは、表面的な記述に留まっており、ビデオグラウンディング（いつ・どこで何が起きたか）を学習するには不十分でした。Molmo2-Cap は、この gap を埋めるために設計されました。

### 密度の重要性

より詳細なキャプションは、モデルに以下の能力を与えます:

- **時空間的な理解**: 「いつ」「どこで」「何が」起きたかを正確に把握
- **細粒度の視覚認識**: 小さな物体、微細な動作、属性の変化を捉える
- **文脈理解**: イベント間の因果関係や時間的依存性を学習

## 既存データセットとの比較

::: {.callout-note collapse="true"}
## 他のビデオキャプションデータセットとの比較

Molmo2-Cap は既存のビデオキャプションデータセットを大きく上回る記述量を実現しています:

| データセット | 平均語数/動画 | 特徴 |
|------------|--------------|------|
| **Molmo2-Cap** | **924 語** | 人手による音声記述 + Molmo 統合 |
| LLaVA-Video-178K | 547 語 | GPT ベースの合成キャプション |
| ShareGPT4-Video | 280 語 | GPT ベースの合成キャプション |
| RDCap | 100 語 | 既存データセット |
| RCap | 89 語 | 既存データセット |
| Video Localized Narratives | 75 語 | 人手アノテーション |

Molmo2-Cap は、LLaVA-Video の **1.7 倍**、ShareGPT4-Video の **3.3 倍**、Video Localized Narratives の **12 倍** 以上の記述量を持ちます。

**重要な差異**:
- Molmo2-Cap は **プロプライエタリモデル（GPT など）に依存しない** 完全にオープンなパイプラインで構築されている
- **人手による音声記述** をベースにしており、合成データよりも自然で詳細
- **フレームレベルのキャプション統合** により、低レベルの視覚的詳細も漏れなく記述

:::

## データ収集パイプライン

Molmo2-Cap のデータ収集は、革新的な **2 段階パイプライン** を採用しています。

### ステージ 1: ビデオソーシングと選定

1. **初期プール構築**: 10M 以上のビデオクリップを複数の大規模ソース（YT-Temporal、YouTube など）から収集
2. **情報量フィルタリング**:
   - 音声トラックを除去し、1 fps で均一サンプリング
   - H.264 でエンコードし、正規化された情報量スコアを算出: `bits / (duration × W × H)`
   - 平均 - 1σ 未満のビデオを除外（視覚的・時間的多様性が低いビデオを排除）
3. **多様性ベースサンプリング**:
   - SAM 2 でフレームをセグメント化し、視覚的複雑さを推定
   - Molmo でフレームをキャプション化し、MetaCLIP パイプラインでキーワード抽出
   - エントロピー最大化を目指した貪欲サンプリング（キーワード分布とセグメント数分布）
   - 最終的に約 **100k のビデオ** を選定（サンプリング率 1%）

### ステージ 2: 人手アノテーション

#### クリップ分割アルゴリズム

ビデオを可変長のクリップ（10〜30 秒）に分割します。**情報密度が高いクリップほど短い期間に設定** することで、アノテーターの負担を均等化しながら詳細な記述を促します。

- 平均 **4〜5 クリップ/動画** に分割

#### 音声記述の収集

::: {.callout-tip}
## なぜ音声記述を使うのか？

音声記述（Spoken Captions）は、タイピングによる記述と比較して以下の利点があります:

1. **記述速度が速い**: タイピングよりも自然に詳細を語れる
2. **自然な言語表現**: 書き言葉よりも口語の方が流暢で豊かな記述が得られる
3. **認知負荷の軽減**: タイピングに気を取られず、ビデオに集中できる

この手法は PixMo-Cap（画像キャプションデータセット）でも採用されており、高品質なキャプション生成に有効であることが実証されています。

:::

**アノテーションプロセス**:

1. **クリップ記述**:
   - アノテーターは短いクリップごとに音声で内容を説明（音声はミュート）
   - 画面上で起きていることを詳細に語る
   - リアルタイム文字起こし（Whisper-1）が自動実行
   - アノテーターは転写結果を編集し、誤認識を修正

2. **ビデオ全体の要約**:
   - すべてのクリップ記述が完了した後、ビデオ全体の包括的な説明を記述

3. **質問ベースのプロンプト**:
   - アノテーターに「動的な視覚的詳細」を記述するよう促すため、事前定義された質問セットを提示
   - 例: 「オブジェクトや出来事は時間とともにどう変化したか？」

### ステージ 3: LLM ベースの文章整形

Whisper の文字起こしは不完全な文や口語表現を含むため、**テキスト専用 LLM** で以下を実行:

- 文の構造を整理し、一貫性を確保
- 冗長性を削除し、読みやすさを向上
- 元の意味を保持しながら流暢な文章に変換

### ステージ 4: Molmo によるフレームレベル統合

人手記述だけでは見落としがちな **低レベルの視覚的詳細** を補完するため:

1. **Molmo でフレームレベルのキャプションを生成**:
   - 個々のフレームを Molmo（早期バージョン）でキャプション化
   - 色、テクスチャ、細かなオブジェクト属性などを記述

2. **LLM でマージ**:
   - クリップレベルのキャプションとフレームレベルのキャプションを統合
   - 重複を削除し、一貫性のある長文キャプションを生成

### パイプライン図解

```
┌─────────────────────────────────────────────────────────────┐
│  Stage 1: Video Selection                                   │
│  10M+ clips -> Filter -> Diversity Sampling -> 100k         │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  Stage 2: Human Annotation                                  │
│  Split -> Voice -> Whisper -> Edit                          │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  Stage 3: LLM Refinement                                    │
│  Organize -> Convert to coherent text                       │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  Stage 4: Molmo Integration                                 │
│  Molmo frame captions -> LLM merge -> Final caption         │
└─────────────────────────────────────────────────────────────┘
```

## データセットの統計

**ビデオソース**:
- YT-Temporal
- YouTube キーワード検索
- 複数の大規模ビデオデータセット

**ライセンス**: Creative Commons（一部）

**フィルタリング**:
- 視覚的・時間的多様性が低いビデオを除外
- 繰り返しパターンを含む低品質キャプションをヒューリスティックルールで除去

## 学習での使用

Molmo2-Cap は、Molmo2 の **事前学習（Pre-training）** フェーズで使用されます:

- **長さ条件付きキャプション生成**: モデルは指定された長さのキャプションを生成するよう学習
- **重み付けサンプリング**: ビデオキャプションデータには固定重み 0.1 を設定（他のタスクとバランス）

## 影響と貢献

Molmo2-Cap は以下の点で重要な貢献をしています:

1. **オープンソースの基盤**: プロプライエタリモデル（GPT など）に依存しない完全にオープンなパイプライン
2. **ビデオグラウンディングの基礎**: 超詳細な記述により、時空間的なポインティングとトラッキングを学習
3. **データ品質の新基準**: 平均 924 語/動画という記述量は、今後のビデオキャプションデータセットのベンチマークとなる
4. **再現可能な手法**: 音声記述 + LLM 整形 + Molmo 統合という明確なパイプラインは、他のプロジェクトでも再利用可能

## 評価: Molmo2-CapTest

Molmo2 のビデオキャプション能力を評価するため、**Molmo2-CapTest** という評価セットが構築されています:

- **693 の Creative Commons ライセンスビデオ**
- Molmo2-Cap と同様のプロトコルで収集されたが、**手動選定された高品質アノテーター** が担当
- 各ビデオに複数のリファレンスキャプションを用意
- **F1 スコア** でキャプション品質を評価

## まとめ

Molmo2-Cap は、以下の革新的な設計により、史上最も詳細なビデオキャプションデータセットとなっています:

- **音声記述 + Whisper**: 自然で流暢な記述を効率的に収集
- **LLM 整形**: 口語表現を読みやすい文章に変換
- **Molmo 統合**: 低レベルの視覚的詳細を補完
- **多様性ベースサンプリング**: 視覚的・意味的に多様なビデオセットを構築

このデータセットは、Molmo2 がビデオグラウンディング（pointing & tracking）を実現する上で不可欠な基盤となっており、完全にオープンなビデオ VLM の可能性を示しています。

---

**関連セクション**:

- [Video Grounding: Pointing & Tracking](02-video-grounding.qmd) - ビデオポインティング・トラッキングデータセット
- [Multi-Image Understanding](03-multi-image-understanding.qmd) - マルチイメージ理解データセット
