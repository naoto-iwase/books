# 重複除去

Olmo 3 の技術レポートに基づく、大規模言語モデル事前学習データの重複除去パイプラインの詳細解説。

---

## なぜ重複除去が重要か

重複除去は LLM の事前学習において極めて重要なステップである。その理由は以下の3つの観察に基づく。

### 1. トークン効率の向上

重複データを除去することで、同じ計算コストでより多様なデータから学習できる。Lee et al. (2022) の研究により、重複除去がトークン効率的な学習につながることが示されている。

### 2. 重複回数は品質シグナル

興味深いことに、重複回数は弱いながらもデータ品質の指標となる。高品質なコンテンツほど多くのサイトで引用・コピーされる傾向があるため、重複回数が多いほど高品質である可能性が高い（Fang et al., 2025a）。

### 3. 過度な繰り返しは収穫逓減

同じドキュメントを繰り返し学習しても、数回を超えると効果は急速に減少する（Muennighoff et al., 2025a）。過学習やメモリゼーションのリスクも増大する。

### 戦略の目標

Olmo 3 チームは、これらの観察から以下の戦略を採用した。

1. **徹底的な重複除去**で品質シグナル（重複回数）を一旦削除
2. **クリーンなベースデータセット**を作成
3. **品質ベースのアップサンプリング**で高品質データのみ選択的に繰り返し
4. 最終的に**重複を最小化しつつ、高品質データに重複を集中**

---

## 3段階アプローチの全体像

Olmo 3 では、3段階のパイプラインで段階的により精緻な重複を除去する。

```
[Input: 38.7B documents]
        |
        v
+-------------------+
| 1. Exact Dedup    |  <- Hash-based exact match
|    -67% reduction |
+-------------------+
        |
        v (12.7B docs)
+-------------------+
| 2. Fuzzy Dedup    |  <- MinHash similarity
|    -24% reduction |
+-------------------+
        |
        v (9.8B docs)
+-------------------+
| 3. Substring Dedup|  <- Suffix Array boilerplate removal
|    -14% bytes     |
+-------------------+
        |
        v
[Output: 9.7B documents (75% reduction)]
```

---

## Stage 1: Exact Deduplication（完全一致除去）

### 概要

ドキュメント全体のハッシュ値を計算し、完全に同一のドキュメントを除去する最もシンプルで高速な手法。

### 実装詳細

1. **ハッシュ計算**: 各ドキュメントに 128-bit ハッシュを付与
2. **2パス処理**:
   - **Pass 1（ダンプ内）**: 104個の CommonCrawl ダンプそれぞれで個別に除去 → **24% 削減**
   - **Pass 2（グローバル）**: 全ダンプを統合してグローバルに除去 → **追加 43% 削減**

### 結果

- **総削減率**: 66%（38.7B → 12.7B ドキュメント）
- **ツール**: Duplodocus

### 技術的補足

Exact Deduplication は MinHash の前に実行することで、MinHash の処理負荷を大幅に軽減できる。完全一致は明らかに Jaccard 類似度 1.0 なので技術的には冗長だが、計算効率の観点から有効。

---

## Stage 2: Fuzzy Deduplication（曖昧重複除去）

### MinHash の仕組み

MinHash は Locality-Sensitive Hashing (LSH) の一種で、Jaccard 類似度の高いドキュメントペアを効率的に発見する確率的アルゴリズム。

#### Jaccard 類似度

2つの集合 $A$ と $B$ に対して:

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

#### MinHash の基本原理

1. **n-gram 集合の構築**: ドキュメントをトークン化し、5-gram のセットを作成
2. **ハッシュ関数の適用**: 複数のハッシュ関数 $h_1, h_2, ..., h_k$ を用意
3. **最小値の選択**: 各ハッシュ関数について、集合内要素のハッシュ値の最小値を取得
4. **シグネチャの構成**: これらの最小値をベクトル化したものがドキュメントの "MinHash シグネチャ"

#### LSH によるバンディング

シグネチャを「バンド」に分割し、少なくとも1つのバンドが一致するペアを候補として抽出。

- **バンド数**: 26
- **バンドサイズ**: 11
- **ターゲット類似度閾値**: 0.80

この設定により、Jaccard 類似度 0.80 以上のペアが高確率で候補として検出される。

### 実装詳細

1. **シャーディング**: 12.7B ドキュメントを 32 シャードに分割
2. **トークン化**: p50k トークナイザーを使用
3. **MinHash 処理**: 各シャードで独立に実行
4. **グラフ構築**: 同一バケットに入ったドキュメントペアをエッジとしてグラフを構成
5. **連結成分抽出**: グラフの連結成分（クラスタ）を特定

### 検証フェーズ

MinHash は偽陽性を含むため、第2段階で実際の Jaccard 類似度を計算して検証。

- **小クラスタ（< 500）**: 3-gram で網羅的にペアワイズ Jaccard を計算
- **大クラスタ（>= 500）**: より厳密な MinHash（200 bands × 31 rows）を再適用

### 代表ドキュメントの選択

各クラスタから**クロール日が最新のドキュメント**を1つだけ保持。これにより最新版のコンテンツが残る。

### 結果

- **削減率**: 24%（12.7B → 9.8B ドキュメント）

---

## Stage 3: Substring Deduplication（部分文字列除去）

### 目的

前2段階はドキュメント全体の重複を対象としたが、個々のドキュメント内の繰り返しコンテンツ（ボイラープレート、ヘッダー/フッター、HTML アーティファクト）は除去できない。

### Suffix Array の仕組み

Suffix Array は文字列の全サフィックス（接尾辞）をソートした配列で、効率的な部分文字列検索を可能にするデータ構造。

#### 構築例

文字列 "banana" の場合:
```
位置  サフィックス
0     banana
1     anana
2     nana
3     ana
4     na
5     a
```

ソート後の Suffix Array: `[5, 3, 1, 0, 4, 2]`

これにより、繰り返し出現する部分文字列を $O(n)$ で発見可能。

### Fuzzy Suffix Array 戦略（Olmo 3 の新手法）

従来の Suffix Array 除去では、重複部分文字列の間に挟まれた短いユニークな断片が残ってしまう問題があった。

#### 新アプローチ

1. **閾値設定**: 500バイト以上の繰り返し部分文字列を特定
2. **スパン判定**: テキストのスパン（連続領域）を評価
   - スパンの両端が繰り返し 500バイトシーケンスで挟まれている
   - スパンの **80% 以上**が繰り返しシーケンスでカバーされている
3. **スパン全体除去**: 条件を満たすスパンは、間のユニーク断片も含めて全体を除去
4. **通常除去**: 条件を満たさない場合、個別の繰り返しシーケンスのみ除去

### 実装詳細

- **シャーディング**: 9.8B ドキュメントを 56 シャードに分割
- **ツール**: bsade（専用 Suffix Array ツール）

### 結果

- **削減率**: 14%（バイト単位）
- **最終出力**: 9.7B ドキュメント、36.5T バイト

---

## Duplodocus ツールの特徴

Olmo 3 チームが開発した大規模重複除去ツール。

### 技術仕様

| 項目 | 詳細 |
|------|------|
| **言語** | Rust（ネイティブ実装） |
| **対応処理** | Exact Dedup, MinHash Fuzzy Dedup |
| **実行モード** | 分散処理対応 |
| **リポジトリ** | github.com/allenai/duplodocus |

### Rust を選択した理由

1. **メモリ安全性**: 大規模データ処理での安定性
2. **ゼロコスト抽象化**: 高レベル API と低レベル性能の両立
3. **並列処理**: Rayon などによる効率的な並列化
4. **メモリ効率**: ガベージコレクションなしで予測可能なメモリ使用量

### 分散処理アーキテクチャ

- 32〜56 シャードへの分割による水平スケーリング
- 各シャードで独立して処理し、結果を統合
- EC2 i4i.32xlarge インスタンスでの実行実績

---

## 各ステップの効果まとめ

| ステージ | 手法 | 削減率 | 出力 | 計算コスト |
|----------|------|--------|------|------------|
| 入力 | - | - | 38.7B docs | - |
| 1. Exact | ハッシュ | 67% | 12.7B docs | 低 |
| 2. Fuzzy | MinHash + Jaccard | 24% | 9.8B docs | 中〜高 |
| 3. Substring | Suffix Array | 14% bytes | 9.7B docs | 高 |
| **合計** | | **75% docs** | **9.7B docs** | |

**処理時間**: 約 1,030 時間（i4i.32xlarge インスタンス累計）

---

## よくある質問

### Q: なぜ3段階に分けるのか？

- **計算効率**: 安価な手法で大量削除してから高価な手法を適用
- **粒度の違い**: ドキュメント全体 → 近似重複 → 部分文字列と段階的に精緻化
- **偽陽性/陰性のトレードオフ**: 各手法の特性に応じた閾値設定

### Q: MinHash の類似度閾値 0.80 の根拠は？

- 低すぎると偽陽性（異なるドキュメントを同一視）
- 高すぎると偽陰性（重複を見逃す）
- 0.80 は「ヘッダー/フッターだけ異なる」ケースを捕捉しつつ、本質的に異なるドキュメントを保護

### Q: 重複除去しすぎるリスクは？

- 品質シグナルの喪失（高品質 = 多くコピーされる）
- 解決策: クリーンなデータを作成後、品質ベースで選択的にアップサンプリング

### Q: Suffix Array vs n-gram ハッシュ

- **Suffix Array**: 任意長の繰り返しを発見可能、正確だが計算コスト高
- **n-gram ハッシュ**: 固定長のみ、高速だが長い繰り返しには不向き
- Olmo 3 は 500 バイト閾値でバランスを取っている

---

<details>
<summary><strong>他LLMプロジェクトの重複除去手法との比較</strong></summary>

各LLMプロジェクトは異なる重複除去戦略を採用している。

| プロジェクト | 手法 | 特徴 |
|------------|------|------|
| **Olmo 3** | Exact + MinHash + Suffix Array | 3段階パイプライン、75%削減 |
| **Llama 3** | MinHash (ccNet) | URL + 行単位正規化、5-gram |
| **Qwen 2.5** | MinHash + Suffix Array | OLMoに類似、詳細非公開 |
| **DeepSeek** | MinHash | 基本的なfuzzy dedup |
| **Gemma 2** | 非公開 | "extensive deduplication" |

**Llama 3の特徴（ccNetベース）:**
- URL正規化による重複判定
- 行単位での処理（ドキュメント全体ではなく）
- 30のMinHashシグネチャを3バンド×10行で処理

**Olmo 3が優れている点:**
1. **Substring deduplication**: 他モデルにはない、ボイラープレート除去に効果的
2. **Fuzzy suffix array**: 従来手法の「断片残り」問題を解決
3. **完全公開**: Duplodocusツールとパイプライン全体を公開

**参考**: [ccNet (Wenzek et al., 2020)](https://aclanthology.org/2020.lrec-1.494/)

</details>

---

## 参考文献

- Lee et al. (2022): "Deduplicating Training Data Makes Language Models Better"
- Fang et al. (2025a): 重複回数と品質の相関に関する研究
- Muennighoff et al. (2025a): データ繰り返しと学習効率の研究
- Olmo 3 Technical Report (2025)

---

## 関連ツール

- **Duplodocus**: github.com/allenai/duplodocus（Exact + MinHash）
- **bsade**: github.com/liujch1998/bsade（Suffix Array）
