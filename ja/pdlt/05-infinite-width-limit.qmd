# Infinite Width Limit

**無限幅極限（Infinite Width Limit）** は、ニューラルネットワークの各層の幅 $n$ を無限大に取る極限操作である。この極限下では、ネットワークの挙動が劇的に簡単化される一方で、深層学習の重要な性質の一部が失われる。

## 無限幅極限の定義

ニューラルネットワークの層 $\ell$ における幅 $n_\ell$ を無限大に取る極限:

$$
n_\ell \to \infty \quad \text{for all layers } \ell
$$

この極限下で、ネットワークの出力分布は以下のようなシンプルな **ガウス過程（Gaussian Process）** に収束する:

$$
p\left(z^{(L)}_D \mid H\right) = \frac{1}{\sqrt{|2\pi K|^{n_L}}} \exp\left(-\frac{1}{2} \sum_{i=1}^{n_L} \sum_{\delta_1, \delta_2 \in D} K^{\delta_1 \delta_2} z^{(L)}_{i;\delta_1} z^{(L)}_{i;\delta_2}\right)
$$

ここで:

- $K_{\delta_1 \delta_2} = K^{(L)}(x_{\delta_1}, x_{\delta_2})$ は **カーネル（Kernel）** で、入力ペア間の相関を表す
- $K^{\delta_1 \delta_2}$ はカーネルの逆行列
- 分布は平均ゼロのガウス分布

## Gaussian Process への収束

無限幅極限下では、各ニューロンの出力が独立になり、中心極限定理により出力分布がガウス分布に収束する。このとき:

- **出力成分間が独立**: 異なるニューロン $i$ と $j$ の出力 $z^{(L)}_i$ と $z^{(L)}_j$ は独立
- **カーネルのみで決まる**: 分布の形はカーネル $K(x_{\delta_1}, x_{\delta_2})$ のみで完全に決定される
- **重みのランダム性が消える**: 個々の重み $W^{(\ell)}_{ij}$ の影響は平均化される

::: {.callout-note collapse="true"}
## Gaussian Process と Neural Network Gaussian Process (NNGP)

無限幅ニューラルネットワークは **Neural Network Gaussian Process (NNGP)** と呼ばれるガウス過程に収束する。このカーネル $K(x, x')$ は、以下の再帰的な式で計算できる（ReLU 活性化の場合）:

$$
K^{(\ell+1)}(x, x') = \sigma_W^2 \mathbb{E}_{\phi \sim \mathcal{N}(0, \Sigma^{(\ell)})} \left[\phi(z^{(\ell)}(x)) \phi(z^{(\ell)}(x'))\right] + \sigma_b^2
$$

ここで $\Sigma^{(\ell)}$ は層 $\ell$ の共分散行列である。この再帰式により、深層ネットワークのカーネルを層ごとに順次計算できる。
:::

## NTK の固定化

無限幅極限のもう一つの重要な帰結は、**Neural Tangent Kernel (NTK)** が訓練中に **固定される** ことである。

### NTK とは

NTK は、ネットワークの出力 $z^{(L)}_i(x)$ のパラメータ $\theta$ に関する勾配の内積として定義される:

$$
\Theta^{(L)}_{\delta_1 \delta_2} = \frac{1}{n_L} \sum_{i=1}^{n_L} \mathbb{E}\left[\frac{\partial z^{(L)}_i(x_{\delta_1})}{\partial \theta} \cdot \frac{\partial z^{(L)}_i(x_{\delta_2})}{\partial \theta}\right]
$$

### 無限幅での固定化

無限幅極限では、訓練中に NTK が初期値 $\Theta^{(L)}_{\delta_1 \delta_2}(t=0)$ から変化しない:

$$
\Theta^{(L)}_{\delta_1 \delta_2}(t) = \Theta^{(L)}_{\delta_1 \delta_2}(0) + O\left(\frac{1}{n}\right)
$$

この固定化は以下を意味する:

- **線形化された学習**: ネットワークは初期化点周りの線形近似で学習
- **カーネル法との等価性**: 無限幅ネットワークは固定カーネルを使うカーネル法と等価
- **特徴学習の欠如**: NTK が変化しないため、新しい特徴を学習できない

## 表現学習が起こらない理由

無限幅極限下では、**表現学習（Representation Learning）** が完全に失われる。これは第6.3.3節で示される重要な結果である。

### 表現学習の定義

表現学習とは、隠れ層の表現 $z^{(\ell)}_D$ が訓練データ $y_A$ の観測によって更新されることを指す。ベイズの定理により、事後分布を考えると:

$$
p\left(z^{(L-1)}_D \mid y_A\right) = \frac{p\left(y_A \mid z^{(L-1)}_D\right) p\left(z^{(L-1)}_D\right)}{p(y_A)}
$$

### 無限幅での結果

無限幅極限では、尤度が前の層の表現に依存しなくなる:

$$
p\left(y_A \mid z^{(L-1)}_D\right) = p(y_A)
$$

その結果、**事後分布が事前分布と一致** する:

$$
p\left(z^{(L-1)}_D \mid y_A\right) = p\left(z^{(L-1)}_D\right)
$$

これは以下を意味する:

- **観測が無意味**: 訓練データ $y_A$ を観測しても、隠れ層の表現は更新されない
- **層間相関の欠如**: 異なる層 $z^{(\ell)}_D$ と $z^{(\ell+1)}_D$ の間に相関がない
- **深層の意義が失われる**: 深い層を重ねる主要な動機である複雑な表現学習が不可能

::: {.callout-important}
## なぜ表現学習が失われるのか？

無限幅極限では、各層のニューロンが完全に独立になるため、層間の情報伝達が平均値（カーネル）のみで行われる。個々のニューロンの活性パターンが隠れ層の表現を形成できないため、表現学習が起こらない。

この問題は、**有限幅ネットワーク** に戻ることで解決される（第6.4節）。
:::

## 有限幅との対比

無限幅極限と有限幅ネットワークの主要な違いを表にまとめる:

| 性質 | 無限幅極限 | 有限幅 |
|------|-----------|--------|
| 出力分布 | ガウス過程 | 非ガウス分布 |
| NTK | 訓練中に固定 | 訓練中に変化 |
| 表現学習 | なし | あり |
| 層間相関 | なし | あり（$O(1/n)$） |
| 学習アルゴリズム | カーネル法と等価 | 真の特徴学習 |
| 出力成分間の相関 | 独立 | 相関あり |
| 計算の複雑さ | 理論的に単純 | 複雑だが実用的 |

: 無限幅極限と有限幅ネットワークの比較 {#tbl-infinite-vs-finite}

@tbl-infinite-vs-finite に示すように、有限幅ネットワークは $O(1/n)$ の相関により表現学習が可能になる。

## Chapter 6.3 の要点

Chapter 6.3 では、無限幅極限下でのベイズ学習について3つの重要な教訓を示している:

1. **臨界性の証拠（§6.3.1）**:

   - 証拠 $p(y_A \mid H)$ を計算し、ベイズモデル比較が臨界性を好むことを示す
   - 十分に深いネットワークでは、臨界条件 $\chi_\parallel(K^*) = 1$ および $\chi_\perp(K^*) = 1$ が最適

2. **独立な出力成分（§6.3.2）**:

   - 事後分布 $p(z^{(L)}_B \mid y_A)$ を計算し、異なる出力成分が完全に独立であることを示す
   - これは "Let's Not Wire Together"（配線されない）という状態を意味する

3. **表現学習の欠如（§6.3.3）**:

   - 事後分布 $p(z^{(L-1)}_D \mid y_A)$ が事前分布 $p(z^{(L-1)}_D)$ と一致することを示す
   - 隠れ層の表現が訓練データから学習されないことを証明

::: {.callout-note collapse="true"}
## 臨界性とベイズ証拠

単一入力の場合、証拠は以下のように書ける:

$$
p(y \mid H) = \frac{1}{\sqrt{2\pi K}^{n_L}} \exp\left(-\frac{1}{2K} \sum_{i=1}^{n_L} y_i^2\right)
$$

- $K \to \infty$ の場合: 証拠は多項式的にゼロに収束（分布が広すぎる）
- $K \to 0$ の場合: 証拠は指数関数的にゼロに収束（ゼロ出力のみ予測）
- $K = O(1)$ の場合: 証拠が最大化される（臨界性が最適）

二入力の場合、証拠を最大化するには $K^{[0]} \approx Y^{[0]}/n_L = O(1)$ かつ $K^{[2]} \approx Y^{[2]}/n_L = O(1)$ が必要で、これは並行感受率条件 $\chi_\parallel(K^*) = 1$ と垂直感受率条件 $\chi_\perp(K^*) = 1$ の両方を満たす臨界性に対応する。
:::

## 実用的な含意

無限幅極限の研究は、以下の理由で重要である:

- **理論的ベンチマーク**: 実際の有限幅ネットワークの挙動を理解するための基準点
- **幅の重要性**: なぜ実用的なネットワークで適度な幅が必要かを説明
- **初期化の設計**: 臨界性条件は実際の初期化スキーム（He初期化など）の理論的根拠を提供
- **特徴学習の必要性**: カーネル法を超えた深層学習の本質的価値を明確化

実際の深層学習では、**有限幅ネットワーク** を使用することで無限幅極限の制約を回避し、真の表現学習と特徴学習を実現する。第6.4節と第11章では、有限幅効果がどのように表現学習を可能にするかが詳述される。
