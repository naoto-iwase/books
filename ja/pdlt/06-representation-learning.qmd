# Representation Learning

**Representation Learning**（表現学習）は、ニューラルネットワークが学習を通じて内部表現を変化させ、タスクに適した特徴を獲得する現象です。深層学習理論において、表現学習の有無は無限幅と有限幅のニューラルネットワークを決定的に区別する重要な性質です。

## 無限幅での表現学習の欠如

無限幅のニューラルネットワークでは、表現学習が発生しません。Chapter 6.3.3 および Chapter 10.1.2 で示されるように、無限幅極限では以下の二つの制約が生じます。

### No Wiring（配線なし）

無限幅では、ネットワーク出力の各成分が独立に更新されます。例えば MSE 損失の場合、$i$ 番目の出力 $z^{(L)}_i$ の更新は $i$ 番目の誤差因子のみに依存します。

$$
d\bar{z}^{(L)}_{i;\delta} = -\eta \sum_{\tilde{\alpha} \in \mathcal{A}} \Theta^{(L)}_{\delta\tilde{\alpha}} \epsilon_{i;\tilde{\alpha}}
$$

これにより、出力成分間の相関を学習することができません。

### No Representation Learning（表現学習なし）

最終隠れ層 $L-1$ の事前活性化 $z^{(L-1)}_j$ の更新分布を解析すると、その平均と分散はともに $1/n$ で抑制されます。

**更新の平均**:

$$
\mathbb{E}\left[ d\bar{z}^{(L-1)}_{j;\delta} \right] = O(1/n)
$$

**更新の共分散**:

$$
\text{Cov}\left[ d\bar{z}^{(L-1)}_{j_1;\delta_1}, d\bar{z}^{(L-1)}_{j_2;\delta_2} \right] = O(1/n)
$$

無限幅極限（$n \to \infty$）では、これらの量はゼロに漸近します。したがって、学習前後で隠れ層の分布は変化せず、**表現学習が発生しません**。

::: {.callout-important}
## 無限幅ネットワークの限界

無限幅ニューラルネットワークは、本質的に**線形モデル**および**カーネル法**と等価です（Chapter 10.4）。これらは以下の制約を持ちます。

- 固定された特徴表現（frozen features）
- 線形な関数近似
- カーネルトリックに基づく予測

実用的な深層学習モデルの柔軟性と表現力を理解するには、有限幅への拡張が不可欠です。
:::

## 有限幅での表現学習の発生

有限幅のニューラルネットワークでは、表現学習が発生します。Chapter 6.4 および Chapter 11 で詳述されるように、有限幅では**非ガウス相互作用**が表現学習を可能にします。

### Hebbian Learning（ヘブ学習）

有限幅ニューラルネットワークは、神経連想（neural association）に対する帰納的バイアスを自動的に持ちます。これは Donald Hebb の古典的原理に基づきます。

> Any two cells or systems of cells that are repeatedly active at the same time will tend to become "associated," so that activity in one facilitates activity in the other.
>
> — Donald Hebb, *The Organization of Behavior* (1949)

有限幅では、ニューロン間の非ガウス相互作用により、一つの事前活性化 $z^{(\ell)}_1$ が別の事前活性化 $z^{(\ell)}_2$ に影響を与えます。この影響は条件付き分布 $p(z^{(\ell)}_2 | \hat{z}^{(\ell)}_1)$ で表現されます。

無限幅では $p(z^{(\ell)}_2 | \hat{z}^{(\ell)}_1) = p(z^{(\ell)}_2)$ となり、ニューロン間に関連性はありません。しかし有限幅では、この条件付き分布が事前分布から逸脱し、**ヘブ学習**が自然に生じます。

::: {.callout-note collapse="true"}
## Hebbian Learning の生物学的背景

ヘブ学習は、もともと生物学的ニューロンのシナプス可塑性を説明するために提案されました。「同時に発火するニューロンは結合する（Cells that fire together, wire together）」という原則は、以下の神経科学的現象と関連します。

- **Long-Term Potentiation (LTP)**: シナプス結合の長期的な強化
- **Spike-Timing-Dependent Plasticity (STDP)**: スパイクのタイミングに依存した可塑性
- **Neural Association**: 記憶と学習における神経回路の形成

人工ニューラルネットワークにおいても、有限幅でこの原理が自然に現れることは、生物学的妥当性の観点から興味深い性質です。
:::

### Let's Wire Together（配線の生成）

有限幅では、学習によって出力成分間の相関が生成されます。Chapter 6.4.2 で示されるように、ベイズ事後分布の平均 $p(z^{(L)}_B | y_A)$ を計算すると、層内のニューロン相互作用により非自明な相関が生じます。

### Presence of Representation Learning（表現学習の存在）

最終隠れ層の事前活性化の事後分布 $p(z^{(L-1)}_B | y_A)$ は、事前分布からの**非ゼロのシフト**を示します。これは層間相互作用により生じ、有限幅における表現学習の存在を示します（Chapter 6.4.3）。

## dNTK の役割

**Differential of the Neural Tangent Kernel (dNTK)** は、表現学習を定量化する中心的な量です（Chapter 11）。

### dNTK の定義

dNTK は、NTK の訓練データに対する微分として定義されます（Chapter 11.1）。

### RG Flow of the dNTK

Chapter 11.2 では、dNTK の再帰方程式（Renormalization Group flow）が導出されます。

**第1層**: dNTK はゼロです（11.2.1）。

**第2層**: dNTK が非ゼロになります（11.2.2）。

**深い層**: dNTK は層とともに成長します（11.2.3）。

この成長は、深いネットワークほど表現学習能力が高いことを示唆します。

### Effective Theory of the dNTK

Chapter 11.3 では、dNTK の臨界解析が行われます。

**Scale-Invariant Universality Class** (11.3.1):

ReLU などのスケール不変活性化関数に対して、dNTK は特定の臨界指数で成長します。

**$K^* = 0$ Universality Class** (11.3.2):

tanh や sin などの活性化関数に対して、異なる臨界挙動を示します。

## 無限幅と有限幅の比較

以下の表は、無限幅と有限幅におけるニューラルネットワークの性質を比較します。

| Property                  | Infinite Width           | Finite Width              |
|---------------------------|--------------------------|---------------------------|
| NTK Behavior              | Frozen (constant)        | Evolves during training   |
| Output Correlation        | Independent (No Wiring)  | Correlated (Wiring)       |
| Hidden Representation     | Fixed (No Learning)      | Changes (Learning)        |
| Hebbian Association       | Absent                    | Present                   |
| Model Class               | Linear / Kernel Method   | Nonlinear Model           |
| dNTK                      | Zero                     | Nonzero (growing)         |

::: {.callout-tip}
## Kernel vs. Nearly-Kernel Methods

Chapter 11.4 では、有限幅ニューラルネットワークを**非線形モデル**および**Nearly-Kernel Methods**として解釈する枠組みが提示されます（11.4.1, 11.4.2）。

- **無限幅**: カーネル法（Kernel Methods） — 固定カーネルに基づく線形予測
- **有限幅**: Nearly-Kernel Methods — カーネルが学習中に微小に進化

有限幅ネットワークは「ほぼカーネル法」として動作しますが、dNTK による微小な非線形性が表現学習を可能にします（11.4.3）。
:::

## 表現学習の重要性

表現学習は、深層学習が従来の機械学習手法を凌駕する主要な理由の一つです。

### タスク適応性

有限幅ネットワークは、訓練データに基づいて内部表現を調整し、タスクに特化した特徴を学習します。

### 階層的特徴抽出

深い層ほど抽象的な特徴を学習し、浅い層は低レベルの特徴を捉えます。dNTK の深さ依存性はこの性質を反映します。

### 汎化性能の向上

適切な表現を学習することで、訓練データに見られないパターンにも対応できる汎化能力が向上します。

## まとめ

**Representation Learning（表現学習）** は、深層学習理論における中心的概念です。

- **無限幅**: 表現学習は発生せず、ネットワークはカーネル法に帰着（Chapter 6.3, 10）
- **有限幅**: Hebbian learning により表現学習が発生（Chapter 6.4）
- **dNTK**: 表現学習を定量化する重要な量（Chapter 11）
- **理論的意義**: 実用的な深層学習モデルの理解には有限幅理論が不可欠

表現学習の理解は、深層ニューラルネットワークがなぜ強力なのかを説明する鍵となります。
