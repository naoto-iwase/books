# RG Flow

Renormalization Group Flow (繰り込み群フロー, RG Flow) は、深層ニューラルネットワークにおける層ごとの表現の変化を理解するための理論的枠組みです。物理学における繰り込み群の概念を深層学習に応用し、ネットワークが入力データから出力へと情報を段階的に変換していく過程を形式化します。

## 繰り込み群とは

繰り込み群 (Renormalization Group, RG) は元々物理学で発展した概念で、スケールの異なる階層において系の振る舞いがどのように変化するかを記述する手法です。微視的な自由度を積分消去 (marginalize out) することで、より粗視化された有効理論を導出します。

::: {.callout-note collapse="true"}
## 物理学における繰り込み群

物理学では、繰り込み群は複雑な相互作用系を理解するための強力な手法として確立されています。

**基本的なアイデア**:

- 微視的スケールから巨視的スケールへと観測スケールを変化させる
- 各スケールで有効な相互作用パラメータがどのように「流れる」かを追跡
- 微視的自由度を積分消去し、粗視化された記述を得る

**深層学習との類似**:

- **物理学**: 微視的スケール → 巨視的スケール
- **深層学習**: 入力層 → 隠れ層 → 出力層

どちらも、細かい情報を段階的に粗視化し、より高次の抽象的な記述へと変換していくプロセスです。
:::

## 深層学習における RG Flow

深層ニューラルネットワークでは、RG Flow は以下のプロセスを形式化します。

**層ごとの変換**:

```
┌──────────────────────────────────────────────────┐
│  Layer l -> Layer l+1                            │
│                                                  │
│  Preactivations z^(l) -> z^(l+1)                 │
│                                                  │
│  Distribution p(z^(l)) -> p(z^(l+1))             │
└──────────────────────────────────────────────────┘
```

各層において、前層の preactivation（活性化前の値）の分布を積分消去し、次層の preactivation の分布を導出します。このプロセスは、物理学における繰り込み変換に対応します。

## Preactivation の RG Flow

### 第1層: ガウス分布

初期化時、第1層の preactivation は**ガウス分布**に従います (Chapter 4.1)。

**定義**:

$$
z^{(1)}_{i;\alpha} = b^{(1)}_i + \sum_{j=1}^{n_0} W^{(1)}_{ij} x_{j;\alpha}
$$

ここで、バイアス $b^{(1)}$ と重み $W^{(1)}$ は独立なガウス分布から初期化されます。

**分布**:

$$
p(z^{(1)} | \mathcal{D}) = \frac{1}{Z} e^{-S(z^{(1)})}
$$

作用 (action) は2次形式:

$$
S(z^{(1)}) = \frac{1}{2} \sum_{i=1}^{n_1} \sum_{\alpha_1, \alpha_2 \in \mathcal{D}} G^{\alpha_1 \alpha_2}_{(1)} z^{(1)}_{i;\alpha_1} z^{(1)}_{i;\alpha_2}
$$

$G^{(1)}_{\alpha_1 \alpha_2}$ は第1層の計量 (metric) で、サンプル間の相関を表します。

**重要な性質**:

- すべての奇数次相関関数がゼロ
- 4次以上の連結相関関数がゼロ（Wick の定理が成立）
- 完全にガウス的

### 第2層: 非ガウス性の出現

第2層では、**非ガウス性** (non-Gaussianity) が出現します (Chapter 4.2)。活性化関数が非線形であるため、4次連結相関関数が非ゼロになります。

**分布の変化**:

$$
p(z^{(2)} | \mathcal{D}) \approx \frac{1}{Z} \exp\left( -S_2(z^{(2)}) - S_4(z^{(2)}) + O(1/n^2) \right)
$$

ここで:

- $S_2$: 2次作用（ガウス部分）
- $S_4$: 4次作用（非ガウス補正、$O(1/n)$ で抑制）

**Large-$n$ expansion**:

ネットワーク幅 $n$ が大きいとき、非ガウス性は $1/n$ で抑制されます。この性質により、有限幅ネットワークの振る舞いを体系的に展開できます。

### 深い層: 非ガウス性の蓄積

第3層以降、非ガウス性は層ごとに蓄積していきます (Chapter 4.3)。

**再帰的構造**:

各層 $\ell$ から層 $\ell+1$ への遷移は、以下の2つの寄与を含みます。

- 第 $\ell$ 層から**継承された**非ガウス性
- 第 $\ell$ 層から第 $\ell+1$ 層への遷移で**新たに生成された**非ガウス性

**深さと幅の比**:

非ガウス性の蓄積は、深さ $L$ と幅 $n$ の比 $L/n$ によって特徴づけられます。ネットワークが well-behaved であるためには、この比が適切に制御される必要があります (depth-to-width ratio)。

**流れの図示**:

```mermaid
graph LR
    A[Layer 1<br/>Gaussian] --> B[Layer 2<br/>Weak non-Gaussian]
    B --> C[Layer 3<br/>Moderate non-Gaussian]
    C --> D[Layer L<br/>Accumulated non-Gaussian]

    style A fill:#e1f5ff
    style B fill:#ffe1f5
    style D fill:#ffe1e1
```

## NTK の RG Flow

Neural Tangent Kernel (NTK) もまた、層ごとに「流れ」ます (Chapter 8)。NTK は勾配降下法による学習のダイナミクスを支配する量です。

### 第 $\ell$ 層の NTK

**定義**:

$$
\hat{H}^{(\ell)}_{i_1 i_2; \alpha_1 \alpha_2} \equiv \sum_{\mu, \nu} \lambda^{\mu\nu} \frac{dz^{(\ell)}_{i_1;\alpha_1}}{d\theta^\mu} \frac{dz^{(\ell)}_{i_2;\alpha_2}}{d\theta^\nu}
$$

ここで、$\lambda^{\mu\nu}$ は学習率テンソル、$\theta^\mu$ はモデルパラメータです。

### NTK の再帰式

第 $\ell$ 層の NTK から第 $\ell+1$ 層の NTK への遷移は、以下の再帰式で記述されます (Chapter 8.0)。

**Forward equation**:

$$
\hat{H}^{(\ell+1)}_{i_1 i_2; \alpha_1 \alpha_2} = \delta_{i_1 i_2} \left( \lambda^{(\ell+1)}_b + \lambda^{(\ell+1)}_W V^{(\ell)}_{\alpha_1 \alpha_2} \right) + \sum_{j_1, j_2} \frac{dz^{(\ell+1)}_{i_1;\alpha_1}}{dz^{(\ell)}_{j_1;\alpha_1}} \frac{dz^{(\ell+1)}_{i_2;\alpha_2}}{dz^{(\ell)}_{j_2;\alpha_2}} \hat{H}^{(\ell)}_{j_1 j_2; \alpha_1 \alpha_2}
$$

この式は:

- 第1項: 第 $\ell+1$ 層のパラメータからの直接寄与
- 第2項: 第 $\ell$ 層以前からの寄与（連鎖律による）

### NTK の統計的性質

初期化時、NTK は確率的な量です (Chapter 8.1-8.3)。

**第1層**: NTK は**決定論的**（ゆらぎなし）

**第2層**: NTK に**ゆらぎ**が出現

**深い層**: NTK のゆらぎが蓄積

Preactivation の RG Flow と完全に並行して、NTK の統計も再帰的に計算できます。

## RG Flow の意義

RG Flow の定式化により、以下が可能になります。

**理論的洞察**:

- ネットワークの深さ $L$ と幅 $n$ の役割の明確化
- 臨界性 (criticality) の概念の拡張
- 有限幅効果の体系的な理解

**実用的示唆**:

- 初期化スキームの設計原理
- 深さと幅のバランスの指針
- 勾配消失・爆発問題の理解

::: {.callout-tip}
## RG Flow と表現学習

RG Flow は、深層ニューラルネットワークがデータの表現を段階的に抽象化していくプロセスを、厳密な数学的枠組みで記述します。

**直感的理解**:

- **入力層**: 細粒度の生データ
- **隠れ層**: 中間的な特徴表現（段階的に抽象化）
- **出力層**: 粗視化された最終表現

このプロセスは、物理学における微視的記述から巨視的記述への移行と本質的に同じ構造を持っています。
:::

## まとめ

Renormalization Group Flow は、深層学習の理論的理解において中心的な役割を果たします。

**主要な結果**:

- 第1層はガウス分布、第2層以降で非ガウス性が出現・蓄積
- 非ガウス性は $1/n$ 展開で体系的に制御可能
- Preactivation と NTK の両方が層ごとに「流れ」る
- 深さと幅の比 $L/n$ が重要なパラメータ

**次のステップ**:

- RG Flow の方程式を解き、臨界性を解析 (Chapter 5)
- NTK の統計を用いて学習ダイナミクスを理解 (Chapter 9, 10)
- 表現学習の出現条件を明らかにする (Chapter 11)

RG Flow は、深層学習の「有効理論」(effective theory) を構築するための基盤となる概念です。
