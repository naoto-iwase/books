# Neural Tangent Kernel

Neural Tangent Kernel（NTK、ニューラル接核）は、勾配降下法による深層学習の訓練ダイナミクスを理解するための重要な理論的ツールである。NTK は、訓練における微小なパラメータ更新がネットワーク出力に与える影響を特徴付ける。

## NTK とは何か

ニューラルネットワークの出力 $z_i$ をパラメータ $\theta$ の関数として考えたとき、勾配降下法による更新は次のように表される：

$$
z_i(\theta(t+1)) - z_i(\theta(t)) = -\eta \sum_{\mu,\nu} \lambda_{\mu\nu} \frac{dz_i}{d\theta_\mu} \frac{\partial \mathcal{L}}{\partial z_j} \frac{dz_j}{d\theta_\nu}
$$

ここで、**Neural Tangent Kernel** は次のように定義される：

$$
H_{i_1 i_2; \alpha_1 \alpha_2} \equiv \sum_{\mu,\nu} \lambda_{\mu\nu} \frac{dz_{i_1;\alpha_1}}{d\theta_\mu} \frac{dz_{i_2;\alpha_2}}{d\theta_\nu}
$$

この NTK $H$ は、訓練データ $\alpha_1, \alpha_2$ に対するネットワーク出力の勾配の内積として解釈でき、訓練中の出力変化を支配する。

### 層ごとの NTK

多層パーセプトロン（MLP）では、各層 $\ell$ における NTK $H^{(\ell)}$ を定義できる。これにより、隠れ層の表現がどのように変化するかも追跡できる。

層ごとの NTK は次の再帰的な関係式を満たす：

$$
\hat{H}^{(\ell+1)}_{i_1 i_2; \alpha_1 \alpha_2} = \delta_{i_1 i_2} \left[ \lambda_b^{(\ell+1)} + \lambda_W^{(\ell+1)} \langle \sigma_{\alpha_1} \sigma_{\alpha_2} \rangle_{K^{(\ell)}} \right] + C_W \langle \sigma'_{\alpha_1} \sigma'_{\alpha_2} \rangle_{K^{(\ell)}} \hat{H}^{(\ell)}_{i_1 i_2; \alpha_1 \alpha_2}
$$

ここで、$\sigma$ は活性化関数、$K^{(\ell)}$ はカーネル、$\lambda_b, \lambda_W$ は学習率である。

## 無限幅極限での NTK の性質

ネットワーク幅 $n \to \infty$ の極限では、NTK は**確定的**になる。つまり、ネットワークの異なるインスタンス間で NTK が自己平均化し、ランダム性が消失する。

この極限で NTK は **Frozen NTK（凍結 NTK）** $\Theta^{(\ell)}$ に収束する：

$$
\Theta^{(\ell+1)} = \lambda_b^{(\ell+1)} + \lambda_W^{(\ell+1)} g(K^{(\ell)}) + \chi_\perp(K^{(\ell)}) \Theta^{(\ell)}
$$

ここで：

- $g(K) = \langle \sigma(z) \sigma(z) \rangle_K$ は活性化関数の2点相関
- $\chi_\perp(K) = C_W \langle \sigma'(z) \sigma'(z) \rangle_K$ は横方向感受率

### 無限幅極限の特徴

無限幅ネットワークでは以下の性質が成り立つ：

1. **No Wiring（配線なし）**: 出力の各成分 $z_i^{(L)}$ が独立に更新され、成分間の相関が学習されない

2. **No Representation Learning（表現学習なし）**: 隠れ層の表現が訓練中に本質的に変化しない

3. **Algorithm Independence（アルゴリズム独立性）**: 勾配降下法でもニュートン法でも、同じ Frozen NTK によって訓練ダイナミクスが支配される

これらの制限は、無限幅ネットワークが本質的に**線形モデル**として振る舞うことを意味する。

## 訓練中の NTK の変化

### 無限幅の場合

無限幅極限では、NTK は訓練中に**不変**である。これを「凍結（frozen）」と呼ぶ理由である。

初期化時の Frozen NTK $\Theta^{(\ell)}$ は訓練を通じて変化せず、ネットワーク出力の更新は次のように線形化される：

$$
\bar{z}^{(\ell)}_{i;\delta}(t=1) - z^{(\ell)}_{i;\delta}(t=0) = -\eta \sum_{\tilde{\alpha} \in \mathcal{A}} \Theta^{(\ell)}_{\delta\tilde{\alpha}} \frac{d\mathcal{L}_\mathcal{A}}{dz^{(\ell)}_{i;\tilde{\alpha}}}
$$

この線形性により、多段階訓練後の予測も閉形式で表現できる（**Kernel Prediction**）。

### 有限幅の場合

幅が有限 $n < \infty$ の場合、NTK は訓練中に変化する。これを **NTK の解凍（defrosting）** と呼ぶ。

有限幅補正は $1/n$ 展開で表され、次のような項が現れる：

- **NTK の平均**: $H^{(\ell)} = \Theta^{(\ell)} + \frac{1}{n} H^{\{1\}(\ell)} + O(1/n^2)$
- **NTK のゆらぎ**: ネットワークインスタンス間で NTK が確率的に変動
- **前活性化との交差相関**: NTK と前活性化 $z^{(\ell)}$ の間に相関が生じる

これらの有限幅効果により、**表現学習（Representation Learning）** が可能になる。

## Chapter 8, 9, 10 の要点

### Chapter 8: RG Flow of the NTK

Chapter 8 では、NTK の統計的性質を層ごとに再帰的に計算する手法を導入する。

**主要な結果**:

- **第1層**: NTK は確定的（ゆらぎなし）

  $$
  \hat{H}^{(1)} = \lambda_b^{(1)} + \lambda_W^{(1)} \frac{1}{n_0} \sum_{j=1}^{n_0} x_j^2
  $$

- **第2層**: NTK にゆらぎが出現し、前活性化との交差相関が生じる

- **深い層**: ゆらぎと交差相関が層を重ねるごとに蓄積される

再帰関係式は次のようになる：

1. **NTK 平均の再帰**: (8.63) 式
2. **NTK-前活性化交差相関の再帰**: (8.77), (8.79) 式
3. **NTK 分散の再帰**: (8.89), (8.97) 式

これらの再帰は、前活性化の RG フロー（Chapter 4）と同様の構造を持つ。

### Chapter 9: Effective Theory of the NTK at Initialization

Chapter 9 では、初期化時の NTK の臨界性（criticality）と普遍性（universality）を解析する。

**臨界性解析**:

初期化ハイパーパラメータ $C_b, C_W$ と学習率 $\lambda_b^{(\ell)}, \lambda_W^{(\ell)}$ を適切に調整することで、深いネットワークでも安定した訓練が可能になる。

**普遍性クラス**:

- **Scale-Invariant 普遍性クラス**（ReLU など）: 深さ対幅の比 $L/n$ が重要なパラメータ
- **$K^* = 0$ 普遍性クラス**（tanh など）: 異なるスケーリング挙動

**勾配消失・爆発問題の解決**:

臨界性の概念により、深層ネットワークでの勾配消失・勾配爆発問題が解決される。適切な初期化（$C_W = 1$ など）と学習率のスケーリングにより、勾配が層を伝播する際に指数的に減衰・増幅することを防ぐ。

### Chapter 10: Kernel Learning

Chapter 10 では、無限幅ネットワークの訓練を完全に解析する。

**小さなステップ（§10.1）**:

初期化直後の1ステップ更新では、Frozen NTK $\Theta^{(\ell)}$ が支配的で、更新は学習率 $\eta$ について線形である。

**大きなジャンプ（§10.2）**:

完全に訓練されたネットワークの閉形式解が得られる。これは、1回のニュートン法でも多段階勾配降下法でも同じである（アルゴリズム独立性）。

**カーネル予測**:

テストデータ $x_{\dot{\beta}}$ に対する予測は次の形になる：

$$
\bar{z}^{(L)}_{i;\dot{\beta}} = z^{(L)}_{i;\dot{\beta}}(t=0) + \sum_{\tilde{\alpha} \in \mathcal{A}} \Theta^{(L)}_{\dot{\beta}\tilde{\alpha}} \cdot (\text{訓練誤差の関数})
$$

予測が初期出力、Frozen NTK、訓練データのみで決定されることがわかる。

**汎化（§10.3）**:

汎化誤差は **Bias-Variance トレードオフ** として分解される：

- **Bias（バイアス）**: アンサンブル平均予測と真の値のずれ
- **Variance（分散）**: ネットワークインスタンス間の予測のばらつき

臨界性により、このトレードオフが最適化される。

**線形モデルとの関連（§10.4）**:

無限幅ネットワークは、ランダム特徴に基づく線形モデルと等価である。従来のカーネル法との対応が明確になる。

::: {.callout-note collapse="true"}
## Kernel Methods との関連

**カーネル法（Kernel Methods）** は、機械学習における古典的な手法で、データを高次元特徴空間に写像し、その空間での線形分離を行う。

### カーネル法の基本

カーネル関数 $K(x, x')$ は、特徴写像 $\phi(x)$ の内積として定義される：

$$
K(x, x') = \langle \phi(x), \phi(x') \rangle
$$

典型的なカーネル：

- **線形カーネル**: $K(x, x') = x^\top x'$
- **多項式カーネル**: $K(x, x') = (x^\top x' + c)^d$
- **RBF カーネル**: $K(x, x') = \exp(-\gamma \|x - x'\|^2)$

### NTK とカーネル法の対応

無限幅ニューラルネットワークの Frozen NTK $\Theta^{(L)}$ は、カーネル法のカーネル関数と同じ役割を果たす。

具体的には、無限幅ネットワークの予測は次の形になる：

$$
f(x) = f_0(x) + \sum_{\alpha \in \mathcal{A}} \Theta^{(L)}(x, x_\alpha) \cdot (\text{誤差})
$$

これは、カーネル法の **Kernel Ridge Regression** と同型である：

$$
f(x) = \sum_{\alpha \in \mathcal{A}} \alpha_\alpha K(x, x_\alpha)
$$

### 無限幅ネットワークの制限

無限幅ネットワークは、次の理由でカーネル法と同様の制限を持つ：

1. **特徴が固定**: 訓練中に内部表現（隠れ層）が変化しない
2. **線形モデル**: パラメータに関して線形な予測を行う
3. **カーネルの固定**: Frozen NTK は訓練を通じて不変

これらの制限を超えるには、**有限幅ネットワーク**の解析が不可欠である（Chapter 11 で扱う）。

### 有限幅による改善

有限幅では、NTK が訓練中に変化し（解凍）、**適応的なカーネル**として振る舞う。これにより、真の表現学習が可能になる。
:::

## まとめ

Neural Tangent Kernel（NTK）は、深層学習の訓練ダイナミクスを理解するための強力な理論的枠組みである。

- **無限幅極限**: NTK は確定的（Frozen）で、ネットワークは線形モデルとして振る舞う
- **有限幅**: NTK が確率的にゆらぎ、訓練中に変化することで表現学習が可能になる
- **臨界性**: 適切なハイパーパラメータチューニングにより、深いネットワークでも安定した訓練が実現
- **カーネル法との関連**: 無限幅ネットワークは従来のカーネル法と等価だが、有限幅では適応的カーネルとして振る舞う

NTK 理論は、深層学習の「なぜ機能するのか」を理論的に解明する上で中心的な役割を果たしている。
